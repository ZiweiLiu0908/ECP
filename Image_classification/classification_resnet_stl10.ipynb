{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "458eb16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import torch\n",
    "\n",
    "import requests\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms as T\n",
    "from torchvision.datasets import CIFAR10\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ab55c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
     ]
    }
   ],
   "source": [
    "from examples.models.resnet import resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f4cd1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "threads = 16\n",
    "torch.set_num_threads(threads)\n",
    "\n",
    "#maybe better performance (Jupyter)\n",
    "#%env OMP_PLACES=cores\n",
    "#%env OMP_PROC_BIND=close\n",
    "#%env OMP_WAIT_POLICY=active\n",
    "\n",
    "#WSL\n",
    "os.environ['OMP_PLACES'] = 'cores'\n",
    "os.environ['OMP_PROC_BIND'] = 'close'\n",
    "os.environ['OMP_WAIT_POLICY'] = 'active'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3cbd963",
   "metadata": {},
   "outputs": [],
   "source": [
    "#axx_mult = 'mul8s_acc'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fac997d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/PyInit_conv2d_exact/build.ninja...\n",
      "Building extension module PyInit_conv2d_exact...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): AdaPT_Conv2d(\n",
       "    3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "    (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "    (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "  )\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): AdaPT_Conv2d(\n",
       "          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): AdaPT_Conv2d(\n",
       "          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): AdaPT_Conv2d(\n",
       "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): AdaPT_Conv2d(\n",
       "        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): AdaPT_Conv2d(\n",
       "        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "axx_mult = 'exact'\n",
    "model = resnet18(pretrained=True, axx_mult = axx_mult)\n",
    "model.eval() # for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47008795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms as T\n",
    "from torchvision.datasets import STL10\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def val_dataloader(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n",
    "\n",
    "    transform = T.Compose(\n",
    "        [\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean, std),\n",
    "        ]\n",
    "    )\n",
    "    dataset = STL10(root=\"datasets/stl10_data\", split=\"test\", download=True, transform=transform)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=32,\n",
    "        num_workers=0,\n",
    "        drop_last=True,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "def calculate_mean_std(dataset):\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=64, shuffle=False, num_workers=0)\n",
    "    mean = torch.zeros(3)\n",
    "    std = torch.zeros(3)\n",
    "    for images, _ in loader:\n",
    "        images = images.view(images.size(0), images.size(1), -1)\n",
    "        mean += images.mean(2).sum(0)\n",
    "        std += images.std(2).sum(0)\n",
    "    mean /= len(dataset)\n",
    "    std /= len(dataset)\n",
    "    return mean, std\n",
    "\n",
    "\n",
    "transform = T.Compose(\n",
    "    [\n",
    "        T.RandomCrop(96, padding=4),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset = STL10(root=\"datasets/stl10_data\", split=\"train\", download=True, transform=transform)\n",
    "\n",
    "mean, std = calculate_mean_std(dataset)\n",
    "\n",
    "transform = T.Compose(\n",
    "    [\n",
    "        T.RandomCrop(96, padding=4),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean, std),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "dataset = STL10(root=\"datasets/stl10_data\", split=\"train\", download=False, transform=transform)\n",
    "\n",
    "\n",
    "evens = list(range(0, len(dataset), 10))\n",
    "trainset_1 = torch.utils.data.Subset(dataset, evens)\n",
    "\n",
    "\n",
    "data = val_dataloader(mean, std)\n",
    "\n",
    "\n",
    "data_t = DataLoader(trainset_1, batch_size=64, shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8b3320f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:04<00:00,  2.45s/it]\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0222 18:59:29.298371 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:29.298740 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:29.298977 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:29.299181 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:29.299377 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:29.299570 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:29.299765 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:29.299949 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:29.300144 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:29.300338 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:29.300535 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:29.300744 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:29.300980 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:29.301178 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:29.301375 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:29.301561 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:29.301751 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:29.301939 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:29.302121 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:29.302301 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:29.302489 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:29.302665 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:29.302850 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:29.303034 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:29.303224 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:29.303409 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:29.303601 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:29.303781 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:29.303962 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:29.304146 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:29.304329 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:29.304502 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:29.304683 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:29.304858 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:29.305036 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:29.305210 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:29.305390 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:29.305564 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:29.305750 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:29.305930 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:29.308521 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:29.308734 140027934322880 tensor_quantizer.py:239] Call .cuda() if running on GPU after loading calibrated amax.\n",
      "W0222 18:59:29.309230 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:29.309643 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:29.310032 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:29.310416 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:29.310833 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:29.311275 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:29.311696 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:29.312098 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:29.312478 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:29.312971 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:29.313475 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:29.313992 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:29.314497 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:29.315007 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:29.315506 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:29.316038 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:29.316531 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:29.317029 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:29.317513 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:29.318013 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:29.318503 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:29.319029 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:29.319541 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:29.320054 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:29.320542 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:29.321049 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:29.321539 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:29.322051 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:29.322541 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:29.323053 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:29.323550 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:29.324048 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:29.324543 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:29.325043 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:29.325530 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:29.326032 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:29.326511 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:29.327002 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:29.327482 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.quantizer                         : TensorQuantizer(8bit per-tensor amax=2.5903 calibrator=HistogramCalibrator quant)\n",
      "conv1.quantizer_w                       : TensorQuantizer(8bit per-tensor amax=0.1513 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.6190 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0620 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3132 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0380 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.4999 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0354 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2533 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0360 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.4729 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0296 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2314 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0280 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.4729 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0738 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2733 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0287 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1852 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0247 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3030 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0197 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1450 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0141 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.3030 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0219 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1374 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0110 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1259 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0075 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1956 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0033 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2160 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0062 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.1956 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0127 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=1.2823 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0022 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.8047 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0067 calibrator=HistogramCalibrator quant)\n"
     ]
    }
   ],
   "source": [
    "from pytorch_quantization import nn as quant_nn\n",
    "from pytorch_quantization import calib\n",
    "\n",
    "def collect_stats(model, data_loader, num_batches):\n",
    "     \"\"\"Feed data to the network and collect statistic\"\"\"\n",
    "\n",
    "     # Enable calibrators\n",
    "     for name, module in model.named_modules():\n",
    "         if isinstance(module, quant_nn.TensorQuantizer):\n",
    "             if module._calibrator is not None:\n",
    "                 module.disable_quant()\n",
    "                 module.enable_calib()\n",
    "             else:\n",
    "                 module.disable()\n",
    "\n",
    "     for i, (image, _) in tqdm(enumerate(data_loader), total=num_batches):\n",
    "         model(image.cpu())\n",
    "         if i >= num_batches:\n",
    "             break\n",
    "\n",
    "     # Disable calibrators\n",
    "     for name, module in model.named_modules():\n",
    "         if isinstance(module, quant_nn.TensorQuantizer):\n",
    "             if module._calibrator is not None:\n",
    "                 module.enable_quant()\n",
    "                 module.disable_calib()\n",
    "             else:\n",
    "                 module.enable()\n",
    "\n",
    "def compute_amax(model, **kwargs):\n",
    " # Load calib result\n",
    " for name, module in model.named_modules():\n",
    "     if isinstance(module, quant_nn.TensorQuantizer):\n",
    "         if module._calibrator is not None:\n",
    "             if isinstance(module._calibrator, calib.MaxCalibrator):\n",
    "                 module.load_calib_amax()\n",
    "             else:\n",
    "                 module.load_calib_amax(**kwargs)\n",
    "         print(F\"{name:40}: {module}\")\n",
    " model.cpu()\n",
    "\n",
    "# It is a bit slow since we collect histograms on CPU\n",
    "with torch.no_grad():\n",
    "    stats = collect_stats(model, data_t, num_batches=2)\n",
    "    amax = compute_amax(model, method=\"percentile\", percentile=99.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a0b99b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 250/250 [26:03<00:00,  6.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563.7572889110015\n",
      "Accuracy of the network on the 10000 test images: 30.1625 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Inference without retraining\n",
    "import timeit\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "model.eval()\n",
    "start_time = timeit.default_timer()\n",
    "with torch.no_grad():\n",
    "    for iteraction, (images, labels) in tqdm(enumerate(data), total=len(data)):\n",
    "        images, labels = images.to(\"cpu\"), labels.to(\"cpu\")\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(timeit.default_timer() - start_time)\n",
    "print('Accuracy of the network on the 10000 test images: %.4f %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4939a153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/PyInit_conv2d_appro1/build.ninja...\n",
      "Building extension module PyInit_conv2d_appro1...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:12<00:00,  6.27s/it]\n",
      "W0127 17:58:22.760195 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 17:58:22.760936 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 17:58:22.761435 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 17:58:22.761948 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 17:58:22.762579 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 17:58:22.763264 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 17:58:22.763914 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 17:58:22.764824 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 17:58:22.765747 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 17:58:22.766747 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 17:58:22.767805 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 17:58:22.768644 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 17:58:22.769611 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 17:58:22.770738 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 17:58:22.772066 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 17:58:22.773229 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 17:58:22.773681 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 17:58:22.774246 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 17:58:22.774867 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 17:58:22.775892 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 17:58:22.776570 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 17:58:22.778018 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 17:58:22.778631 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 17:58:22.779322 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 17:58:22.780951 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 17:58:22.781404 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 17:58:22.782128 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 17:58:22.782648 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 17:58:22.783103 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 17:58:22.783622 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 17:58:22.784126 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 17:58:22.784703 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 17:58:22.785205 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 17:58:22.785821 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 17:58:22.786425 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 17:58:22.787035 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 17:58:22.787711 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 17:58:22.788244 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 17:58:22.788720 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 17:58:22.789303 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 17:58:22.790135 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 17:58:22.790935 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 17:58:22.792595 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 17:58:22.793752 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 17:58:22.794447 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 17:58:22.795811 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 17:58:22.796537 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 17:58:22.797437 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 17:58:22.798836 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 17:58:22.799597 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 17:58:22.800322 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 17:58:22.801210 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 17:58:22.802196 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 17:58:22.802986 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 17:58:22.803826 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 17:58:22.804657 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 17:58:22.806829 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 17:58:22.807588 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 17:58:22.808600 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 17:58:22.809433 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 17:58:22.810444 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 17:58:22.811407 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 17:58:22.813163 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 17:58:22.814468 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 17:58:22.815784 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 17:58:22.818710 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 17:58:22.819734 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 17:58:22.821302 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 17:58:22.825062 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 17:58:22.826902 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 17:58:22.828821 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 17:58:22.830497 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 17:58:22.831556 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 17:58:22.833110 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 17:58:22.839642 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 17:58:22.843323 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 17:58:22.845010 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 17:58:22.846580 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 17:58:22.847503 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 17:58:22.848563 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.quantizer                         : TensorQuantizer(8bit per-tensor amax=2.5911 calibrator=HistogramCalibrator quant)\n",
      "conv1.quantizer_w                       : TensorQuantizer(8bit per-tensor amax=0.1513 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.6151 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0620 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3165 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0380 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.4968 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0354 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2505 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0360 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.4736 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0296 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2310 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0280 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.4736 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0738 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2745 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0287 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1845 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0247 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3045 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0197 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1452 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0141 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.3045 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0219 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1383 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0110 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1259 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0075 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1957 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0033 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2129 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0062 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.1957 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0127 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=1.2553 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0022 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.7944 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0067 calibrator=HistogramCalibrator quant)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 250/250 [24:27<00:00,  5.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1467.5419439399993\n",
      "Accuracy of the network on the 10000 test images: 12.8000 %\n",
      "Epoch 1/15\n",
      "Epoch: [0]  [0/8]  eta: 0:03:20  lr: 0.0001  img/s: 2.5626608683080474  loss: 2.1491 (2.1491)  acc1: 32.8125 (32.8125)  acc5: 71.8750 (71.8750)  time: 25.0021  data: 0.0281\n",
      "Epoch: [0]  [1/8]  eta: 0:03:00  lr: 0.0001  img/s: 2.407788637427607  loss: 2.0949 (2.1220)  acc1: 32.8125 (32.8125)  acc5: 71.8750 (76.5625)  time: 25.8086  data: 0.0313\n",
      "Epoch: [0]  [2/8]  eta: 0:02:38  lr: 0.0001  img/s: 2.3360421490263303  loss: 2.0949 (2.0397)  acc1: 32.8125 (35.4167)  acc5: 81.2500 (78.1250)  time: 26.3494  data: 0.0323\n",
      "Epoch: [0]  [3/8]  eta: 0:02:13  lr: 0.0001  img/s: 2.328649485527525  loss: 2.0949 (2.0949)  acc1: 32.8125 (33.9844)  acc5: 71.8750 (76.5625)  time: 26.6411  data: 0.0324\n",
      "Epoch: [0]  [4/8]  eta: 0:01:46  lr: 0.0001  img/s: 2.455134510887385  loss: 2.0949 (2.0512)  acc1: 32.8125 (35.9375)  acc5: 73.4375 (75.9375)  time: 26.5335  data: 0.0330\n",
      "Epoch: [0]  [5/8]  eta: 0:01:19  lr: 0.0001  img/s: 2.4257678086306815  loss: 1.9188 (2.0291)  acc1: 32.8125 (35.6771)  acc5: 73.4375 (77.3438)  time: 26.5143  data: 0.0333\n",
      "Epoch: [0]  [6/8]  eta: 0:00:53  lr: 0.0001  img/s: 2.3552053349339594  loss: 2.0804 (2.0365)  acc1: 32.8125 (35.0446)  acc5: 76.5625 (77.2321)  time: 26.6139  data: 0.0339\n",
      "Epoch: [0]  [7/8]  eta: 0:00:26  lr: 0.0001  img/s: 2.3050676137662323  loss: 2.0804 (2.0799)  acc1: 32.8125 (34.0000)  acc5: 73.4375 (76.2000)  time: 26.1104  data: 0.0331\n",
      "Epoch: [0] Total time: 0:03:28\n",
      "Epoch 2/15\n",
      "Epoch: [1]  [0/8]  eta: 0:03:34  lr: 1e-05  img/s: 2.390907251811962  loss: 1.7166 (1.7166)  acc1: 39.0625 (39.0625)  acc5: 87.5000 (87.5000)  time: 26.8093  data: 0.0412\n",
      "Epoch: [1]  [1/8]  eta: 0:03:07  lr: 1e-05  img/s: 2.4065158982183967  loss: 1.5705 (1.6435)  acc1: 39.0625 (46.8750)  acc5: 87.5000 (89.0625)  time: 26.7251  data: 0.0438\n",
      "Epoch: [1]  [2/8]  eta: 0:02:38  lr: 1e-05  img/s: 2.4808895856889546  loss: 1.5705 (1.6077)  acc1: 53.1250 (48.9583)  acc5: 87.5000 (87.5000)  time: 26.4311  data: 0.0445\n",
      "Epoch: [1]  [3/8]  eta: 0:02:13  lr: 1e-05  img/s: 2.3244287806180854  loss: 1.5705 (1.6798)  acc1: 39.0625 (45.7031)  acc5: 84.3750 (84.7656)  time: 26.7167  data: 0.0433\n",
      "Epoch: [1]  [4/8]  eta: 0:01:47  lr: 1e-05  img/s: 2.3720890142619186  loss: 1.6595 (1.6757)  acc1: 53.1250 (47.1875)  acc5: 87.5000 (85.9375)  time: 26.7802  data: 0.0454\n",
      "Epoch: [1]  [5/8]  eta: 0:01:20  lr: 1e-05  img/s: 2.340067965462404  loss: 1.6595 (1.6867)  acc1: 40.6250 (46.0938)  acc5: 84.3750 (85.6771)  time: 26.8815  data: 0.0442\n",
      "Epoch: [1]  [6/8]  eta: 0:00:53  lr: 1e-05  img/s: 2.3979556394280532  loss: 1.7166 (1.6916)  acc1: 43.7500 (45.7589)  acc5: 84.3750 (85.4911)  time: 26.8585  data: 0.0424\n",
      "Epoch: [1]  [7/8]  eta: 0:00:26  lr: 1e-05  img/s: 2.350507812904138  loss: 1.7166 (1.7379)  acc1: 40.6250 (44.8000)  acc5: 84.3750 (84.4000)  time: 26.2709  data: 0.0414\n",
      "Epoch: [1] Total time: 0:03:30\n",
      "Epoch 3/15\n",
      "Epoch: [2]  [0/8]  eta: 0:03:35  lr: 1.0000000000000002e-06  img/s: 2.383786940897858  loss: 1.7841 (1.7841)  acc1: 39.0625 (39.0625)  acc5: 85.9375 (85.9375)  time: 26.8875  data: 0.0394\n",
      "Epoch: [2]  [1/8]  eta: 0:03:09  lr: 1.0000000000000002e-06  img/s: 2.342990988426909  loss: 1.6036 (1.6939)  acc1: 39.0625 (46.0938)  acc5: 85.9375 (88.2812)  time: 27.1242  data: 0.0424\n",
      "Epoch: [2]  [2/8]  eta: 0:02:43  lr: 1.0000000000000002e-06  img/s: 2.331276609540391  loss: 1.6036 (1.6435)  acc1: 48.4375 (46.8750)  acc5: 90.6250 (89.0625)  time: 27.2585  data: 0.0531\n",
      "Epoch: [2]  [3/8]  eta: 0:02:15  lr: 1.0000000000000002e-06  img/s: 2.3800615911209824  loss: 1.6036 (1.7158)  acc1: 39.0625 (44.5312)  acc5: 85.9375 (86.3281)  time: 27.1768  data: 0.0501\n",
      "Epoch: [2]  [4/8]  eta: 0:01:48  lr: 1.0000000000000002e-06  img/s: 2.390402700178488  loss: 1.6036 (1.6885)  acc1: 48.4375 (46.2500)  acc5: 85.9375 (86.2500)  time: 27.1089  data: 0.0529\n",
      "Epoch: [2]  [5/8]  eta: 0:01:21  lr: 1.0000000000000002e-06  img/s: 2.3312837362936842  loss: 1.6036 (1.6846)  acc1: 48.4375 (46.6146)  acc5: 85.9375 (86.1979)  time: 27.1724  data: 0.0502\n",
      "Epoch: [2]  [6/8]  eta: 0:00:54  lr: 1.0000000000000002e-06  img/s: 2.310750361824835  loss: 1.6651 (1.6968)  acc1: 48.4375 (46.4286)  acc5: 85.9375 (86.1607)  time: 27.2554  data: 0.0512\n",
      "Epoch: [2]  [7/8]  eta: 0:00:26  lr: 1.0000000000000002e-06  img/s: 2.2975997254642757  loss: 1.6651 (1.7355)  acc1: 45.3125 (45.4000)  acc5: 85.9375 (85.4000)  time: 26.6817  data: 0.0489\n",
      "Epoch: [2] Total time: 0:03:33\n",
      "Epoch 4/15\n",
      "Epoch: [3]  [0/8]  eta: 0:03:32  lr: 1.0000000000000002e-07  img/s: 2.4107050839913335  loss: 1.6973 (1.6973)  acc1: 42.1875 (42.1875)  acc5: 84.3750 (84.3750)  time: 26.5788  data: 0.0305\n",
      "Epoch: [3]  [1/8]  eta: 0:03:07  lr: 1.0000000000000002e-07  img/s: 2.3768635072766027  loss: 1.6046 (1.6509)  acc1: 42.1875 (45.3125)  acc5: 84.3750 (85.9375)  time: 26.7814  data: 0.0442\n",
      "Epoch: [3]  [2/8]  eta: 0:02:41  lr: 1.0000000000000002e-07  img/s: 2.3640921334462592  loss: 1.6046 (1.5985)  acc1: 48.4375 (47.9167)  acc5: 87.5000 (87.5000)  time: 26.8945  data: 0.0457\n",
      "Epoch: [3]  [3/8]  eta: 0:02:14  lr: 1.0000000000000002e-07  img/s: 2.3668014695366755  loss: 1.6046 (1.6649)  acc1: 42.1875 (46.0938)  acc5: 84.3750 (84.7656)  time: 26.9416  data: 0.0448\n",
      "Epoch: [3]  [4/8]  eta: 0:01:47  lr: 1.0000000000000002e-07  img/s: 2.386642973265492  loss: 1.6046 (1.6508)  acc1: 48.4375 (46.8750)  acc5: 87.5000 (86.2500)  time: 26.9241  data: 0.0435\n",
      "Epoch: [3]  [5/8]  eta: 0:01:20  lr: 1.0000000000000002e-07  img/s: 2.4165013358318554  loss: 1.6046 (1.6638)  acc1: 43.7500 (46.3542)  acc5: 84.3750 (85.6771)  time: 26.8570  data: 0.0424\n",
      "Epoch: [3]  [6/8]  eta: 0:00:53  lr: 1.0000000000000002e-07  img/s: 2.426329201501826  loss: 1.6973 (1.6792)  acc1: 45.3125 (46.2054)  acc5: 84.3750 (85.4911)  time: 26.7931  data: 0.0410\n",
      "Epoch: [3]  [7/8]  eta: 0:00:26  lr: 1.0000000000000002e-07  img/s: 2.3098325408059583  loss: 1.6973 (1.7219)  acc1: 43.7500 (44.8000)  acc5: 84.3750 (85.0000)  time: 26.2616  data: 0.0395\n",
      "Epoch: [3] Total time: 0:03:30\n",
      "Epoch 5/15\n",
      "Epoch: [4]  [0/8]  eta: 0:03:38  lr: 1.0000000000000004e-08  img/s: 2.342671474385128  loss: 1.7880 (1.7880)  acc1: 34.3750 (34.3750)  acc5: 85.9375 (85.9375)  time: 27.3714  data: 0.0522\n",
      "Epoch: [4]  [1/8]  eta: 0:03:14  lr: 1.0000000000000004e-08  img/s: 2.2811539401151433  loss: 1.5315 (1.6597)  acc1: 34.3750 (42.1875)  acc5: 85.9375 (88.2812)  time: 27.7334  data: 0.0458\n",
      "Epoch: [4]  [2/8]  eta: 0:02:45  lr: 1.0000000000000004e-08  img/s: 2.363304531443747  loss: 1.5315 (1.5970)  acc1: 50.0000 (46.3542)  acc5: 90.6250 (89.0625)  time: 27.5259  data: 0.0406\n",
      "Epoch: [4]  [3/8]  eta: 0:02:16  lr: 1.0000000000000004e-08  img/s: 2.413531165918565  loss: 1.5315 (1.6794)  acc1: 37.5000 (44.1406)  acc5: 85.9375 (86.3281)  time: 27.2894  data: 0.0461\n",
      "Epoch: [4]  [4/8]  eta: 0:01:48  lr: 1.0000000000000004e-08  img/s: 2.36598497223314  loss: 1.6043 (1.6644)  acc1: 45.3125 (44.3750)  acc5: 89.0625 (86.8750)  time: 27.2492  data: 0.0446\n",
      "Epoch: [4]  [5/8]  eta: 0:01:21  lr: 1.0000000000000004e-08  img/s: 2.4100958040073035  loss: 1.6043 (1.6663)  acc1: 45.3125 (44.7917)  acc5: 85.9375 (86.4583)  time: 27.1399  data: 0.0435\n",
      "Epoch: [4]  [6/8]  eta: 0:00:54  lr: 1.0000000000000004e-08  img/s: 2.3865299848066397  loss: 1.6761 (1.6697)  acc1: 45.3125 (44.8661)  acc5: 85.9375 (86.1607)  time: 27.0988  data: 0.0423\n",
      "Epoch: [4]  [7/8]  eta: 0:00:26  lr: 1.0000000000000004e-08  img/s: 2.3285568608176574  loss: 1.6761 (1.7049)  acc1: 45.3125 (42.8000)  acc5: 84.3750 (85.4000)  time: 26.5066  data: 0.0407\n",
      "Epoch: [4] Total time: 0:03:32\n",
      "Epoch 6/15\n",
      "Epoch: [5]  [0/8]  eta: 0:03:34  lr: 1.0000000000000005e-09  img/s: 2.3939306712398616  loss: 1.6573 (1.6573)  acc1: 43.7500 (43.7500)  acc5: 89.0625 (89.0625)  time: 26.8036  data: 0.0693\n",
      "Epoch: [5]  [1/8]  eta: 0:03:09  lr: 1.0000000000000005e-09  img/s: 2.3330707874472303  loss: 1.5177 (1.5875)  acc1: 43.7500 (47.6562)  acc5: 89.0625 (90.6250)  time: 27.1363  data: 0.0533\n",
      "Epoch: [5]  [2/8]  eta: 0:02:41  lr: 1.0000000000000005e-09  img/s: 2.4220070658046207  loss: 1.5177 (1.5551)  acc1: 50.0000 (48.4375)  acc5: 89.0625 (89.0625)  time: 26.9219  data: 0.0585\n",
      "Epoch: [5]  [3/8]  eta: 0:02:13  lr: 1.0000000000000005e-09  img/s: 2.4961612966317515  loss: 1.5177 (1.6494)  acc1: 43.7500 (45.7031)  acc5: 85.9375 (84.7656)  time: 26.6138  data: 0.0561\n",
      "Epoch: [5]  [4/8]  eta: 0:01:46  lr: 1.0000000000000005e-09  img/s: 2.413379598770019  loss: 1.6573 (1.6574)  acc1: 48.4375 (46.2500)  acc5: 87.5000 (85.3125)  time: 26.6034  data: 0.0535\n",
      "Epoch: [5]  [5/8]  eta: 0:01:20  lr: 1.0000000000000005e-09  img/s: 2.3683959090464817  loss: 1.6573 (1.6659)  acc1: 46.8750 (46.3542)  acc5: 85.9375 (85.4167)  time: 26.6792  data: 0.0506\n",
      "Epoch: [5]  [6/8]  eta: 0:00:53  lr: 1.0000000000000005e-09  img/s: 2.3602240889757997  loss: 1.6825 (1.6683)  acc1: 46.8750 (45.7589)  acc5: 85.9375 (85.4911)  time: 26.7488  data: 0.0504\n",
      "Epoch: [5]  [7/8]  eta: 0:00:26  lr: 1.0000000000000005e-09  img/s: 2.334065581453207  loss: 1.6825 (1.7123)  acc1: 43.7500 (44.2000)  acc5: 85.9375 (84.4000)  time: 26.1957  data: 0.0498\n",
      "Epoch: [5] Total time: 0:03:29\n",
      "Epoch 7/15\n",
      "Epoch: [6]  [0/8]  eta: 0:03:33  lr: 1.0000000000000006e-10  img/s: 2.4053388602224826  loss: 1.6457 (1.6457)  acc1: 37.5000 (37.5000)  acc5: 90.6250 (90.6250)  time: 26.6574  data: 0.0500\n",
      "Epoch: [6]  [1/8]  eta: 0:03:05  lr: 1.0000000000000006e-10  img/s: 2.4318238325252013  loss: 1.5196 (1.5826)  acc1: 37.5000 (46.0938)  acc5: 90.6250 (91.4062)  time: 26.5092  data: 0.0466\n",
      "Epoch: [6]  [2/8]  eta: 0:02:40  lr: 1.0000000000000006e-10  img/s: 2.3474849526981734  loss: 1.5409 (1.5687)  acc1: 50.0000 (47.3958)  acc5: 90.6250 (89.5833)  time: 26.7755  data: 0.0460\n",
      "Epoch: [6]  [3/8]  eta: 0:02:14  lr: 1.0000000000000006e-10  img/s: 2.386120642431636  loss: 1.5409 (1.6439)  acc1: 39.0625 (45.3125)  acc5: 85.9375 (87.1094)  time: 26.8046  data: 0.0520\n",
      "Epoch: [6]  [4/8]  eta: 0:01:47  lr: 1.0000000000000006e-10  img/s: 2.386487635457292  loss: 1.6457 (1.6447)  acc1: 46.8750 (45.6250)  acc5: 87.5000 (87.1875)  time: 26.8161  data: 0.0505\n",
      "Epoch: [6]  [5/8]  eta: 0:01:20  lr: 1.0000000000000006e-10  img/s: 2.381603165105172  loss: 1.6457 (1.6625)  acc1: 43.7500 (45.3125)  acc5: 85.9375 (85.6771)  time: 26.8323  data: 0.0486\n",
      "Epoch: [6]  [6/8]  eta: 0:00:53  lr: 1.0000000000000006e-10  img/s: 2.369417863878912  loss: 1.6481 (1.6701)  acc1: 43.7500 (44.4196)  acc5: 87.5000 (85.9375)  time: 26.8635  data: 0.0473\n",
      "Epoch: [6]  [7/8]  eta: 0:00:26  lr: 1.0000000000000006e-10  img/s: 2.2982231209875117  loss: 1.6481 (1.7110)  acc1: 39.0625 (42.8000)  acc5: 85.9375 (85.6000)  time: 26.3376  data: 0.0452\n",
      "Epoch: [6] Total time: 0:03:30\n",
      "Epoch 8/15\n",
      "Epoch: [7]  [0/8]  eta: 0:03:38  lr: 1.0000000000000006e-11  img/s: 2.3504558529040143  loss: 1.6555 (1.6555)  acc1: 39.0625 (39.0625)  acc5: 85.9375 (85.9375)  time: 27.2715  data: 0.0427\n",
      "Epoch: [7]  [1/8]  eta: 0:03:11  lr: 1.0000000000000006e-11  img/s: 2.3456996604585085  loss: 1.5215 (1.5885)  acc1: 39.0625 (42.1875)  acc5: 85.9375 (89.0625)  time: 27.2993  data: 0.0429\n",
      "Epoch: [7]  [2/8]  eta: 0:02:42  lr: 1.0000000000000006e-11  img/s: 2.402033004048323  loss: 1.5254 (1.5675)  acc1: 45.3125 (45.3125)  acc5: 85.9375 (88.0208)  time: 27.0923  data: 0.0400\n",
      "Epoch: [7]  [3/8]  eta: 0:02:15  lr: 1.0000000000000006e-11  img/s: 2.379172637484521  loss: 1.5254 (1.6464)  acc1: 39.0625 (41.7969)  acc5: 85.9375 (86.3281)  time: 27.0605  data: 0.0462\n",
      "Epoch: [7]  [4/8]  eta: 0:01:48  lr: 1.0000000000000006e-11  img/s: 2.374753252688082  loss: 1.6373 (1.6446)  acc1: 43.7500 (42.1875)  acc5: 85.9375 (87.5000)  time: 27.0478  data: 0.0464\n",
      "Epoch: [7]  [5/8]  eta: 0:01:21  lr: 1.0000000000000006e-11  img/s: 2.326502163954912  loss: 1.6373 (1.6658)  acc1: 42.1875 (42.1875)  acc5: 85.9375 (85.9375)  time: 27.1309  data: 0.0448\n",
      "Epoch: [7]  [6/8]  eta: 0:00:54  lr: 1.0000000000000006e-11  img/s: 2.3692381608857858  loss: 1.6555 (1.6789)  acc1: 42.1875 (41.7411)  acc5: 85.9375 (85.4911)  time: 27.1195  data: 0.0439\n",
      "Epoch: [7]  [7/8]  eta: 0:00:26  lr: 1.0000000000000006e-11  img/s: 2.32137669944079  loss: 1.6555 (1.7257)  acc1: 39.0625 (40.0000)  acc5: 82.8125 (85.0000)  time: 26.5342  data: 0.0429\n",
      "Epoch: [7] Total time: 0:03:32\n",
      "Epoch 9/15\n",
      "Epoch: [8]  [0/8]  eta: 0:03:33  lr: 1.0000000000000006e-12  img/s: 2.401894139081348  loss: 1.6835 (1.6835)  acc1: 42.1875 (42.1875)  acc5: 84.3750 (84.3750)  time: 26.6879  data: 0.0422\n",
      "Epoch: [8]  [1/8]  eta: 0:03:07  lr: 1.0000000000000006e-12  img/s: 2.3956458194920285  loss: 1.5754 (1.6295)  acc1: 42.1875 (46.0938)  acc5: 84.3750 (85.1562)  time: 26.7203  data: 0.0399\n",
      "Epoch: [8]  [2/8]  eta: 0:02:40  lr: 1.0000000000000006e-12  img/s: 2.4045714432313883  loss: 1.5754 (1.6050)  acc1: 50.0000 (47.9167)  acc5: 85.9375 (85.9375)  time: 26.6984  data: 0.0395\n",
      "Epoch: [8]  [3/8]  eta: 0:02:13  lr: 1.0000000000000006e-12  img/s: 2.3813363868371327  loss: 1.5754 (1.6644)  acc1: 42.1875 (46.0938)  acc5: 84.3750 (84.7656)  time: 26.7560  data: 0.0429\n",
      "Epoch: [8]  [4/8]  eta: 0:01:47  lr: 1.0000000000000006e-12  img/s: 2.3893742645900633  loss: 1.6833 (1.6682)  acc1: 46.8750 (46.2500)  acc5: 85.9375 (85.6250)  time: 26.7700  data: 0.0425\n",
      "Epoch: [8]  [5/8]  eta: 0:01:19  lr: 1.0000000000000006e-12  img/s: 2.4782270808260063  loss: 1.6833 (1.6842)  acc1: 42.1875 (45.3125)  acc5: 84.3750 (85.1562)  time: 26.6184  data: 0.0413\n",
      "Epoch: [8]  [6/8]  eta: 0:00:53  lr: 1.0000000000000006e-12  img/s: 2.310826568437027  loss: 1.6835 (1.7083)  acc1: 42.1875 (42.8571)  acc5: 84.3750 (84.8214)  time: 26.7779  data: 0.0410\n",
      "Epoch: [8]  [7/8]  eta: 0:00:26  lr: 1.0000000000000006e-12  img/s: 2.3874548999472447  loss: 1.6835 (1.7577)  acc1: 40.6250 (41.4000)  acc5: 84.3750 (84.8000)  time: 26.1576  data: 0.0402\n",
      "Epoch: [8] Total time: 0:03:29\n",
      "Epoch 10/15\n",
      "Epoch: [9]  [0/8]  eta: 0:03:33  lr: 1.0000000000000007e-13  img/s: 2.3984401333481666  loss: 1.7673 (1.7673)  acc1: 40.6250 (40.6250)  acc5: 84.3750 (84.3750)  time: 26.7211  data: 0.0370\n",
      "Epoch: [9]  [1/8]  eta: 0:03:08  lr: 1.0000000000000007e-13  img/s: 2.3618476627166727  loss: 1.5550 (1.6611)  acc1: 40.6250 (42.1875)  acc5: 84.3750 (86.7188)  time: 26.9292  data: 0.0384\n",
      "Epoch: [9]  [2/8]  eta: 0:02:41  lr: 1.0000000000000007e-13  img/s: 2.379170781841777  loss: 1.5550 (1.6179)  acc1: 43.7500 (44.7917)  acc5: 89.0625 (88.0208)  time: 26.9311  data: 0.0373\n",
      "Epoch: [9]  [3/8]  eta: 0:02:13  lr: 1.0000000000000007e-13  img/s: 2.45047723861193  loss: 1.5550 (1.6782)  acc1: 40.6250 (43.3594)  acc5: 84.3750 (85.5469)  time: 26.7394  data: 0.0396\n",
      "Epoch: [9]  [4/8]  eta: 0:01:47  lr: 1.0000000000000007e-13  img/s: 2.357976762660948  loss: 1.6022 (1.6630)  acc1: 43.7500 (45.0000)  acc5: 89.0625 (86.2500)  time: 26.8273  data: 0.0391\n",
      "Epoch: [9]  [5/8]  eta: 0:01:20  lr: 1.0000000000000007e-13  img/s: 2.3873668199363016  loss: 1.6022 (1.6850)  acc1: 40.6250 (44.2708)  acc5: 84.3750 (85.4167)  time: 26.8316  data: 0.0401\n",
      "Epoch: [9]  [6/8]  eta: 0:00:53  lr: 1.0000000000000007e-13  img/s: 2.3482732808962488  loss: 1.7673 (1.7013)  acc1: 40.6250 (43.5268)  acc5: 84.3750 (85.2679)  time: 26.8987  data: 0.0411\n",
      "Epoch: [9]  [7/8]  eta: 0:00:26  lr: 1.0000000000000007e-13  img/s: 2.36351923104369  loss: 1.7673 (1.7358)  acc1: 40.6250 (42.4000)  acc5: 84.3750 (84.6000)  time: 26.2897  data: 0.0392\n",
      "Epoch: [9] Total time: 0:03:30\n",
      "Epoch 11/15\n",
      "Epoch: [10]  [0/8]  eta: 0:03:35  lr: 1.0000000000000008e-14  img/s: 2.3800909452365264  loss: 1.7116 (1.7116)  acc1: 42.1875 (42.1875)  acc5: 84.3750 (84.3750)  time: 26.9333  data: 0.0435\n",
      "Epoch: [10]  [1/8]  eta: 0:03:08  lr: 1.0000000000000008e-14  img/s: 2.3817524365817957  loss: 1.6033 (1.6574)  acc1: 42.1875 (42.9688)  acc5: 84.3750 (86.7188)  time: 26.9237  data: 0.0433\n",
      "Epoch: [10]  [2/8]  eta: 0:02:42  lr: 1.0000000000000008e-14  img/s: 2.3530998881513523  loss: 1.6033 (1.6167)  acc1: 43.7500 (46.3542)  acc5: 85.9375 (86.4583)  time: 27.0335  data: 0.0472\n",
      "Epoch: [10]  [3/8]  eta: 0:02:15  lr: 1.0000000000000008e-14  img/s: 2.3283154729703317  loss: 1.6033 (1.6791)  acc1: 42.1875 (42.9688)  acc5: 84.3750 (84.3750)  time: 27.1598  data: 0.0482\n",
      "Epoch: [10]  [4/8]  eta: 0:01:48  lr: 1.0000000000000008e-14  img/s: 2.358611284088239  loss: 1.6129 (1.6659)  acc1: 43.7500 (44.0625)  acc5: 85.9375 (85.0000)  time: 27.1636  data: 0.0473\n",
      "Epoch: [10]  [5/8]  eta: 0:01:22  lr: 1.0000000000000008e-14  img/s: 2.218485587045901  loss: 1.6129 (1.6725)  acc1: 43.7500 (44.5312)  acc5: 84.3750 (84.8958)  time: 27.4515  data: 0.0465\n",
      "Epoch: [10]  [6/8]  eta: 0:00:55  lr: 1.0000000000000008e-14  img/s: 2.2952258878159517  loss: 1.6571 (1.6703)  acc1: 45.3125 (44.6429)  acc5: 85.9375 (85.4911)  time: 27.5226  data: 0.0492\n",
      "Epoch: [10]  [7/8]  eta: 0:00:26  lr: 1.0000000000000008e-14  img/s: 2.335393658528599  loss: 1.6571 (1.7163)  acc1: 43.7500 (43.4000)  acc5: 84.3750 (84.8000)  time: 26.8702  data: 0.0477\n",
      "Epoch: [10] Total time: 0:03:34\n",
      "Epoch 12/15\n",
      "Epoch: [11]  [0/8]  eta: 0:03:37  lr: 1.0000000000000009e-15  img/s: 2.361062199034017  loss: 1.6988 (1.6988)  acc1: 42.1875 (42.1875)  acc5: 84.3750 (84.3750)  time: 27.1433  data: 0.0368\n",
      "Epoch: [11]  [1/8]  eta: 0:03:09  lr: 1.0000000000000009e-15  img/s: 2.3644979509099224  loss: 1.5752 (1.6370)  acc1: 42.1875 (46.0938)  acc5: 84.3750 (86.7188)  time: 27.1293  data: 0.0425\n",
      "Epoch: [11]  [2/8]  eta: 0:02:43  lr: 1.0000000000000009e-15  img/s: 2.33348002006493  loss: 1.5752 (1.5859)  acc1: 50.0000 (48.9583)  acc5: 89.0625 (87.5000)  time: 27.2413  data: 0.0412\n",
      "Epoch: [11]  [3/8]  eta: 0:02:15  lr: 1.0000000000000009e-15  img/s: 2.4039696511873587  loss: 1.5752 (1.6655)  acc1: 42.1875 (46.0938)  acc5: 84.3750 (84.7656)  time: 27.0951  data: 0.0394\n",
      "Epoch: [11]  [4/8]  eta: 0:01:47  lr: 1.0000000000000009e-15  img/s: 2.4092094028105655  loss: 1.6568 (1.6637)  acc1: 48.4375 (46.5625)  acc5: 89.0625 (86.2500)  time: 26.9977  data: 0.0402\n",
      "Epoch: [11]  [5/8]  eta: 0:01:21  lr: 1.0000000000000009e-15  img/s: 2.3699357734238067  loss: 1.6568 (1.6788)  acc1: 42.1875 (45.8333)  acc5: 84.3750 (85.6771)  time: 27.0049  data: 0.0394\n",
      "Epoch: [11]  [6/8]  eta: 0:00:53  lr: 1.0000000000000009e-15  img/s: 2.3938618431359178  loss: 1.6988 (1.6822)  acc1: 45.3125 (45.7589)  acc5: 87.5000 (85.9375)  time: 26.9734  data: 0.0409\n",
      "Epoch: [11]  [7/8]  eta: 0:00:26  lr: 1.0000000000000009e-15  img/s: 2.329151253840536  loss: 1.6988 (1.7355)  acc1: 42.1875 (43.2000)  acc5: 84.3750 (84.8000)  time: 26.3977  data: 0.0410\n",
      "Epoch: [11] Total time: 0:03:31\n",
      "Epoch 13/15\n",
      "Epoch: [12]  [0/8]  eta: 0:03:34  lr: 1.000000000000001e-16  img/s: 2.3900493768484488  loss: 1.7641 (1.7641)  acc1: 42.1875 (42.1875)  acc5: 82.8125 (82.8125)  time: 26.8180  data: 0.0403\n",
      "Epoch: [12]  [1/8]  eta: 0:03:06  lr: 1.000000000000001e-16  img/s: 2.4129927928373496  loss: 1.4978 (1.6310)  acc1: 42.1875 (45.3125)  acc5: 82.8125 (87.5000)  time: 26.6928  data: 0.0424\n",
      "Epoch: [12]  [2/8]  eta: 0:02:40  lr: 1.000000000000001e-16  img/s: 2.3693155764288263  loss: 1.4978 (1.5574)  acc1: 48.4375 (49.4792)  acc5: 92.1875 (89.0625)  time: 26.8150  data: 0.0441\n",
      "Epoch: [12]  [3/8]  eta: 0:02:14  lr: 1.000000000000001e-16  img/s: 2.3737820443340394  loss: 1.4978 (1.6559)  acc1: 42.1875 (45.7031)  acc5: 82.8125 (86.3281)  time: 26.8612  data: 0.0427\n",
      "Epoch: [12]  [4/8]  eta: 0:01:47  lr: 1.000000000000001e-16  img/s: 2.3756670160459623  loss: 1.6585 (1.6564)  acc1: 48.4375 (46.2500)  acc5: 89.0625 (86.8750)  time: 26.8831  data: 0.0404\n",
      "Epoch: [12]  [5/8]  eta: 0:01:20  lr: 1.000000000000001e-16  img/s: 2.4128147692794006  loss: 1.6494 (1.6552)  acc1: 48.4375 (46.6146)  acc5: 82.8125 (85.9375)  time: 26.8320  data: 0.0422\n",
      "Epoch: [12]  [6/8]  eta: 0:00:53  lr: 1.000000000000001e-16  img/s: 2.4033478260059886  loss: 1.6585 (1.6632)  acc1: 48.4375 (46.4286)  acc5: 85.9375 (85.9375)  time: 26.8089  data: 0.0420\n",
      "Epoch: [12]  [7/8]  eta: 0:00:26  lr: 1.000000000000001e-16  img/s: 2.3364740238010238  loss: 1.6585 (1.7090)  acc1: 45.3125 (45.2000)  acc5: 82.8125 (85.4000)  time: 26.2441  data: 0.0410\n",
      "Epoch: [12] Total time: 0:03:29\n",
      "Epoch 14/15\n",
      "Epoch: [13]  [0/8]  eta: 0:03:28  lr: 1.000000000000001e-17  img/s: 2.459457938535123  loss: 1.7077 (1.7077)  acc1: 39.0625 (39.0625)  acc5: 84.3750 (84.3750)  time: 26.0657  data: 0.0437\n",
      "Epoch: [13]  [1/8]  eta: 0:03:03  lr: 1.000000000000001e-17  img/s: 2.434194110824502  loss: 1.5476 (1.6276)  acc1: 39.0625 (46.8750)  acc5: 84.3750 (87.5000)  time: 26.2026  data: 0.0456\n",
      "Epoch: [13]  [2/8]  eta: 0:02:37  lr: 1.000000000000001e-17  img/s: 2.4294905540688005  loss: 1.5476 (1.5564)  acc1: 54.6875 (50.5208)  acc5: 87.5000 (87.5000)  time: 26.2660  data: 0.0470\n",
      "Epoch: [13]  [3/8]  eta: 0:02:12  lr: 1.000000000000001e-17  img/s: 2.3633264825286875  loss: 1.5476 (1.6442)  acc1: 39.0625 (46.0938)  acc5: 84.3750 (85.9375)  time: 26.4824  data: 0.0480\n",
      "Epoch: [13]  [4/8]  eta: 0:01:45  lr: 1.000000000000001e-17  img/s: 2.429998016789684  loss: 1.6882 (1.6530)  acc1: 42.1875 (45.3125)  acc5: 87.5000 (86.5625)  time: 26.4614  data: 0.0464\n",
      "Epoch: [13]  [5/8]  eta: 0:01:19  lr: 1.000000000000001e-17  img/s: 2.4418126765407053  loss: 1.6882 (1.6623)  acc1: 42.1875 (46.0938)  acc5: 84.3750 (85.9375)  time: 26.4255  data: 0.0447\n",
      "Epoch: [13]  [6/8]  eta: 0:00:52  lr: 1.000000000000001e-17  img/s: 2.430176319048189  loss: 1.7036 (1.6682)  acc1: 42.1875 (45.5357)  acc5: 87.5000 (86.6071)  time: 26.4194  data: 0.0450\n",
      "Epoch: [13]  [7/8]  eta: 0:00:25  lr: 1.000000000000001e-17  img/s: 2.3782040628982446  loss: 1.7036 (1.7121)  acc1: 42.1875 (44.4000)  acc5: 84.6154 (86.4000)  time: 25.8548  data: 0.0441\n",
      "Epoch: [13] Total time: 0:03:26\n",
      "Epoch 15/15\n",
      "Epoch: [14]  [0/8]  eta: 0:03:32  lr: 1.000000000000001e-18  img/s: 2.4105721632626516  loss: 1.7546 (1.7546)  acc1: 43.7500 (43.7500)  acc5: 84.3750 (84.3750)  time: 26.5867  data: 0.0369\n",
      "Epoch: [14]  [1/8]  eta: 0:03:03  lr: 1.000000000000001e-18  img/s: 2.4763062538270897  loss: 1.5479 (1.6513)  acc1: 43.7500 (47.6562)  acc5: 84.3750 (86.7188)  time: 26.2367  data: 0.0393\n",
      "Epoch: [14]  [2/8]  eta: 0:02:37  lr: 1.000000000000001e-18  img/s: 2.453721901819499  loss: 1.5846 (1.6290)  acc1: 51.5625 (49.4792)  acc5: 89.0625 (87.5000)  time: 26.2025  data: 0.0433\n",
      "Epoch: [14]  [3/8]  eta: 0:02:11  lr: 1.000000000000001e-18  img/s: 2.4382006272523524  loss: 1.5846 (1.6674)  acc1: 43.7500 (47.6562)  acc5: 84.3750 (86.7188)  time: 26.2273  data: 0.0457\n",
      "Epoch: [14]  [4/8]  eta: 0:01:45  lr: 1.000000000000001e-18  img/s: 2.391379996428727  loss: 1.5982 (1.6536)  acc1: 45.3125 (47.1875)  acc5: 89.0625 (87.5000)  time: 26.3436  data: 0.0457\n",
      "Epoch: [14]  [5/8]  eta: 0:01:19  lr: 1.000000000000001e-18  img/s: 2.392632783897027  loss: 1.5982 (1.6672)  acc1: 43.7500 (46.3542)  acc5: 84.3750 (86.1979)  time: 26.4189  data: 0.0459\n",
      "Epoch: [14]  [6/8]  eta: 0:00:52  lr: 1.000000000000001e-18  img/s: 2.4131329007527906  loss: 1.6796 (1.6690)  acc1: 43.7500 (45.5357)  acc5: 89.0625 (86.6071)  time: 26.4403  data: 0.0461\n",
      "Epoch: [14]  [7/8]  eta: 0:00:25  lr: 1.000000000000001e-18  img/s: 2.3799976226628647  loss: 1.6796 (1.7207)  acc1: 42.1875 (44.0000)  acc5: 84.3750 (85.4000)  time: 25.8702  data: 0.0441\n",
      "Epoch: [14] Total time: 0:03:26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 250/250 [23:46<00:00,  5.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1426.6530973600165\n",
      "Accuracy of the network on the 10000 test images: 38.9500 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "axx_mult = 'appro1'\n",
    "model = resnet18(pretrained=True, axx_mult = axx_mult)\n",
    "model.eval() # for evaluation\n",
    "# It is a bit slow since we collect histograms on CPU\n",
    "with torch.no_grad():\n",
    "    stats = collect_stats(model, data_t, num_batches=2)\n",
    "    amax = compute_amax(model, method=\"percentile\", percentile=99.99)\n",
    "    \n",
    "# Inference without retraining\n",
    "import timeit\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "model.eval()\n",
    "start_time = timeit.default_timer()\n",
    "with torch.no_grad():\n",
    "    for iteraction, (images, labels) in tqdm(enumerate(data), total=len(data)):\n",
    "        images, labels = images.to(\"cpu\"), labels.to(\"cpu\")\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(timeit.default_timer() - start_time)\n",
    "print('Accuracy of the network on the 10000 test images: %.4f %%' % (\n",
    "    100 * correct / total))\n",
    "\n",
    "# Retraining for 15 epochs\n",
    "from adapt.references.classification.train import evaluate, train_one_epoch, load_data\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "for epoch in range(15):\n",
    "    print(f\"Epoch {epoch + 1}/15\")\n",
    "    train_one_epoch(model, criterion, optimizer, data_t, \"cpu\", epoch, 1)\n",
    "    lr_scheduler.step()\n",
    "\n",
    "# Inference after retraining\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "model.eval()\n",
    "start_time = timeit.default_timer()\n",
    "with torch.no_grad():\n",
    "    for iteraction, (images, labels) in tqdm(enumerate(data), total=len(data)):\n",
    "        images, labels = images.to(\"cpu\"), labels.to(\"cpu\")\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(timeit.default_timer() - start_time)\n",
    "print('Accuracy of the network on the 10000 test images: %.4f %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b591026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/PyInit_conv2d_appro2/build.ninja...\n",
      "Building extension module PyInit_conv2d_appro2...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:12<00:00,  6.07s/it]\n",
      "W0127 19:39:26.841186 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 19:39:26.842470 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 19:39:26.842966 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 19:39:26.843447 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 19:39:26.844125 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 19:39:26.844841 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 19:39:26.846164 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 19:39:26.846925 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 19:39:26.847591 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 19:39:26.848444 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 19:39:26.850964 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 19:39:26.851508 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 19:39:26.852253 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 19:39:26.854284 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 19:39:26.854855 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 19:39:26.855602 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 19:39:26.856219 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 19:39:26.856763 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 19:39:26.857355 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 19:39:26.857866 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 19:39:26.858566 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 19:39:26.859118 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 19:39:26.859822 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 19:39:26.860980 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 19:39:26.861541 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 19:39:26.862471 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 19:39:26.863023 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 19:39:26.863592 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 19:39:26.864252 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 19:39:26.864953 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 19:39:26.865803 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 19:39:26.866455 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 19:39:26.866989 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 19:39:26.868215 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 19:39:26.869097 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 19:39:26.869645 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 19:39:26.870327 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 19:39:26.871054 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 19:39:26.871794 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 19:39:26.872403 140173119096640 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0127 19:39:26.873599 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 19:39:26.874534 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 19:39:26.875639 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 19:39:26.876633 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 19:39:26.877511 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 19:39:26.878809 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 19:39:26.880299 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 19:39:26.881534 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 19:39:26.882807 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 19:39:26.883797 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 19:39:26.884980 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 19:39:26.885884 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 19:39:26.888252 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 19:39:26.889278 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 19:39:26.890393 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 19:39:26.893628 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 19:39:26.895427 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 19:39:26.899067 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 19:39:26.900176 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 19:39:26.900935 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 19:39:26.902055 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 19:39:26.903300 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 19:39:26.904618 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 19:39:26.905793 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 19:39:26.908155 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 19:39:26.909080 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 19:39:26.910042 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 19:39:26.911794 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 19:39:26.915745 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 19:39:26.916744 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 19:39:26.918127 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 19:39:26.918814 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 19:39:26.921084 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 19:39:26.921896 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 19:39:26.922839 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 19:39:26.924092 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 19:39:26.925899 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 19:39:26.926838 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 19:39:26.927958 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0127 19:39:26.928843 140173119096640 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.quantizer                         : TensorQuantizer(8bit per-tensor amax=2.5911 calibrator=HistogramCalibrator quant)\n",
      "conv1.quantizer_w                       : TensorQuantizer(8bit per-tensor amax=0.1513 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.6091 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0620 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3120 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0380 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.5012 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0354 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2515 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0360 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.4746 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0296 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2303 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0280 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.4746 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0738 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2746 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0287 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1839 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0247 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3035 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0197 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1454 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0141 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.3035 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0219 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1366 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0110 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1257 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0075 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1959 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0033 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2134 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0062 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.1959 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0127 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=1.2652 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0022 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.7867 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0067 calibrator=HistogramCalibrator quant)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 250/250 [24:09<00:00,  5.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1449.5421034650062\n",
      "Accuracy of the network on the 10000 test images: 10.0000 %\n",
      "Epoch 1/15\n",
      "Epoch: [0]  [0/8]  eta: 0:03:15  lr: 0.0001  img/s: 2.6233859169991365  loss: 2.3461 (2.3461)  acc1: 12.5000 (12.5000)  acc5: 54.6875 (54.6875)  time: 24.4357  data: 0.0397\n",
      "Epoch: [0]  [1/8]  eta: 0:03:00  lr: 0.0001  img/s: 2.3686947424807823  loss: 2.3086 (2.3274)  acc1: 12.5000 (15.6250)  acc5: 50.0000 (52.3438)  time: 25.7530  data: 0.0455\n",
      "Epoch: [0]  [2/8]  eta: 0:02:37  lr: 0.0001  img/s: 2.3404308043677378  loss: 2.3461 (2.3549)  acc1: 12.5000 (14.0625)  acc5: 50.0000 (50.0000)  time: 26.2973  data: 0.0438\n",
      "Epoch: [0]  [3/8]  eta: 0:02:12  lr: 0.0001  img/s: 2.336144836608688  loss: 2.3137 (2.3446)  acc1: 12.5000 (14.4531)  acc5: 50.0000 (52.3438)  time: 26.5796  data: 0.0406\n",
      "Epoch: [0]  [4/8]  eta: 0:01:47  lr: 0.0001  img/s: 2.3298520128356155  loss: 2.3461 (2.3506)  acc1: 12.5000 (12.5000)  acc5: 51.5625 (52.1875)  time: 26.7651  data: 0.0400\n",
      "Epoch: [0]  [5/8]  eta: 0:01:20  lr: 0.0001  img/s: 2.3922281888529833  loss: 2.3461 (2.3525)  acc1: 10.9375 (11.9792)  acc5: 50.0000 (51.3021)  time: 26.7691  data: 0.0392\n",
      "Epoch: [0]  [6/8]  eta: 0:00:53  lr: 0.0001  img/s: 2.343306539364061  loss: 2.3461 (2.3427)  acc1: 12.5000 (12.0536)  acc5: 51.5625 (53.3482)  time: 26.8523  data: 0.0393\n",
      "Epoch: [0]  [7/8]  eta: 0:00:26  lr: 0.0001  img/s: 2.33286738837756  loss: 2.3461 (2.3471)  acc1: 12.5000 (12.2000)  acc5: 51.5625 (53.2000)  time: 26.2867  data: 0.0391\n",
      "Epoch: [0] Total time: 0:03:30\n",
      "Epoch 2/15\n",
      "Epoch: [1]  [0/8]  eta: 0:03:35  lr: 1e-05  img/s: 2.383545430053651  loss: 2.3683 (2.3683)  acc1: 6.2500 (6.2500)  acc5: 48.4375 (48.4375)  time: 26.9004  data: 0.0496\n",
      "Epoch: [1]  [1/8]  eta: 0:03:09  lr: 1e-05  img/s: 2.3536986838705043  loss: 2.3305 (2.3494)  acc1: 6.2500 (10.9375)  acc5: 48.4375 (52.3438)  time: 27.0648  data: 0.0438\n",
      "Epoch: [1]  [2/8]  eta: 0:02:43  lr: 1e-05  img/s: 2.3040481035433604  loss: 2.3683 (2.3760)  acc1: 6.2500 (9.3750)  acc5: 48.4375 (48.4375)  time: 27.3162  data: 0.0431\n",
      "Epoch: [1]  [3/8]  eta: 0:02:17  lr: 1e-05  img/s: 2.3175585538783814  loss: 2.3683 (2.3860)  acc1: 6.2500 (8.9844)  acc5: 42.1875 (46.8750)  time: 27.4015  data: 0.0429\n",
      "Epoch: [1]  [4/8]  eta: 0:01:49  lr: 1e-05  img/s: 2.3440381270489996  loss: 2.4161 (2.3955)  acc1: 6.2500 (8.1250)  acc5: 42.1875 (45.3125)  time: 27.3928  data: 0.0452\n",
      "Epoch: [1]  [5/8]  eta: 0:01:21  lr: 1e-05  img/s: 2.3799689541962414  loss: 2.4070 (2.3974)  acc1: 6.2500 (8.0729)  acc5: 42.1875 (45.5729)  time: 27.3164  data: 0.0449\n",
      "Epoch: [1]  [6/8]  eta: 0:00:54  lr: 1e-05  img/s: 2.326031581361021  loss: 2.4070 (2.3805)  acc1: 7.8125 (9.1518)  acc5: 46.8750 (47.7679)  time: 27.3503  data: 0.0441\n",
      "Epoch: [1]  [7/8]  eta: 0:00:26  lr: 1e-05  img/s: 2.31206382355061  loss: 2.3984 (2.3827)  acc1: 7.8125 (9.6000)  acc5: 44.2308 (47.4000)  time: 26.7468  data: 0.0425\n",
      "Epoch: [1] Total time: 0:03:33\n",
      "Epoch 3/15\n",
      "Epoch: [2]  [0/8]  eta: 0:03:40  lr: 1.0000000000000002e-06  img/s: 2.320313779086087  loss: 2.4081 (2.4081)  acc1: 9.3750 (9.3750)  acc5: 39.0625 (39.0625)  time: 27.6197  data: 0.0372\n",
      "Epoch: [2]  [1/8]  eta: 0:03:12  lr: 1.0000000000000002e-06  img/s: 2.334764869481051  loss: 2.2770 (2.3425)  acc1: 9.3750 (11.7188)  acc5: 39.0625 (52.3438)  time: 27.5380  data: 0.0409\n",
      "Epoch: [2]  [2/8]  eta: 0:02:44  lr: 1.0000000000000002e-06  img/s: 2.3390422038618883  loss: 2.3435 (2.3428)  acc1: 9.3750 (10.9375)  acc5: 54.6875 (53.1250)  time: 27.4922  data: 0.0402\n",
      "Epoch: [2]  [3/8]  eta: 0:02:16  lr: 1.0000000000000002e-06  img/s: 2.3720421033819967  loss: 2.3435 (2.3461)  acc1: 9.3750 (10.5469)  acc5: 50.0000 (52.3438)  time: 27.3813  data: 0.0471\n",
      "Epoch: [2]  [4/8]  eta: 0:01:49  lr: 1.0000000000000002e-06  img/s: 2.3334035898705863  loss: 2.3435 (2.3397)  acc1: 9.3750 (10.3125)  acc5: 50.0000 (51.5625)  time: 27.3995  data: 0.0465\n",
      "Epoch: [2]  [5/8]  eta: 0:01:22  lr: 1.0000000000000002e-06  img/s: 2.3285941770766407  loss: 2.3435 (2.3447)  acc1: 9.3750 (11.4583)  acc5: 48.4375 (49.4792)  time: 27.4206  data: 0.0458\n",
      "Epoch: [2]  [6/8]  eta: 0:00:54  lr: 1.0000000000000002e-06  img/s: 2.3314486153671483  loss: 2.3435 (2.3434)  acc1: 9.3750 (11.1607)  acc5: 50.0000 (50.0000)  time: 27.4298  data: 0.0441\n",
      "Epoch: [2]  [7/8]  eta: 0:00:26  lr: 1.0000000000000002e-06  img/s: 2.3298756133068808  loss: 2.3435 (2.3575)  acc1: 9.3750 (10.6000)  acc5: 48.4375 (48.2000)  time: 26.7946  data: 0.0422\n",
      "Epoch: [2] Total time: 0:03:34\n",
      "Epoch 4/15\n",
      "Epoch: [3]  [0/8]  eta: 0:03:33  lr: 1.0000000000000002e-07  img/s: 2.3958638713936624  loss: 2.4093 (2.4093)  acc1: 6.2500 (6.2500)  acc5: 43.7500 (43.7500)  time: 26.7452  data: 0.0325\n",
      "Epoch: [3]  [1/8]  eta: 0:03:10  lr: 1.0000000000000002e-07  img/s: 2.3264460501427116  loss: 2.2939 (2.3516)  acc1: 6.2500 (10.9375)  acc5: 43.7500 (52.3438)  time: 27.1490  data: 0.0377\n",
      "Epoch: [3]  [2/8]  eta: 0:02:43  lr: 1.0000000000000002e-07  img/s: 2.3509302148305258  loss: 2.3765 (2.3599)  acc1: 9.3750 (10.4167)  acc5: 53.1250 (52.6042)  time: 27.1865  data: 0.0379\n",
      "Epoch: [3]  [3/8]  eta: 0:02:16  lr: 1.0000000000000002e-07  img/s: 2.3471768953439813  loss: 2.3498 (2.3574)  acc1: 9.3750 (11.3281)  acc5: 45.3125 (50.7812)  time: 27.2208  data: 0.0427\n",
      "Epoch: [3]  [4/8]  eta: 0:01:49  lr: 1.0000000000000002e-07  img/s: 2.333993477575946  loss: 2.3550 (2.3569)  acc1: 10.9375 (11.2500)  acc5: 50.0000 (50.6250)  time: 27.2686  data: 0.0420\n",
      "Epoch: [3]  [5/8]  eta: 0:01:21  lr: 1.0000000000000002e-07  img/s: 2.343772862737034  loss: 2.3498 (2.3536)  acc1: 10.9375 (11.1979)  acc5: 50.0000 (51.3021)  time: 27.2829  data: 0.0429\n",
      "Epoch: [3]  [6/8]  eta: 0:00:54  lr: 1.0000000000000002e-07  img/s: 2.332229348734347  loss: 2.3498 (2.3530)  acc1: 10.9375 (10.2679)  acc5: 53.1250 (52.0089)  time: 27.3110  data: 0.0422\n",
      "Epoch: [3]  [7/8]  eta: 0:00:26  lr: 1.0000000000000002e-07  img/s: 2.224567028379778  loss: 2.3491 (2.3508)  acc1: 10.9375 (10.6000)  acc5: 50.0000 (51.6000)  time: 26.8245  data: 0.0424\n",
      "Epoch: [3] Total time: 0:03:34\n",
      "Epoch 5/15\n",
      "Epoch: [4]  [0/8]  eta: 0:03:56  lr: 1.0000000000000004e-08  img/s: 2.165297044435658  loss: 2.3722 (2.3722)  acc1: 9.3750 (9.3750)  acc5: 45.3125 (45.3125)  time: 29.6062  data: 0.0491\n",
      "Epoch: [4]  [1/8]  eta: 0:03:19  lr: 1.0000000000000004e-08  img/s: 2.3405448370368074  loss: 2.2887 (2.3304)  acc1: 9.3750 (13.2812)  acc5: 45.3125 (53.1250)  time: 28.4951  data: 0.0445\n",
      "Epoch: [4]  [2/8]  eta: 0:02:48  lr: 1.0000000000000004e-08  img/s: 2.340210627055075  loss: 2.3280 (2.3296)  acc1: 9.3750 (10.9375)  acc5: 53.1250 (53.1250)  time: 28.1243  data: 0.0412\n",
      "Epoch: [4]  [3/8]  eta: 0:02:21  lr: 1.0000000000000004e-08  img/s: 2.243435116675477  loss: 2.3280 (2.3460)  acc1: 6.2500 (9.3750)  acc5: 45.3125 (49.2188)  time: 28.2406  data: 0.0464\n",
      "Epoch: [4]  [4/8]  eta: 0:01:52  lr: 1.0000000000000004e-08  img/s: 2.286829285380757  loss: 2.3436 (2.3455)  acc1: 7.8125 (9.0625)  acc5: 45.3125 (48.4375)  time: 28.1995  data: 0.0468\n",
      "Epoch: [4]  [5/8]  eta: 0:01:24  lr: 1.0000000000000004e-08  img/s: 2.328897840330598  loss: 2.3436 (2.3486)  acc1: 7.8125 (9.1146)  acc5: 45.3125 (47.9167)  time: 28.0866  data: 0.0460\n",
      "Epoch: [4]  [6/8]  eta: 0:00:56  lr: 1.0000000000000004e-08  img/s: 2.293919935951003  loss: 2.3436 (2.3449)  acc1: 9.3750 (9.1518)  acc5: 45.3125 (47.9911)  time: 28.0655  data: 0.0450\n",
      "Epoch: [4]  [7/8]  eta: 0:00:27  lr: 1.0000000000000004e-08  img/s: 2.3105113985622427  loss: 2.3280 (2.3398)  acc1: 9.3750 (9.8000)  acc5: 45.3125 (48.4000)  time: 27.3748  data: 0.0436\n",
      "Epoch: [4] Total time: 0:03:39\n",
      "Epoch 6/15\n",
      "Epoch: [5]  [0/8]  eta: 0:03:33  lr: 1.0000000000000005e-09  img/s: 2.4024059190541784  loss: 2.3652 (2.3652)  acc1: 7.8125 (7.8125)  acc5: 46.8750 (46.8750)  time: 26.6829  data: 0.0430\n",
      "Epoch: [5]  [1/8]  eta: 0:03:08  lr: 1.0000000000000005e-09  img/s: 2.353327366572018  loss: 2.3081 (2.3367)  acc1: 7.8125 (10.9375)  acc5: 46.8750 (53.1250)  time: 26.9589  data: 0.0411\n",
      "Epoch: [5]  [2/8]  eta: 0:02:41  lr: 1.0000000000000005e-09  img/s: 2.379816150682396  loss: 2.3652 (2.3750)  acc1: 7.8125 (9.8958)  acc5: 46.8750 (47.9167)  time: 26.9490  data: 0.0395\n",
      "Epoch: [5]  [3/8]  eta: 0:02:15  lr: 1.0000000000000005e-09  img/s: 2.3533993923490746  loss: 2.3652 (2.3826)  acc1: 7.8125 (9.3750)  acc5: 40.6250 (46.0938)  time: 27.0214  data: 0.0406\n",
      "Epoch: [5]  [4/8]  eta: 0:01:48  lr: 1.0000000000000005e-09  img/s: 2.346674320500224  loss: 2.4052 (2.3892)  acc1: 7.8125 (8.4375)  acc5: 46.8750 (46.2500)  time: 27.0795  data: 0.0403\n",
      "Epoch: [5]  [5/8]  eta: 0:01:21  lr: 1.0000000000000005e-09  img/s: 2.377843094866021  loss: 2.3652 (2.3743)  acc1: 7.8125 (9.8958)  acc5: 46.8750 (47.1354)  time: 27.0590  data: 0.0405\n",
      "Epoch: [5]  [6/8]  eta: 0:00:54  lr: 1.0000000000000005e-09  img/s: 2.3756807032502083  loss: 2.4052 (2.3838)  acc1: 7.8125 (9.5982)  acc5: 46.8750 (46.6518)  time: 27.0472  data: 0.0400\n",
      "Epoch: [5]  [7/8]  eta: 0:00:26  lr: 1.0000000000000005e-09  img/s: 2.3064659591978174  loss: 2.3930 (2.3850)  acc1: 7.8125 (9.6000)  acc5: 43.7500 (46.2000)  time: 26.4893  data: 0.0399\n",
      "Epoch: [5] Total time: 0:03:31\n",
      "Epoch 7/15\n",
      "Epoch: [6]  [0/8]  eta: 0:03:33  lr: 1.0000000000000006e-10  img/s: 2.39794246550445  loss: 2.3565 (2.3565)  acc1: 4.6875 (4.6875)  acc5: 51.5625 (51.5625)  time: 26.7369  data: 0.0473\n",
      "Epoch: [6]  [1/8]  eta: 0:03:09  lr: 1.0000000000000006e-10  img/s: 2.348593461887726  loss: 2.3385 (2.3475)  acc1: 4.6875 (10.1562)  acc5: 51.5625 (52.3438)  time: 27.0138  data: 0.0438\n",
      "Epoch: [6]  [2/8]  eta: 0:02:42  lr: 1.0000000000000006e-10  img/s: 2.3523067705317713  loss: 2.3385 (2.3362)  acc1: 9.3750 (9.8958)  acc5: 53.1250 (54.1667)  time: 27.0908  data: 0.0417\n",
      "Epoch: [6]  [3/8]  eta: 0:02:16  lr: 1.0000000000000006e-10  img/s: 2.3245901751495426  loss: 2.3136 (2.3267)  acc1: 7.8125 (9.3750)  acc5: 53.1250 (54.2969)  time: 27.2113  data: 0.0416\n",
      "Epoch: [6]  [4/8]  eta: 0:01:49  lr: 1.0000000000000006e-10  img/s: 2.3257976410550745  loss: 2.3385 (2.3319)  acc1: 9.3750 (9.3750)  acc5: 53.1250 (54.0625)  time: 27.2805  data: 0.0412\n",
      "Epoch: [6]  [5/8]  eta: 0:01:21  lr: 1.0000000000000006e-10  img/s: 2.3415918119941757  loss: 2.3385 (2.3373)  acc1: 9.3750 (9.3750)  acc5: 53.1250 (52.6042)  time: 27.2979  data: 0.0432\n",
      "Epoch: [6]  [6/8]  eta: 0:00:54  lr: 1.0000000000000006e-10  img/s: 2.3371433140619353  loss: 2.3385 (2.3329)  acc1: 9.3750 (10.2679)  acc5: 53.1250 (53.5714)  time: 27.3161  data: 0.0429\n",
      "Epoch: [6]  [7/8]  eta: 0:00:26  lr: 1.0000000000000006e-10  img/s: 2.341065221893633  loss: 2.3385 (2.3393)  acc1: 9.3750 (10.8000)  acc5: 53.1250 (52.4000)  time: 26.6839  data: 0.0434\n",
      "Epoch: [6] Total time: 0:03:33\n",
      "Epoch 8/15\n",
      "Epoch: [7]  [0/8]  eta: 0:03:41  lr: 1.0000000000000006e-11  img/s: 2.316374216882655  loss: 2.3865 (2.3865)  acc1: 12.5000 (12.5000)  acc5: 40.6250 (40.6250)  time: 27.6680  data: 0.0386\n",
      "Epoch: [7]  [1/8]  eta: 0:03:13  lr: 1.0000000000000006e-11  img/s: 2.320778780225437  loss: 2.3765 (2.3815)  acc1: 7.8125 (10.1562)  acc5: 40.6250 (43.7500)  time: 27.6436  data: 0.0404\n",
      "Epoch: [7]  [2/8]  eta: 0:02:45  lr: 1.0000000000000006e-11  img/s: 2.3534036632798174  loss: 2.3865 (2.4070)  acc1: 7.8125 (6.7708)  acc5: 40.6250 (41.1458)  time: 27.5061  data: 0.0391\n",
      "Epoch: [7]  [3/8]  eta: 0:02:17  lr: 1.0000000000000006e-11  img/s: 2.3356416906987447  loss: 2.3765 (2.3919)  acc1: 7.8125 (7.8125)  acc5: 40.6250 (43.3594)  time: 27.4955  data: 0.0449\n",
      "Epoch: [7]  [4/8]  eta: 0:01:49  lr: 1.0000000000000006e-11  img/s: 2.3490058152879234  loss: 2.3765 (2.3848)  acc1: 7.8125 (7.8125)  acc5: 46.8750 (45.0000)  time: 27.4543  data: 0.0447\n",
      "Epoch: [7]  [5/8]  eta: 0:01:22  lr: 1.0000000000000006e-11  img/s: 2.3473606772951507  loss: 2.3704 (2.3824)  acc1: 7.8125 (8.3333)  acc5: 46.8750 (46.3542)  time: 27.4294  data: 0.0439\n",
      "Epoch: [7]  [6/8]  eta: 0:00:54  lr: 1.0000000000000006e-11  img/s: 2.3854001160107496  loss: 2.3704 (2.3751)  acc1: 9.3750 (8.4821)  acc5: 50.0000 (47.9911)  time: 27.3492  data: 0.0431\n",
      "Epoch: [7]  [7/8]  eta: 0:00:26  lr: 1.0000000000000006e-11  img/s: 2.3211325166544734  loss: 2.3704 (2.3761)  acc1: 7.8125 (8.2000)  acc5: 50.0000 (48.6000)  time: 26.7346  data: 0.0414\n",
      "Epoch: [7] Total time: 0:03:33\n",
      "Epoch 9/15\n",
      "Epoch: [8]  [0/8]  eta: 0:03:32  lr: 1.0000000000000006e-12  img/s: 2.4138953737771724  loss: 2.3733 (2.3733)  acc1: 9.3750 (9.3750)  acc5: 51.5625 (51.5625)  time: 26.5477  data: 0.0346\n",
      "Epoch: [8]  [1/8]  eta: 0:03:08  lr: 1.0000000000000006e-12  img/s: 2.3579961293036846  loss: 2.3324 (2.3528)  acc1: 9.3750 (10.1562)  acc5: 51.5625 (53.1250)  time: 26.8642  data: 0.0368\n",
      "Epoch: [8]  [2/8]  eta: 0:02:43  lr: 1.0000000000000006e-12  img/s: 2.2900770936569943  loss: 2.3733 (2.3773)  acc1: 9.3750 (9.8958)  acc5: 51.5625 (50.0000)  time: 27.2389  data: 0.0384\n",
      "Epoch: [8]  [3/8]  eta: 0:02:17  lr: 1.0000000000000006e-12  img/s: 2.283618806254387  loss: 2.3362 (2.3670)  acc1: 9.3750 (10.5469)  acc5: 51.5625 (51.5625)  time: 27.4499  data: 0.0431\n",
      "Epoch: [8]  [4/8]  eta: 0:01:50  lr: 1.0000000000000006e-12  img/s: 2.311742455336645  loss: 2.3733 (2.3744)  acc1: 9.3750 (9.6875)  acc5: 53.1250 (51.8750)  time: 27.5041  data: 0.0417\n",
      "Epoch: [8]  [5/8]  eta: 0:01:22  lr: 1.0000000000000006e-12  img/s: 2.3449031652771444  loss: 2.3733 (2.3797)  acc1: 9.3750 (9.6354)  acc5: 51.5625 (50.0000)  time: 27.4794  data: 0.0452\n",
      "Epoch: [8]  [6/8]  eta: 0:00:54  lr: 1.0000000000000006e-12  img/s: 2.3258791963956544  loss: 2.3733 (2.3774)  acc1: 9.3750 (9.1518)  acc5: 53.1250 (50.4464)  time: 27.4910  data: 0.0450\n",
      "Epoch: [8]  [7/8]  eta: 0:00:26  lr: 1.0000000000000006e-12  img/s: 2.2612732326086578  loss: 2.3733 (2.3825)  acc1: 9.3750 (9.4000)  acc5: 51.5625 (49.8000)  time: 26.9325  data: 0.0428\n",
      "Epoch: [8] Total time: 0:03:35\n",
      "Epoch 10/15\n",
      "Epoch: [9]  [0/8]  eta: 0:03:36  lr: 1.0000000000000007e-13  img/s: 2.3723755295060984  loss: 2.3927 (2.3927)  acc1: 9.3750 (9.3750)  acc5: 39.0625 (39.0625)  time: 27.0131  data: 0.0359\n",
      "Epoch: [9]  [1/8]  eta: 0:03:09  lr: 1.0000000000000007e-13  img/s: 2.3591614886293812  loss: 2.2344 (2.3135)  acc1: 9.3750 (14.0625)  acc5: 39.0625 (54.6875)  time: 27.0963  data: 0.0436\n",
      "Epoch: [9]  [2/8]  eta: 0:02:42  lr: 1.0000000000000007e-13  img/s: 2.356404197297082  loss: 2.3844 (2.3372)  acc1: 9.3750 (12.5000)  acc5: 46.8750 (52.0833)  time: 27.1298  data: 0.0413\n",
      "Epoch: [9]  [3/8]  eta: 0:02:15  lr: 1.0000000000000007e-13  img/s: 2.405978364094702  loss: 2.3677 (2.3448)  acc1: 9.3750 (11.3281)  acc5: 46.8750 (52.7344)  time: 27.0116  data: 0.0451\n",
      "Epoch: [9]  [4/8]  eta: 0:01:47  lr: 1.0000000000000007e-13  img/s: 2.380307504331073  loss: 2.3677 (2.3492)  acc1: 9.3750 (10.6250)  acc5: 46.8750 (51.5625)  time: 26.9940  data: 0.0433\n",
      "Epoch: [9]  [5/8]  eta: 0:01:21  lr: 1.0000000000000007e-13  img/s: 2.368333723386255  loss: 2.3677 (2.3655)  acc1: 9.3750 (10.4167)  acc5: 46.8750 (50.2604)  time: 27.0060  data: 0.0432\n",
      "Epoch: [9]  [6/8]  eta: 0:00:53  lr: 1.0000000000000007e-13  img/s: 2.3893079737934655  loss: 2.3677 (2.3599)  acc1: 9.3750 (10.2679)  acc5: 46.8750 (50.6696)  time: 26.9799  data: 0.0424\n",
      "Epoch: [9]  [7/8]  eta: 0:00:26  lr: 1.0000000000000007e-13  img/s: 2.3215983958876087  loss: 2.3677 (2.3631)  acc1: 9.3750 (10.6000)  acc5: 46.8750 (50.2000)  time: 26.4115  data: 0.0414\n",
      "Epoch: [9] Total time: 0:03:31\n",
      "Epoch 11/15\n",
      "Epoch: [10]  [0/8]  eta: 0:03:33  lr: 1.0000000000000008e-14  img/s: 2.4047175114110004  loss: 2.3324 (2.3324)  acc1: 14.0625 (14.0625)  acc5: 46.8750 (46.8750)  time: 26.6625  data: 0.0481\n",
      "Epoch: [10]  [1/8]  eta: 0:03:08  lr: 1.0000000000000008e-14  img/s: 2.35483845867196  loss: 2.2566 (2.2945)  acc1: 14.0625 (17.1875)  acc5: 46.8750 (53.1250)  time: 26.9391  data: 0.0429\n",
      "Epoch: [10]  [2/8]  eta: 0:02:44  lr: 1.0000000000000008e-14  img/s: 2.2619714510338302  loss: 2.3324 (2.3213)  acc1: 14.0625 (14.5833)  acc5: 46.8750 (50.0000)  time: 27.4023  data: 0.0402\n",
      "Epoch: [10]  [3/8]  eta: 0:02:18  lr: 1.0000000000000008e-14  img/s: 2.268981969064404  loss: 2.3324 (2.3289)  acc1: 14.0625 (14.8438)  acc5: 46.8750 (50.3906)  time: 27.6108  data: 0.0376\n",
      "Epoch: [10]  [4/8]  eta: 0:01:51  lr: 1.0000000000000008e-14  img/s: 2.1929473052245623  loss: 2.3517 (2.3363)  acc1: 14.0625 (12.8125)  acc5: 51.5625 (50.9375)  time: 27.9370  data: 0.0415\n",
      "Epoch: [10]  [5/8]  eta: 0:01:23  lr: 1.0000000000000008e-14  img/s: 2.3794861561405285  loss: 2.3402 (2.3370)  acc1: 10.9375 (12.5000)  acc5: 51.5625 (51.3021)  time: 27.7690  data: 0.0400\n",
      "Epoch: [10]  [6/8]  eta: 0:00:55  lr: 1.0000000000000008e-14  img/s: 2.347627883627225  loss: 2.3402 (2.3338)  acc1: 12.5000 (12.5000)  acc5: 51.5625 (51.3393)  time: 27.7021  data: 0.0399\n",
      "Epoch: [10]  [7/8]  eta: 0:00:27  lr: 1.0000000000000008e-14  img/s: 2.2757395685352413  loss: 2.3402 (2.3368)  acc1: 12.5000 (12.6000)  acc5: 51.5625 (51.0000)  time: 27.0999  data: 0.0393\n",
      "Epoch: [10] Total time: 0:03:36\n",
      "Epoch 12/15\n",
      "Epoch: [11]  [0/8]  eta: 0:03:30  lr: 1.0000000000000009e-15  img/s: 2.429650748989812  loss: 2.3896 (2.3896)  acc1: 7.8125 (7.8125)  acc5: 42.1875 (42.1875)  time: 26.3729  data: 0.0317\n",
      "Epoch: [11]  [1/8]  eta: 0:03:07  lr: 1.0000000000000009e-15  img/s: 2.3497016404370785  loss: 2.3493 (2.3695)  acc1: 7.8125 (9.3750)  acc5: 42.1875 (46.0938)  time: 26.8343  data: 0.0449\n",
      "Epoch: [11]  [2/8]  eta: 0:02:42  lr: 1.0000000000000009e-15  img/s: 2.3434754353452085  loss: 2.3896 (2.3952)  acc1: 7.8125 (8.3333)  acc5: 42.1875 (44.7917)  time: 27.0124  data: 0.0495\n",
      "Epoch: [11]  [3/8]  eta: 0:02:15  lr: 1.0000000000000009e-15  img/s: 2.330799236547002  loss: 2.3896 (2.4020)  acc1: 6.2500 (7.8125)  acc5: 42.1875 (45.3125)  time: 27.1336  data: 0.0469\n",
      "Epoch: [11]  [4/8]  eta: 0:01:48  lr: 1.0000000000000009e-15  img/s: 2.343461973508095  loss: 2.3931 (2.4002)  acc1: 7.8125 (8.1250)  acc5: 42.1875 (44.3750)  time: 27.1780  data: 0.0466\n",
      "Epoch: [11]  [5/8]  eta: 0:01:21  lr: 1.0000000000000009e-15  img/s: 2.359860294679312  loss: 2.3896 (2.3950)  acc1: 7.8125 (8.3333)  acc5: 42.1875 (44.0104)  time: 27.1745  data: 0.0449\n",
      "Epoch: [11]  [6/8]  eta: 0:00:54  lr: 1.0000000000000009e-15  img/s: 2.3589487392482416  loss: 2.3896 (2.3904)  acc1: 7.8125 (8.0357)  acc5: 42.1875 (45.3125)  time: 27.1753  data: 0.0455\n",
      "Epoch: [11]  [7/8]  eta: 0:00:26  lr: 1.0000000000000009e-15  img/s: 2.2989600639135577  loss: 2.3690 (2.3817)  acc1: 7.8125 (8.2000)  acc5: 42.1875 (46.8000)  time: 26.6097  data: 0.0438\n",
      "Epoch: [11] Total time: 0:03:32\n",
      "Epoch 13/15\n",
      "Epoch: [12]  [0/8]  eta: 0:03:36  lr: 1.000000000000001e-16  img/s: 2.365667996054051  loss: 2.3237 (2.3237)  acc1: 9.3750 (9.3750)  acc5: 65.6250 (65.6250)  time: 27.0889  data: 0.0352\n",
      "Epoch: [12]  [1/8]  eta: 0:03:11  lr: 1.000000000000001e-16  img/s: 2.329618253685507  loss: 2.2765 (2.3001)  acc1: 9.3750 (12.5000)  acc5: 59.3750 (62.5000)  time: 27.2971  data: 0.0341\n",
      "Epoch: [12]  [2/8]  eta: 0:02:44  lr: 1.000000000000001e-16  img/s: 2.3222361744651763  loss: 2.3237 (2.3202)  acc1: 9.3750 (11.4583)  acc5: 59.3750 (58.3333)  time: 27.3969  data: 0.0350\n",
      "Epoch: [12]  [3/8]  eta: 0:02:19  lr: 1.000000000000001e-16  img/s: 2.1857692321472135  loss: 2.3237 (2.3220)  acc1: 9.3750 (11.3281)  acc5: 57.8125 (58.2031)  time: 27.8783  data: 0.0368\n",
      "Epoch: [12]  [4/8]  eta: 0:01:50  lr: 1.000000000000001e-16  img/s: 2.363413978819582  loss: 2.3276 (2.3245)  acc1: 10.9375 (12.5000)  acc5: 57.8125 (55.3125)  time: 27.7267  data: 0.0377\n",
      "Epoch: [12]  [5/8]  eta: 0:01:23  lr: 1.000000000000001e-16  img/s: 2.304701932534384  loss: 2.3276 (2.3325)  acc1: 9.3750 (11.7188)  acc5: 50.0000 (54.1667)  time: 27.7422  data: 0.0398\n",
      "Epoch: [12]  [6/8]  eta: 0:00:55  lr: 1.000000000000001e-16  img/s: 2.2935868358737648  loss: 2.3344 (2.3377)  acc1: 10.9375 (11.6071)  acc5: 50.0000 (52.9018)  time: 27.7715  data: 0.0402\n",
      "Epoch: [12]  [7/8]  eta: 0:00:27  lr: 1.000000000000001e-16  img/s: 2.295751647333568  loss: 2.3344 (2.3413)  acc1: 10.9375 (11.8000)  acc5: 48.4375 (51.6000)  time: 27.1353  data: 0.0391\n",
      "Epoch: [12] Total time: 0:03:37\n",
      "Epoch 14/15\n",
      "Epoch: [13]  [0/8]  eta: 0:03:34  lr: 1.000000000000001e-17  img/s: 2.387758111668008  loss: 2.3813 (2.3813)  acc1: 6.2500 (6.2500)  acc5: 43.7500 (43.7500)  time: 26.8740  data: 0.0706\n",
      "Epoch: [13]  [1/8]  eta: 0:03:09  lr: 1.000000000000001e-17  img/s: 2.3455894495550926  loss: 2.3009 (2.3411)  acc1: 6.2500 (7.8125)  acc5: 43.7500 (50.0000)  time: 27.0986  data: 0.0543\n",
      "Epoch: [13]  [2/8]  eta: 0:02:42  lr: 1.000000000000001e-17  img/s: 2.3499256850997603  loss: 2.3813 (2.3666)  acc1: 9.3750 (8.3333)  acc5: 48.4375 (49.4792)  time: 27.1574  data: 0.0495\n",
      "Epoch: [13]  [3/8]  eta: 0:02:15  lr: 1.000000000000001e-17  img/s: 2.3677913254279424  loss: 2.3729 (2.3682)  acc1: 7.8125 (8.2031)  acc5: 46.8750 (48.8281)  time: 27.1370  data: 0.0488\n",
      "Epoch: [13]  [4/8]  eta: 0:01:48  lr: 1.000000000000001e-17  img/s: 2.3430336486939165  loss: 2.3729 (2.3667)  acc1: 7.8125 (7.5000)  acc5: 48.4375 (48.7500)  time: 27.1828  data: 0.0492\n",
      "Epoch: [13]  [5/8]  eta: 0:01:21  lr: 1.000000000000001e-17  img/s: 2.316015780786276  loss: 2.3609 (2.3655)  acc1: 7.8125 (7.8125)  acc5: 46.8750 (47.9167)  time: 27.2646  data: 0.0476\n",
      "Epoch: [13]  [6/8]  eta: 0:00:54  lr: 1.000000000000001e-17  img/s: 2.3231925755171963  loss: 2.3609 (2.3596)  acc1: 9.3750 (9.1518)  acc5: 48.4375 (49.3304)  time: 27.3103  data: 0.0460\n",
      "Epoch: [13]  [7/8]  eta: 0:00:26  lr: 1.000000000000001e-17  img/s: 2.326219112981103  loss: 2.3609 (2.3653)  acc1: 7.8125 (9.0000)  acc5: 46.8750 (49.0000)  time: 26.6947  data: 0.0442\n",
      "Epoch: [13] Total time: 0:03:33\n",
      "Epoch 15/15\n",
      "Epoch: [14]  [0/8]  eta: 0:03:31  lr: 1.000000000000001e-18  img/s: 2.421639510538352  loss: 2.2962 (2.2962)  acc1: 9.3750 (9.3750)  acc5: 59.3750 (59.3750)  time: 26.4650  data: 0.0366\n",
      "Epoch: [14]  [1/8]  eta: 0:03:07  lr: 1.000000000000001e-18  img/s: 2.367359552610505  loss: 2.2653 (2.2807)  acc1: 9.3750 (12.5000)  acc5: 54.6875 (57.0312)  time: 26.7713  data: 0.0399\n",
      "Epoch: [14]  [2/8]  eta: 0:02:41  lr: 1.000000000000001e-18  img/s: 2.3409001890288845  loss: 2.2962 (2.3063)  acc1: 9.3750 (11.4583)  acc5: 54.6875 (56.2500)  time: 26.9729  data: 0.0387\n",
      "Epoch: [14]  [3/8]  eta: 0:02:15  lr: 1.000000000000001e-18  img/s: 2.3654019200825687  loss: 2.2962 (2.3180)  acc1: 9.3750 (12.5000)  acc5: 54.6875 (54.6875)  time: 27.0027  data: 0.0378\n",
      "Epoch: [14]  [4/8]  eta: 0:01:48  lr: 1.000000000000001e-18  img/s: 2.325983814053457  loss: 2.3531 (2.3256)  acc1: 9.3750 (11.8750)  acc5: 54.6875 (54.3750)  time: 27.1174  data: 0.0425\n",
      "Epoch: [14]  [5/8]  eta: 0:01:21  lr: 1.000000000000001e-18  img/s: 2.319748786760526  loss: 2.3531 (2.3332)  acc1: 9.3750 (12.2396)  acc5: 53.1250 (53.3854)  time: 27.2020  data: 0.0414\n",
      "Epoch: [14]  [6/8]  eta: 0:00:54  lr: 1.000000000000001e-18  img/s: 2.327413449559621  loss: 2.3531 (2.3318)  acc1: 12.5000 (12.2768)  acc5: 53.1250 (53.3482)  time: 27.2504  data: 0.0416\n",
      "Epoch: [14]  [7/8]  eta: 0:00:26  lr: 1.000000000000001e-18  img/s: 2.308295788727037  loss: 2.3531 (2.3355)  acc1: 9.6154 (12.0000)  acc5: 53.1250 (53.6000)  time: 26.6661  data: 0.0424\n",
      "Epoch: [14] Total time: 0:03:33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 250/250 [25:19<00:00,  6.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1519.267150871019\n",
      "Accuracy of the network on the 10000 test images: 9.7500 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "axx_mult = 'appro2'\n",
    "model = resnet18(pretrained=True, axx_mult = axx_mult)\n",
    "model.eval() # for evaluation\n",
    "# It is a bit slow since we collect histograms on CPU\n",
    "with torch.no_grad():\n",
    "    stats = collect_stats(model, data_t, num_batches=2)\n",
    "    amax = compute_amax(model, method=\"percentile\", percentile=99.99)\n",
    "    \n",
    "# Inference without retraining\n",
    "import timeit\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "model.eval()\n",
    "start_time = timeit.default_timer()\n",
    "with torch.no_grad():\n",
    "    for iteraction, (images, labels) in tqdm(enumerate(data), total=len(data)):\n",
    "        images, labels = images.to(\"cpu\"), labels.to(\"cpu\")\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(timeit.default_timer() - start_time)\n",
    "print('Accuracy of the network on the 10000 test images: %.4f %%' % (\n",
    "    100 * correct / total))\n",
    "\n",
    "# Retraining for 15 epochs\n",
    "from adapt.references.classification.train import evaluate, train_one_epoch, load_data\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "for epoch in range(15):\n",
    "    print(f\"Epoch {epoch + 1}/15\")\n",
    "    train_one_epoch(model, criterion, optimizer, data_t, \"cpu\", epoch, 1)\n",
    "    lr_scheduler.step()\n",
    "\n",
    "# Inference after retraining\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "model.eval()\n",
    "start_time = timeit.default_timer()\n",
    "with torch.no_grad():\n",
    "    for iteraction, (images, labels) in tqdm(enumerate(data), total=len(data)):\n",
    "        images, labels = images.to(\"cpu\"), labels.to(\"cpu\")\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(timeit.default_timer() - start_time)\n",
    "print('Accuracy of the network on the 10000 test images: %.4f %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6ca82eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/PyInit_conv2d_appro3/build.ninja...\n",
      "Building extension module PyInit_conv2d_appro3...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:04<00:00,  2.45s/it]\n",
      "W0222 18:59:41.352475 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:41.353087 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:41.353557 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:41.353981 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:41.354461 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:41.354884 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:41.356292 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:41.356751 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:41.357200 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:41.357582 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:41.358002 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:41.358402 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:41.358807 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:41.359192 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:41.359606 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:41.360004 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:41.360452 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:41.360900 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:41.361342 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:41.361772 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:41.362211 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:41.362644 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:41.363028 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:41.363294 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:41.364570 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:41.364850 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:41.365137 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:41.365412 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:41.365695 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:41.365966 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:41.366245 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:41.366512 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:41.366783 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:41.367047 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:41.367319 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:41.367600 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:41.367881 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:41.368148 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:41.368419 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:41.368683 140027934322880 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 18:59:41.370101 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:41.370794 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:41.371614 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:41.372416 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:41.373217 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:41.373996 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:41.374781 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:41.375560 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:41.376342 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:41.377114 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:41.377912 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:41.378678 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:41.379468 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:41.380250 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:41.381048 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:41.381817 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:41.382598 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:41.383367 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:41.384166 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:41.384934 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:41.385722 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:41.386489 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:41.387270 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:41.388039 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:41.388813 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:41.389577 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:41.390358 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:41.391128 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:41.391931 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:41.392715 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:41.393493 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:41.394260 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:41.395038 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:41.395832 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:41.396601 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:41.397373 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:41.398149 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:41.398927 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:41.399698 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 18:59:41.400471 140027934322880 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.quantizer                         : TensorQuantizer(8bit per-tensor amax=2.5903 calibrator=HistogramCalibrator quant)\n",
      "conv1.quantizer_w                       : TensorQuantizer(8bit per-tensor amax=0.1513 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.6102 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0620 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3062 calibrator=HistogramCalibrator quant)\n",
      "layer1.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0380 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.5005 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0354 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2481 calibrator=HistogramCalibrator quant)\n",
      "layer1.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0360 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.4762 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0296 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2313 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0280 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.4762 calibrator=HistogramCalibrator quant)\n",
      "layer2.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0738 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2751 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0287 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1851 calibrator=HistogramCalibrator quant)\n",
      "layer2.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0247 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.3046 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0197 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1440 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0141 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.3046 calibrator=HistogramCalibrator quant)\n",
      "layer3.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0219 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1375 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0110 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1254 calibrator=HistogramCalibrator quant)\n",
      "layer3.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0075 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=0.1969 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0033 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.2146 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0062 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.downsample.0.quantizer         : TensorQuantizer(8bit per-tensor amax=0.1969 calibrator=HistogramCalibrator quant)\n",
      "layer4.0.downsample.0.quantizer_w       : TensorQuantizer(8bit per-tensor amax=0.0127 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv1.quantizer                : TensorQuantizer(8bit per-tensor amax=1.2856 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv1.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0022 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv2.quantizer                : TensorQuantizer(8bit per-tensor amax=0.7925 calibrator=HistogramCalibrator quant)\n",
      "layer4.1.conv2.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.0067 calibrator=HistogramCalibrator quant)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [06:07<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "367.93398952670395\n",
      "Accuracy of the network on the 10000 test images: 10.0000 %\n",
      "Epoch 1/15\n",
      "Epoch: [0]  [0/8]  eta: 0:01:25  lr: 0.0001  img/s: 6.000115292499507  loss: 2.0273 (2.0273)  acc1: 32.8125 (32.8125)  acc5: 73.4375 (73.4375)  time: 10.6880  data: 0.0216\n",
      "Epoch: [0]  [1/8]  eta: 0:01:14  lr: 0.0001  img/s: 6.090958676034975  loss: 2.0273 (2.0401)  acc1: 29.6875 (31.2500)  acc5: 73.4375 (75.7812)  time: 10.6089  data: 0.0220\n",
      "Epoch: [0]  [2/8]  eta: 0:01:03  lr: 0.0001  img/s: 6.117231697513587  loss: 2.0273 (2.0067)  acc1: 32.8125 (32.2917)  acc5: 78.1250 (78.6458)  time: 10.5675  data: 0.0221\n",
      "Epoch: [0]  [3/8]  eta: 0:00:52  lr: 0.0001  img/s: 6.112930398837203  loss: 2.0273 (2.0228)  acc1: 31.2500 (32.0312)  acc5: 73.4375 (77.3438)  time: 10.5486  data: 0.0222\n",
      "Epoch: [0]  [4/8]  eta: 0:00:42  lr: 0.0001  img/s: 6.167621200552706  loss: 2.0273 (1.9993)  acc1: 32.8125 (33.4375)  acc5: 75.0000 (76.8750)  time: 10.5187  data: 0.0222\n",
      "Epoch: [0]  [5/8]  eta: 0:00:31  lr: 0.0001  img/s: 6.182528161565434  loss: 2.0273 (2.0059)  acc1: 31.2500 (32.5521)  acc5: 75.0000 (77.0833)  time: 10.4945  data: 0.0221\n",
      "Epoch: [0]  [6/8]  eta: 0:00:21  lr: 0.0001  img/s: 6.070946832020973  loss: 2.0328 (2.0097)  acc1: 31.2500 (32.3661)  acc5: 76.5625 (77.0089)  time: 10.5044  data: 0.0220\n",
      "Epoch: [0]  [7/8]  eta: 0:00:10  lr: 0.0001  img/s: 5.834362166750649  loss: 2.0328 (2.0257)  acc1: 31.2500 (32.2000)  acc5: 75.0000 (76.6000)  time: 10.3077  data: 0.0216\n",
      "Epoch: [0] Total time: 0:01:22\n",
      "Epoch 2/15\n",
      "Epoch: [1]  [0/8]  eta: 0:01:23  lr: 1e-05  img/s: 6.1247330622789615  loss: 1.6681 (1.6681)  acc1: 45.3125 (45.3125)  acc5: 87.5000 (87.5000)  time: 10.4711  data: 0.0216\n",
      "Epoch: [1]  [1/8]  eta: 0:01:13  lr: 1e-05  img/s: 6.118412523602294  loss: 1.6681 (1.6895)  acc1: 43.7500 (44.5312)  acc5: 87.5000 (89.0625)  time: 10.4792  data: 0.0243\n",
      "Epoch: [1]  [2/8]  eta: 0:01:02  lr: 1e-05  img/s: 6.137019814194487  loss: 1.6681 (1.6326)  acc1: 45.3125 (46.8750)  acc5: 89.0625 (89.0625)  time: 10.4697  data: 0.0237\n",
      "Epoch: [1]  [3/8]  eta: 0:00:52  lr: 1e-05  img/s: 6.147229978398226  loss: 1.6681 (1.6914)  acc1: 43.7500 (46.0938)  acc5: 87.5000 (88.2812)  time: 10.4607  data: 0.0234\n",
      "Epoch: [1]  [4/8]  eta: 0:00:41  lr: 1e-05  img/s: 6.176501084978626  loss: 1.7108 (1.6967)  acc1: 43.7500 (45.6250)  acc5: 87.5000 (88.1250)  time: 10.4454  data: 0.0232\n",
      "Epoch: [1]  [5/8]  eta: 0:00:31  lr: 1e-05  img/s: 6.152811760403474  loss: 1.7108 (1.7008)  acc1: 43.7500 (45.8333)  acc5: 87.5000 (88.0208)  time: 10.4417  data: 0.0229\n",
      "Epoch: [1]  [6/8]  eta: 0:00:20  lr: 1e-05  img/s: 6.176485452179356  loss: 1.7108 (1.6999)  acc1: 43.7500 (45.0893)  acc5: 87.5000 (87.9464)  time: 10.4334  data: 0.0227\n",
      "Epoch: [1]  [7/8]  eta: 0:00:10  lr: 1e-05  img/s: 5.889634815262178  loss: 1.7108 (1.7343)  acc1: 43.7500 (43.6000)  acc5: 87.5000 (87.0000)  time: 10.2352  data: 0.0222\n",
      "Epoch: [1] Total time: 0:01:21\n",
      "Epoch 3/15\n",
      "Epoch: [2]  [0/8]  eta: 0:01:24  lr: 1.0000000000000002e-06  img/s: 6.110790850431423  loss: 1.6152 (1.6152)  acc1: 39.0625 (39.0625)  acc5: 89.0625 (89.0625)  time: 10.5207  data: 0.0474\n",
      "Epoch: [2]  [1/8]  eta: 0:01:13  lr: 1.0000000000000002e-06  img/s: 6.1470162925710525  loss: 1.5483 (1.5817)  acc1: 39.0625 (46.8750)  acc5: 89.0625 (90.6250)  time: 10.4774  data: 0.0350\n",
      "Epoch: [2]  [2/8]  eta: 0:01:02  lr: 1.0000000000000002e-06  img/s: 6.158811040261764  loss: 1.5483 (1.5636)  acc1: 54.6875 (49.4792)  acc5: 89.0625 (89.0625)  time: 10.4561  data: 0.0306\n",
      "Epoch: [2]  [3/8]  eta: 0:00:52  lr: 1.0000000000000002e-06  img/s: 6.185707322281014  loss: 1.5483 (1.6483)  acc1: 39.0625 (45.7031)  acc5: 85.9375 (87.1094)  time: 10.4342  data: 0.0285\n",
      "Epoch: [2]  [4/8]  eta: 0:00:41  lr: 1.0000000000000002e-06  img/s: 6.152656209724506  loss: 1.6152 (1.6455)  acc1: 48.4375 (46.2500)  acc5: 89.0625 (87.5000)  time: 10.4321  data: 0.0272\n",
      "Epoch: [2]  [5/8]  eta: 0:00:31  lr: 1.0000000000000002e-06  img/s: 6.189549325668037  loss: 1.6152 (1.6584)  acc1: 39.0625 (45.0521)  acc5: 85.9375 (86.9792)  time: 10.4204  data: 0.0262\n",
      "Epoch: [2]  [6/8]  eta: 0:00:20  lr: 1.0000000000000002e-06  img/s: 6.186055283677516  loss: 1.6342 (1.6560)  acc1: 45.3125 (45.0893)  acc5: 89.0625 (87.2768)  time: 10.4129  data: 0.0257\n",
      "Epoch: [2]  [7/8]  eta: 0:00:10  lr: 1.0000000000000002e-06  img/s: 5.888742085402873  loss: 1.6342 (1.6946)  acc1: 39.0625 (44.2000)  acc5: 85.9375 (86.0000)  time: 10.2174  data: 0.0248\n",
      "Epoch: [2] Total time: 0:01:21\n",
      "Epoch 4/15\n",
      "Epoch: [3]  [0/8]  eta: 0:01:23  lr: 1.0000000000000002e-07  img/s: 6.181762602710732  loss: 1.6592 (1.6592)  acc1: 43.7500 (43.7500)  acc5: 89.0625 (89.0625)  time: 10.3754  data: 0.0224\n",
      "Epoch: [3]  [1/8]  eta: 0:01:13  lr: 1.0000000000000002e-07  img/s: 6.1002189493906185  loss: 1.6074 (1.6333)  acc1: 43.7500 (48.4375)  acc5: 89.0625 (90.6250)  time: 10.4465  data: 0.0243\n",
      "Epoch: [3]  [2/8]  eta: 0:01:02  lr: 1.0000000000000002e-07  img/s: 6.125732817227119  loss: 1.6592 (1.6456)  acc1: 43.7500 (46.8750)  acc5: 89.0625 (89.5833)  time: 10.4544  data: 0.0236\n",
      "Epoch: [3]  [3/8]  eta: 0:00:52  lr: 1.0000000000000002e-07  img/s: 6.147425236606251  loss: 1.6592 (1.6857)  acc1: 43.7500 (44.9219)  acc5: 87.5000 (89.0625)  time: 10.4491  data: 0.0233\n",
      "Epoch: [3]  [4/8]  eta: 0:00:41  lr: 1.0000000000000002e-07  img/s: 6.143560812646374  loss: 1.6592 (1.6747)  acc1: 43.7500 (46.2500)  acc5: 89.0625 (89.0625)  time: 10.4471  data: 0.0230\n",
      "Epoch: [3]  [5/8]  eta: 0:00:31  lr: 1.0000000000000002e-07  img/s: 6.1356807486626375  loss: 1.6592 (1.6802)  acc1: 43.7500 (45.3125)  acc5: 87.5000 (88.2812)  time: 10.4481  data: 0.0229\n",
      "Epoch: [3]  [6/8]  eta: 0:00:20  lr: 1.0000000000000002e-07  img/s: 6.144865199945564  loss: 1.6703 (1.6888)  acc1: 43.7500 (44.6429)  acc5: 87.5000 (88.1696)  time: 10.4466  data: 0.0228\n",
      "Epoch: [3]  [7/8]  eta: 0:00:10  lr: 1.0000000000000002e-07  img/s: 5.837846356776387  loss: 1.6703 (1.7135)  acc1: 40.6250 (44.2000)  acc5: 87.5000 (86.6000)  time: 10.2565  data: 0.0222\n",
      "Epoch: [3] Total time: 0:01:22\n",
      "Epoch 5/15\n",
      "Epoch: [4]  [0/8]  eta: 0:01:24  lr: 1.0000000000000004e-08  img/s: 6.0760214505024255  loss: 1.6332 (1.6332)  acc1: 43.7500 (43.7500)  acc5: 85.9375 (85.9375)  time: 10.5549  data: 0.0217\n",
      "Epoch: [4]  [1/8]  eta: 0:01:13  lr: 1.0000000000000004e-08  img/s: 6.124936257395699  loss: 1.5811 (1.6071)  acc1: 43.7500 (47.6562)  acc5: 85.9375 (89.8438)  time: 10.5131  data: 0.0219\n",
      "Epoch: [4]  [2/8]  eta: 0:01:02  lr: 1.0000000000000004e-08  img/s: 6.125625460359591  loss: 1.5874 (1.6006)  acc1: 51.5625 (49.4792)  acc5: 85.9375 (88.5417)  time: 10.4988  data: 0.0221\n",
      "Epoch: [4]  [3/8]  eta: 0:00:52  lr: 1.0000000000000004e-08  img/s: 6.143330932242907  loss: 1.5874 (1.6555)  acc1: 43.7500 (45.3125)  acc5: 85.9375 (87.8906)  time: 10.4841  data: 0.0221\n",
      "Epoch: [4]  [4/8]  eta: 0:00:41  lr: 1.0000000000000004e-08  img/s: 6.149093954243818  loss: 1.6332 (1.6630)  acc1: 43.7500 (44.6875)  acc5: 85.9375 (87.8125)  time: 10.4734  data: 0.0222\n",
      "Epoch: [4]  [5/8]  eta: 0:00:31  lr: 1.0000000000000004e-08  img/s: 6.154442483796569  loss: 1.6332 (1.6800)  acc1: 42.1875 (43.2292)  acc5: 85.9375 (86.9792)  time: 10.4648  data: 0.0222\n",
      "Epoch: [4]  [6/8]  eta: 0:00:20  lr: 1.0000000000000004e-08  img/s: 6.153715181598879  loss: 1.6685 (1.6784)  acc1: 42.1875 (42.8571)  acc5: 85.9375 (87.9464)  time: 10.4587  data: 0.0222\n",
      "Epoch: [4]  [7/8]  eta: 0:00:10  lr: 1.0000000000000004e-08  img/s: 5.7844143704940505  loss: 1.6685 (1.7103)  acc1: 40.6250 (42.4000)  acc5: 85.9375 (86.8000)  time: 10.2774  data: 0.0218\n",
      "Epoch: [4] Total time: 0:01:22\n",
      "Epoch 6/15\n",
      "Epoch: [5]  [0/8]  eta: 0:01:24  lr: 1.0000000000000005e-09  img/s: 6.065997333257104  loss: 1.6874 (1.6874)  acc1: 39.0625 (39.0625)  acc5: 87.5000 (87.5000)  time: 10.5966  data: 0.0460\n",
      "Epoch: [5]  [1/8]  eta: 0:01:13  lr: 1.0000000000000005e-09  img/s: 6.089522348464818  loss: 1.6186 (1.6530)  acc1: 39.0625 (42.9688)  acc5: 87.5000 (89.8438)  time: 10.5644  data: 0.0341\n",
      "Epoch: [5]  [2/8]  eta: 0:01:03  lr: 1.0000000000000005e-09  img/s: 6.099820974359049  loss: 1.6186 (1.6166)  acc1: 46.8750 (45.3125)  acc5: 87.5000 (88.5417)  time: 10.5478  data: 0.0303\n",
      "Epoch: [5]  [3/8]  eta: 0:00:52  lr: 1.0000000000000005e-09  img/s: 6.118237092811768  loss: 1.6186 (1.6570)  acc1: 39.0625 (42.9688)  acc5: 85.9375 (87.8906)  time: 10.5316  data: 0.0283\n",
      "Epoch: [5]  [4/8]  eta: 0:00:42  lr: 1.0000000000000005e-09  img/s: 6.096829576783287  loss: 1.6874 (1.6635)  acc1: 42.1875 (42.8125)  acc5: 87.5000 (88.1250)  time: 10.5292  data: 0.0272\n",
      "Epoch: [5]  [5/8]  eta: 0:00:31  lr: 1.0000000000000005e-09  img/s: 6.11421596129839  loss: 1.6874 (1.6836)  acc1: 40.6250 (42.4479)  acc5: 85.9375 (86.7188)  time: 10.5227  data: 0.0264\n",
      "Epoch: [5]  [6/8]  eta: 0:00:21  lr: 1.0000000000000005e-09  img/s: 6.13774822729049  loss: 1.6893 (1.6927)  acc1: 40.6250 (42.1875)  acc5: 85.9375 (86.6071)  time: 10.5122  data: 0.0258\n",
      "Epoch: [5]  [7/8]  eta: 0:00:10  lr: 1.0000000000000005e-09  img/s: 5.818884369927741  loss: 1.6893 (1.7207)  acc1: 40.6250 (40.8000)  acc5: 85.9375 (85.8000)  time: 10.3176  data: 0.0249\n",
      "Epoch: [5] Total time: 0:01:22\n",
      "Epoch 7/15\n",
      "Epoch: [6]  [0/8]  eta: 0:01:24  lr: 1.0000000000000006e-10  img/s: 6.084089781507127  loss: 1.5486 (1.5486)  acc1: 50.0000 (50.0000)  acc5: 85.9375 (85.9375)  time: 10.5676  data: 0.0484\n",
      "Epoch: [6]  [1/8]  eta: 0:01:13  lr: 1.0000000000000006e-10  img/s: 6.124038890723501  loss: 1.5486 (1.5970)  acc1: 48.4375 (49.2188)  acc5: 85.9375 (89.0625)  time: 10.5234  data: 0.0384\n",
      "Epoch: [6]  [2/8]  eta: 0:01:02  lr: 1.0000000000000006e-10  img/s: 6.140800573187691  loss: 1.6139 (1.6027)  acc1: 50.0000 (49.4792)  acc5: 87.5000 (88.5417)  time: 10.4969  data: 0.0329\n",
      "Epoch: [6]  [3/8]  eta: 0:00:52  lr: 1.0000000000000006e-10  img/s: 6.129049593079381  loss: 1.6139 (1.6601)  acc1: 48.4375 (48.8281)  acc5: 85.9375 (87.1094)  time: 10.4887  data: 0.0301\n",
      "Epoch: [6]  [4/8]  eta: 0:00:41  lr: 1.0000000000000006e-10  img/s: 6.125488753545537  loss: 1.6455 (1.6639)  acc1: 48.4375 (48.1250)  acc5: 87.5000 (87.1875)  time: 10.4850  data: 0.0286\n",
      "Epoch: [6]  [5/8]  eta: 0:00:31  lr: 1.0000000000000006e-10  img/s: 6.152833901982715  loss: 1.6455 (1.6834)  acc1: 46.8750 (47.1354)  acc5: 85.9375 (86.1979)  time: 10.4749  data: 0.0276\n",
      "Epoch: [6]  [6/8]  eta: 0:00:20  lr: 1.0000000000000006e-10  img/s: 6.102820719922689  loss: 1.6692 (1.6814)  acc1: 48.4375 (47.5446)  acc5: 85.9375 (85.7143)  time: 10.4797  data: 0.0267\n",
      "Epoch: [6]  [7/8]  eta: 0:00:10  lr: 1.0000000000000006e-10  img/s: 5.807340360704893  loss: 1.6692 (1.7100)  acc1: 46.8750 (46.2000)  acc5: 82.8125 (85.2000)  time: 10.2913  data: 0.0257\n",
      "Epoch: [6] Total time: 0:01:22\n",
      "Epoch 8/15\n",
      "Epoch: [7]  [0/8]  eta: 0:01:24  lr: 1.0000000000000006e-11  img/s: 6.1074807394044655  loss: 1.6451 (1.6451)  acc1: 45.3125 (45.3125)  acc5: 90.6250 (90.6250)  time: 10.5003  data: 0.0214\n",
      "Epoch: [7]  [1/8]  eta: 0:01:13  lr: 1.0000000000000006e-11  img/s: 6.134721486494414  loss: 1.6308 (1.6379)  acc1: 45.3125 (48.4375)  acc5: 89.0625 (89.8438)  time: 10.4773  data: 0.0216\n",
      "Epoch: [7]  [2/8]  eta: 0:01:02  lr: 1.0000000000000006e-11  img/s: 6.14693929611499  loss: 1.6413 (1.6391)  acc1: 50.0000 (48.9583)  acc5: 89.0625 (88.0208)  time: 10.4629  data: 0.0219\n",
      "Epoch: [7]  [3/8]  eta: 0:00:52  lr: 1.0000000000000006e-11  img/s: 6.116870109286557  loss: 1.6413 (1.6955)  acc1: 45.3125 (44.9219)  acc5: 84.3750 (87.1094)  time: 10.4685  data: 0.0220\n",
      "Epoch: [7]  [4/8]  eta: 0:00:41  lr: 1.0000000000000006e-11  img/s: 6.157044825347894  loss: 1.6451 (1.6855)  acc1: 50.0000 (46.8750)  acc5: 89.0625 (87.5000)  time: 10.4582  data: 0.0221\n",
      "Epoch: [7]  [5/8]  eta: 0:00:31  lr: 1.0000000000000006e-11  img/s: 6.172026657334127  loss: 1.6451 (1.6884)  acc1: 45.3125 (45.3125)  acc5: 85.9375 (87.2396)  time: 10.4470  data: 0.0220\n",
      "Epoch: [7]  [6/8]  eta: 0:00:20  lr: 1.0000000000000006e-11  img/s: 6.172167152308068  loss: 1.6455 (1.6865)  acc1: 45.3125 (45.0893)  acc5: 85.9375 (86.8304)  time: 10.4391  data: 0.0221\n",
      "Epoch: [7]  [7/8]  eta: 0:00:10  lr: 1.0000000000000006e-11  img/s: 5.86629418545547  loss: 1.6455 (1.7209)  acc1: 43.7500 (43.0000)  acc5: 84.3750 (85.8000)  time: 10.2445  data: 0.0216\n",
      "Epoch: [7] Total time: 0:01:21\n",
      "Epoch 9/15\n",
      "Epoch: [8]  [0/8]  eta: 0:01:24  lr: 1.0000000000000006e-12  img/s: 6.056007193153103  loss: 1.6285 (1.6285)  acc1: 46.8750 (46.8750)  acc5: 89.0625 (89.0625)  time: 10.5897  data: 0.0217\n",
      "Epoch: [8]  [1/8]  eta: 0:01:13  lr: 1.0000000000000006e-12  img/s: 6.105183789975939  loss: 1.6285 (1.6721)  acc1: 45.3125 (46.0938)  acc5: 89.0625 (89.0625)  time: 10.5475  data: 0.0220\n",
      "Epoch: [8]  [2/8]  eta: 0:01:03  lr: 1.0000000000000006e-12  img/s: 6.102756342328231  loss: 1.6285 (1.6241)  acc1: 46.8750 (46.8750)  acc5: 89.0625 (89.5833)  time: 10.5348  data: 0.0221\n",
      "Epoch: [8]  [3/8]  eta: 0:00:52  lr: 1.0000000000000006e-12  img/s: 6.0907346500122355  loss: 1.6285 (1.6799)  acc1: 45.3125 (44.5312)  acc5: 89.0625 (88.2812)  time: 10.5337  data: 0.0222\n",
      "Epoch: [8]  [4/8]  eta: 0:00:42  lr: 1.0000000000000006e-12  img/s: 6.106821870472671  loss: 1.6884 (1.6816)  acc1: 46.8750 (45.0000)  acc5: 89.0625 (88.1250)  time: 10.5275  data: 0.0223\n",
      "Epoch: [8]  [5/8]  eta: 0:00:31  lr: 1.0000000000000006e-12  img/s: 6.118863697473736  loss: 1.6884 (1.6901)  acc1: 45.3125 (45.0521)  acc5: 87.5000 (87.2396)  time: 10.5199  data: 0.0223\n",
      "Epoch: [8]  [6/8]  eta: 0:00:21  lr: 1.0000000000000006e-12  img/s: 6.120538992305647  loss: 1.6884 (1.6880)  acc1: 46.8750 (45.3125)  acc5: 87.5000 (87.2768)  time: 10.5141  data: 0.0223\n",
      "Epoch: [8]  [7/8]  eta: 0:00:10  lr: 1.0000000000000006e-12  img/s: 5.825032797620394  loss: 1.6884 (1.7131)  acc1: 45.3125 (44.4000)  acc5: 87.5000 (87.0000)  time: 10.3180  data: 0.0219\n",
      "Epoch: [8] Total time: 0:01:22\n",
      "Epoch 10/15\n",
      "Epoch: [9]  [0/8]  eta: 0:01:24  lr: 1.0000000000000007e-13  img/s: 6.07859490868525  loss: 1.5861 (1.5861)  acc1: 48.4375 (48.4375)  acc5: 89.0625 (89.0625)  time: 10.5507  data: 0.0219\n",
      "Epoch: [9]  [1/8]  eta: 0:01:14  lr: 1.0000000000000007e-13  img/s: 6.024055111836375  loss: 1.5861 (1.6149)  acc1: 46.8750 (47.6562)  acc5: 89.0625 (91.4062)  time: 10.5986  data: 0.0222\n",
      "Epoch: [9]  [2/8]  eta: 0:01:03  lr: 1.0000000000000007e-13  img/s: 6.126274411713423  loss: 1.5861 (1.5810)  acc1: 48.4375 (48.4375)  acc5: 89.0625 (90.6250)  time: 10.5553  data: 0.0221\n",
      "Epoch: [9]  [3/8]  eta: 0:00:52  lr: 1.0000000000000007e-13  img/s: 6.171075717120954  loss: 1.5861 (1.6480)  acc1: 46.8750 (44.9219)  acc5: 89.0625 (89.8438)  time: 10.5147  data: 0.0220\n",
      "Epoch: [9]  [4/8]  eta: 0:00:42  lr: 1.0000000000000007e-13  img/s: 6.1341160199342495  loss: 1.6436 (1.6482)  acc1: 46.8750 (45.0000)  acc5: 89.0625 (89.6875)  time: 10.5028  data: 0.0220\n",
      "Epoch: [9]  [5/8]  eta: 0:00:31  lr: 1.0000000000000007e-13  img/s: 6.149613624350939  loss: 1.6436 (1.6707)  acc1: 45.3125 (44.2708)  acc5: 89.0625 (88.2812)  time: 10.4906  data: 0.0221\n",
      "Epoch: [9]  [6/8]  eta: 0:00:20  lr: 1.0000000000000007e-13  img/s: 6.150244277247918  loss: 1.6436 (1.6604)  acc1: 46.8750 (44.6429)  acc5: 89.0625 (89.0625)  time: 10.4817  data: 0.0221\n",
      "Epoch: [9]  [7/8]  eta: 0:00:10  lr: 1.0000000000000007e-13  img/s: 5.844696921176754  loss: 1.6436 (1.6874)  acc1: 45.3125 (43.8000)  acc5: 89.0625 (88.2000)  time: 10.2859  data: 0.0216\n",
      "Epoch: [9] Total time: 0:01:22\n",
      "Epoch 11/15\n",
      "Epoch: [10]  [0/8]  eta: 0:01:24  lr: 1.0000000000000008e-14  img/s: 6.123301715356453  loss: 1.6201 (1.6201)  acc1: 45.3125 (45.3125)  acc5: 84.3750 (84.3750)  time: 10.5010  data: 0.0491\n",
      "Epoch: [10]  [1/8]  eta: 0:01:13  lr: 1.0000000000000008e-14  img/s: 6.17032362911461  loss: 1.6028 (1.6115)  acc1: 45.3125 (46.0938)  acc5: 84.3750 (86.7188)  time: 10.4479  data: 0.0358\n",
      "Epoch: [10]  [2/8]  eta: 0:01:02  lr: 1.0000000000000008e-14  img/s: 6.174659375304998  loss: 1.6028 (1.6049)  acc1: 45.3125 (45.3125)  acc5: 87.5000 (86.9792)  time: 10.4275  data: 0.0312\n",
      "Epoch: [10]  [3/8]  eta: 0:00:52  lr: 1.0000000000000008e-14  img/s: 6.140957491771254  loss: 1.6028 (1.6576)  acc1: 45.3125 (45.3125)  acc5: 84.3750 (85.5469)  time: 10.4317  data: 0.0290\n",
      "Epoch: [10]  [4/8]  eta: 0:00:41  lr: 1.0000000000000008e-14  img/s: 6.214110232181182  loss: 1.6201 (1.6637)  acc1: 45.3125 (45.0000)  acc5: 87.5000 (86.5625)  time: 10.4097  data: 0.0277\n",
      "Epoch: [10]  [5/8]  eta: 0:00:31  lr: 1.0000000000000008e-14  img/s: 6.153774149456256  loss: 1.6201 (1.6702)  acc1: 43.7500 (44.0104)  acc5: 84.3750 (85.9375)  time: 10.4119  data: 0.0268\n",
      "Epoch: [10]  [6/8]  eta: 0:00:20  lr: 1.0000000000000008e-14  img/s: 6.18609477214292  loss: 1.6883 (1.6749)  acc1: 43.7500 (43.9732)  acc5: 87.5000 (86.1607)  time: 10.4056  data: 0.0262\n",
      "Epoch: [10]  [7/8]  eta: 0:00:10  lr: 1.0000000000000008e-14  img/s: 5.862053638010021  loss: 1.6883 (1.7040)  acc1: 43.7500 (43.2000)  acc5: 84.3750 (85.4000)  time: 10.2161  data: 0.0253\n",
      "Epoch: [10] Total time: 0:01:21\n",
      "Epoch 12/15\n",
      "Epoch: [11]  [0/8]  eta: 0:01:24  lr: 1.0000000000000009e-15  img/s: 6.106657522752997  loss: 1.6654 (1.6654)  acc1: 40.6250 (40.6250)  acc5: 82.8125 (82.8125)  time: 10.5295  data: 0.0492\n",
      "Epoch: [11]  [1/8]  eta: 0:01:13  lr: 1.0000000000000009e-15  img/s: 6.1794957985669425  loss: 1.6208 (1.6431)  acc1: 40.6250 (42.1875)  acc5: 82.8125 (87.5000)  time: 10.4542  data: 0.0356\n",
      "Epoch: [11]  [2/8]  eta: 0:01:02  lr: 1.0000000000000009e-15  img/s: 6.153687249850351  loss: 1.6208 (1.6157)  acc1: 43.7500 (43.7500)  acc5: 92.1875 (89.0625)  time: 10.4438  data: 0.0313\n",
      "Epoch: [11]  [3/8]  eta: 0:00:52  lr: 1.0000000000000009e-15  img/s: 6.196154925792264  loss: 1.6208 (1.6494)  acc1: 40.6250 (42.9688)  acc5: 87.5000 (88.6719)  time: 10.4205  data: 0.0289\n",
      "Epoch: [11]  [4/8]  eta: 0:00:41  lr: 1.0000000000000009e-15  img/s: 6.192525280459659  loss: 1.6208 (1.6388)  acc1: 43.7500 (44.6875)  acc5: 92.1875 (89.3750)  time: 10.4078  data: 0.0275\n",
      "Epoch: [11]  [5/8]  eta: 0:00:31  lr: 1.0000000000000009e-15  img/s: 6.174192977525525  loss: 1.6208 (1.6512)  acc1: 42.1875 (44.2708)  acc5: 87.5000 (88.0208)  time: 10.4045  data: 0.0267\n",
      "Epoch: [11]  [6/8]  eta: 0:00:20  lr: 1.0000000000000009e-15  img/s: 6.188700413852715  loss: 1.6654 (1.6600)  acc1: 43.7500 (44.6429)  acc5: 87.5000 (87.9464)  time: 10.3988  data: 0.0261\n",
      "Epoch: [11]  [7/8]  eta: 0:00:10  lr: 1.0000000000000009e-15  img/s: 5.783108526979679  loss: 1.6654 (1.6984)  acc1: 42.1875 (43.6000)  acc5: 87.5000 (86.6000)  time: 10.2252  data: 0.0251\n",
      "Epoch: [11] Total time: 0:01:21\n",
      "Epoch 13/15\n",
      "Epoch: [12]  [0/8]  eta: 0:01:22  lr: 1.000000000000001e-16  img/s: 6.1916095719512665  loss: 1.6183 (1.6183)  acc1: 45.3125 (45.3125)  acc5: 87.5000 (87.5000)  time: 10.3580  data: 0.0215\n",
      "Epoch: [12]  [1/8]  eta: 0:01:12  lr: 1.000000000000001e-16  img/s: 6.193862835734726  loss: 1.5918 (1.6050)  acc1: 45.3125 (45.3125)  acc5: 87.5000 (89.8438)  time: 10.3562  data: 0.0215\n",
      "Epoch: [12]  [2/8]  eta: 0:01:02  lr: 1.000000000000001e-16  img/s: 6.164159791669733  loss: 1.5918 (1.5614)  acc1: 45.3125 (47.9167)  acc5: 87.5000 (88.5417)  time: 10.3725  data: 0.0219\n",
      "Epoch: [12]  [3/8]  eta: 0:00:51  lr: 1.000000000000001e-16  img/s: 6.159466618425414  loss: 1.5918 (1.6325)  acc1: 45.3125 (45.7031)  acc5: 85.9375 (86.7188)  time: 10.3826  data: 0.0220\n",
      "Epoch: [12]  [4/8]  eta: 0:00:41  lr: 1.000000000000001e-16  img/s: 6.167067879007818  loss: 1.6183 (1.6339)  acc1: 45.3125 (45.0000)  acc5: 87.5000 (87.5000)  time: 10.3860  data: 0.0219\n",
      "Epoch: [12]  [5/8]  eta: 0:00:31  lr: 1.000000000000001e-16  img/s: 6.184570056705063  loss: 1.6183 (1.6524)  acc1: 42.1875 (44.5312)  acc5: 85.9375 (86.9792)  time: 10.3834  data: 0.0219\n",
      "Epoch: [12]  [6/8]  eta: 0:00:20  lr: 1.000000000000001e-16  img/s: 6.210199900844258  loss: 1.6393 (1.6674)  acc1: 42.1875 (43.0804)  acc5: 85.9375 (86.6071)  time: 10.3754  data: 0.0219\n",
      "Epoch: [12]  [7/8]  eta: 0:00:10  lr: 1.000000000000001e-16  img/s: 5.795253777995755  loss: 1.6393 (1.6988)  acc1: 42.1875 (42.0000)  acc5: 84.3750 (85.4000)  time: 10.2024  data: 0.0215\n",
      "Epoch: [12] Total time: 0:01:21\n",
      "Epoch 14/15\n",
      "Epoch: [13]  [0/8]  eta: 0:01:24  lr: 1.000000000000001e-17  img/s: 6.094694086654876  loss: 1.5837 (1.5837)  acc1: 46.8750 (46.8750)  acc5: 92.1875 (92.1875)  time: 10.5224  data: 0.0214\n",
      "Epoch: [13]  [1/8]  eta: 0:01:13  lr: 1.000000000000001e-17  img/s: 6.173776062790509  loss: 1.5641 (1.5739)  acc1: 46.8750 (50.0000)  acc5: 90.6250 (91.4062)  time: 10.4553  data: 0.0216\n",
      "Epoch: [13]  [2/8]  eta: 0:01:02  lr: 1.000000000000001e-17  img/s: 6.153617421588362  loss: 1.5837 (1.5819)  acc1: 51.5625 (50.5208)  acc5: 90.6250 (90.6250)  time: 10.4444  data: 0.0218\n",
      "Epoch: [13]  [3/8]  eta: 0:00:52  lr: 1.000000000000001e-17  img/s: 6.180215690508051  loss: 1.5837 (1.6407)  acc1: 46.8750 (48.8281)  acc5: 89.0625 (88.2812)  time: 10.4277  data: 0.0218\n",
      "Epoch: [13]  [4/8]  eta: 0:00:41  lr: 1.000000000000001e-17  img/s: 6.21255140386621  loss: 1.5979 (1.6394)  acc1: 50.0000 (49.0625)  acc5: 90.6250 (88.7500)  time: 10.4069  data: 0.0219\n",
      "Epoch: [13]  [5/8]  eta: 0:00:31  lr: 1.000000000000001e-17  img/s: 6.21443708271365  loss: 1.5979 (1.6446)  acc1: 46.8750 (47.1354)  acc5: 89.0625 (88.5417)  time: 10.3926  data: 0.0220\n",
      "Epoch: [13]  [6/8]  eta: 0:00:20  lr: 1.000000000000001e-17  img/s: 6.1640052236488625  loss: 1.6345 (1.6598)  acc1: 46.8750 (45.9821)  acc5: 89.0625 (87.7232)  time: 10.3945  data: 0.0221\n",
      "Epoch: [13]  [7/8]  eta: 0:00:10  lr: 1.000000000000001e-17  img/s: 5.821303463441598  loss: 1.6345 (1.6945)  acc1: 43.7500 (44.8000)  acc5: 87.5000 (86.4000)  time: 10.2140  data: 0.0216\n",
      "Epoch: [13] Total time: 0:01:21\n",
      "Epoch 15/15\n",
      "Epoch: [14]  [0/8]  eta: 0:01:23  lr: 1.000000000000001e-18  img/s: 6.181501385081356  loss: 1.6162 (1.6162)  acc1: 45.3125 (45.3125)  acc5: 90.6250 (90.6250)  time: 10.3756  data: 0.0221\n",
      "Epoch: [14]  [1/8]  eta: 0:01:12  lr: 1.000000000000001e-18  img/s: 6.1788857289837855  loss: 1.5858 (1.6010)  acc1: 45.3125 (46.0938)  acc5: 90.6250 (91.4062)  time: 10.3811  data: 0.0254\n",
      "Epoch: [14]  [2/8]  eta: 0:01:02  lr: 1.000000000000001e-18  img/s: 6.189812223043066  loss: 1.5858 (1.5724)  acc1: 46.8750 (47.9167)  acc5: 92.1875 (91.6667)  time: 10.3747  data: 0.0244\n",
      "Epoch: [14]  [3/8]  eta: 0:00:51  lr: 1.000000000000001e-18  img/s: 6.203303280249715  loss: 1.5858 (1.6335)  acc1: 45.3125 (46.0938)  acc5: 90.6250 (90.6250)  time: 10.3658  data: 0.0238\n",
      "Epoch: [14]  [4/8]  eta: 0:00:41  lr: 1.000000000000001e-18  img/s: 6.205237712431971  loss: 1.6162 (1.6435)  acc1: 45.3125 (45.6250)  acc5: 90.6250 (89.0625)  time: 10.3598  data: 0.0234\n",
      "Epoch: [14]  [5/8]  eta: 0:00:31  lr: 1.000000000000001e-18  img/s: 6.201369767251565  loss: 1.6162 (1.6516)  acc1: 43.7500 (45.0521)  acc5: 87.5000 (88.5417)  time: 10.3569  data: 0.0232\n",
      "Epoch: [14]  [6/8]  eta: 0:00:20  lr: 1.000000000000001e-18  img/s: 6.187747324487007  loss: 1.6484 (1.6511)  acc1: 43.7500 (44.8661)  acc5: 87.5000 (88.3929)  time: 10.3580  data: 0.0230\n",
      "Epoch: [14]  [7/8]  eta: 0:00:10  lr: 1.000000000000001e-18  img/s: 5.84749401319296  loss: 1.6484 (1.6911)  acc1: 43.7500 (43.8000)  acc5: 87.5000 (86.6000)  time: 10.1772  data: 0.0224\n",
      "Epoch: [14] Total time: 0:01:21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [05:53<00:00,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "353.4519580937922\n",
      "Accuracy of the network on the 10000 test images: 38.0125 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "axx_mult = 'appro3'\n",
    "model = resnet18(pretrained=True, axx_mult = axx_mult)\n",
    "model.eval() # for evaluation\n",
    "# It is a bit slow since we collect histograms on CPU\n",
    "with torch.no_grad():\n",
    "    stats = collect_stats(model, data_t, num_batches=2)\n",
    "    amax = compute_amax(model, method=\"percentile\", percentile=99.99)\n",
    "    \n",
    "# Inference without retraining\n",
    "import timeit\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "model.eval()\n",
    "start_time = timeit.default_timer()\n",
    "with torch.no_grad():\n",
    "    for iteraction, (images, labels) in tqdm(enumerate(data), total=len(data)):\n",
    "        images, labels = images.to(\"cpu\"), labels.to(\"cpu\")\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(timeit.default_timer() - start_time)\n",
    "print('Accuracy of the network on the 10000 test images: %.4f %%' % (\n",
    "    100 * correct / total))\n",
    "\n",
    "# Retraining for 15 epochs\n",
    "from adapt.references.classification.train import evaluate, train_one_epoch, load_data\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "for epoch in range(15):\n",
    "    print(f\"Epoch {epoch + 1}/15\")\n",
    "    train_one_epoch(model, criterion, optimizer, data_t, \"cpu\", epoch, 1)\n",
    "    lr_scheduler.step()\n",
    "\n",
    "# Inference after retraining\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "model.eval()\n",
    "start_time = timeit.default_timer()\n",
    "with torch.no_grad():\n",
    "    for iteraction, (images, labels) in tqdm(enumerate(data), total=len(data)):\n",
    "        images, labels = images.to(\"cpu\"), labels.to(\"cpu\")\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(timeit.default_timer() - start_time)\n",
    "print('Accuracy of the network on the 10000 test images: %.4f %%' % (\n",
    "    100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
