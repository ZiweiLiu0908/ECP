{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb626d36",
   "metadata": {},
   "source": [
    "Name: Ziwei Liu\n",
    "Student ID: 3766789"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "458eb16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import torch\n",
    "\n",
    "import requests\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms as T\n",
    "from torchvision.datasets import STL10\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ab55c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
     ]
    }
   ],
   "source": [
    "from examples.models.densenet import densenet121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f4cd1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "threads = 16\n",
    "torch.set_num_threads(threads)\n",
    "\n",
    "#maybe better performance (Jupyter)\n",
    "#%env OMP_PLACES=cores\n",
    "#%env OMP_PROC_BIND=close\n",
    "#%env OMP_WAIT_POLICY=active\n",
    "\n",
    "#WSL\n",
    "os.environ['OMP_PLACES'] = 'cores'\n",
    "os.environ['OMP_PROC_BIND'] = 'close'\n",
    "os.environ['OMP_WAIT_POLICY'] = 'active'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3cbd963",
   "metadata": {},
   "outputs": [],
   "source": [
    "#axx_mult = 'mul8s_acc'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fac997d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/PyInit_conv2d_exact/build.ninja...\n",
      "Building extension module PyInit_conv2d_exact...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DenseNet(\n",
       "  (features): Sequential(\n",
       "    (conv0): AdaPT_Conv2d(\n",
       "      3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "      (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "    )\n",
       "    (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu0): ReLU(inplace=True)\n",
       "    (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (denseblock1): _DenseBlock(\n",
       "      (denselayer1): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer2): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer3): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer4): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer5): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer6): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (transition1): _Transition(\n",
       "      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv): AdaPT_Conv2d(\n",
       "        256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    )\n",
       "    (denseblock2): _DenseBlock(\n",
       "      (denselayer1): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer2): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer3): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer4): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer5): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer6): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer7): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer8): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer9): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer10): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer11): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer12): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (transition2): _Transition(\n",
       "      (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv): AdaPT_Conv2d(\n",
       "        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    )\n",
       "    (denseblock3): _DenseBlock(\n",
       "      (denselayer1): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer2): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer3): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer4): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer5): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer6): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer7): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer8): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer9): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer10): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer11): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer12): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer13): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer14): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer15): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer16): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer17): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer18): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer19): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer20): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer21): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer22): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer23): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer24): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (transition3): _Transition(\n",
       "      (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv): AdaPT_Conv2d(\n",
       "        1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    )\n",
       "    (denseblock4): _DenseBlock(\n",
       "      (denselayer1): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer2): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer3): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer4): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer5): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer6): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer7): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer8): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer9): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer10): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer11): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer12): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer13): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer14): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer15): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer16): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=1024, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "axx_mult = 'exact'\n",
    "model = densenet121(pretrained=True, axx_mult = axx_mult)\n",
    "model.eval() # for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47008795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "torch.Size([1024, 3, 224, 224])\n",
      "torch.Size([1024])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms as T\n",
    "from torchvision.datasets import STL10\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def val_dataloader(mean = (0.485, 0.456, 0.406), std = (0.229, 0.224, 0.225)):\n",
    "\n",
    "    transform = T.Compose(\n",
    "        [\n",
    "            T.Resize(224),  # Resize to 224x224 before applying normalization\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean, std),\n",
    "        ]\n",
    "    )\n",
    "    dataset = STL10(root=\"datasets/stl10_data\", split='test', download=True, transform=transform)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=1024,\n",
    "        num_workers=0,\n",
    "        drop_last=True,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "transform = T.Compose(\n",
    "    [\n",
    "        T.Resize(224),  # Resize to 224x224\n",
    "        T.RandomCrop(224),  # Apply random crop to 224x224\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean = (0.485, 0.456, 0.406), std = (0.229, 0.224, 0.225)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "dataset = STL10(root=\"datasets/stl10_data\", split='train', download=True, transform=transform)\n",
    "\n",
    "\n",
    "evens = list(range(0, len(dataset), 10))\n",
    "trainset_1 = torch.utils.data.Subset(dataset, evens)\n",
    "\n",
    "\n",
    "data = val_dataloader()\n",
    "\n",
    "\n",
    "data_t = DataLoader(trainset_1, batch_size=128, shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "data_iter = iter(data)\n",
    "images, labels = next(data_iter)\n",
    "\n",
    "print(images.shape)\n",
    "print(labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8b3320f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [04:59<00:00, 149.59s/it]\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0222 20:39:57.184715 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.185098 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.185382 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.185596 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.185798 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.185988 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.186201 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.186431 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.186669 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.186898 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.187138 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.187367 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.187624 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.187933 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.188170 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.188374 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.188571 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.188762 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.188959 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.189144 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.189338 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.189518 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.189706 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.189882 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.190070 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.190255 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.190461 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.190691 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.190930 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.191155 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.191381 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.191606 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.191840 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.192061 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.192281 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.192497 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.192722 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.192941 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.193172 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.193386 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.193591 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.193773 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.193957 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.194137 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.194325 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.194497 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.194679 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.194855 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.195034 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.195209 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.195398 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.195580 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.195767 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.195945 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.196127 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.196302 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.196482 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.196653 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.196829 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.196998 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.197178 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.197348 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.197536 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.197705 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.197885 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.198057 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.198232 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.198401 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.198590 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.198750 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.198923 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.199090 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.199283 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.199444 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.199624 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.199798 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.199979 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.200150 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.200334 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.200503 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.200678 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.200848 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.201024 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.201196 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.201372 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.201542 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.201720 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.201891 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.202067 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.202237 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.202414 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.202584 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.202759 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.202927 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.203102 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.203269 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.203443 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.203635 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.203807 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.203976 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.204149 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.204320 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.204505 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.204675 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.204849 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.205015 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.205190 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.205362 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.205546 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.205730 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.205909 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.206081 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.206260 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.206433 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.206716 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.206901 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.207079 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.207255 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.207434 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.207612 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.207792 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.207964 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.208151 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.208321 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.208498 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.208667 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.208844 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.209013 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.209186 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.209353 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.209527 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.209704 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.209878 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.210044 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.210223 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.210389 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.210565 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.210737 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.210916 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.211088 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.211263 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.211433 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.211614 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.211789 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.211970 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.212141 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.212319 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.212497 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.212675 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.212847 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.213025 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.213195 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.213369 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.213537 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.213716 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.213890 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.214063 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.214232 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.214410 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.214584 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.214764 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.214938 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.215142 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.215360 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.215538 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.215737 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.215975 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.216257 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.216567 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.216859 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.217154 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.217444 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.217718 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.217984 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.218278 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.218559 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.218855 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.219141 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.219433 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.219743 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.220049 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.220340 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.220631 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.220913 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.221204 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.221493 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.221784 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.222069 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.222250 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.222421 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.222595 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.222764 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.222940 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.223110 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.223286 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.223456 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.223648 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.223819 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.223991 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.224167 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.224338 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.224510 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.224684 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.224852 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.225026 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.225194 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.225365 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.225532 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.225706 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.225874 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.226047 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.226218 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.226395 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.226564 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.226738 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.226907 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.227083 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.227251 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.227425 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.227604 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.227783 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.227950 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.228121 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.228290 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.228466 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.228634 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.228806 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.228973 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.229151 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.229365 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.229544 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.229715 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.229890 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.230057 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.230230 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.230400 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.230578 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.230745 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.230917 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.231087 140123427950784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 20:39:57.244056 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.244322 140123427950784 tensor_quantizer.py:239] Call .cuda() if running on GPU after loading calibrated amax.\n",
      "W0222 20:39:57.244844 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.245263 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.245648 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.246037 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.246415 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.246803 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.247179 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.247566 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.247945 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.248325 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.248692 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.249070 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.249437 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.249813 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.250177 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.250547 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.250910 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.251282 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.251655 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.252027 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.252392 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.252761 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.253120 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.253487 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.253844 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.254214 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.254581 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.254967 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.255334 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.255722 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.256096 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.256492 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.257244 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.257836 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.258413 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.259000 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.259585 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.260178 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.260758 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.261342 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.261864 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.262494 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.262868 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.263694 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.264253 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.264626 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.264989 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.265365 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.265731 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.266100 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.266463 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.266833 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.267189 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.267560 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.267925 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.268303 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.268670 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.269037 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.269394 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.269762 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.270118 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.270483 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.270847 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.271221 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.271587 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.271954 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.272319 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.272691 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.273056 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.273426 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.273790 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.274178 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.274566 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.275204 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.275942 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.276550 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.277134 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.277727 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.278306 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.278700 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.279492 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.280093 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.280481 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.281242 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.281818 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.282253 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.283027 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.283414 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.283789 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.284161 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.284526 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.284896 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.285261 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.285635 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.285997 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.286364 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.286730 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.287103 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.287463 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.287838 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.288201 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.288576 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.288932 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.289295 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.289659 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.290035 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.290398 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.290775 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.291137 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.291530 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.292286 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.292680 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.293425 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.294006 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.294581 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.294957 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.295724 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.296108 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.296873 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.297265 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.298017 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.298606 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.299167 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.299544 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.299910 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.300279 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.300641 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.301007 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.301368 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.301738 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.302103 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.302474 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.302841 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.303211 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.303578 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.303950 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.304311 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.304691 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.305055 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.305421 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.305782 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.306150 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.306510 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.306870 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.307229 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.307603 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.307962 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.308436 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.308817 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.309576 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.310156 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.310732 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.311300 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.311887 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.312259 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.313018 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.313398 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.314150 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.314729 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.315305 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.315896 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.316354 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.316717 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.317084 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.317453 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.317821 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.318181 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.318540 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.318901 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.319273 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.319638 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.320008 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.320370 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.320735 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.321095 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.321470 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.321831 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.322195 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.322557 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.322929 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.323286 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.323662 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.324029 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.324569 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.325191 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.325653 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.326373 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.326971 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.327562 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.328158 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.328545 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.329305 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.329890 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.330425 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.331040 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.331424 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.332180 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.332558 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.332922 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.333292 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.333656 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.334027 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.334394 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.334765 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.335131 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.335499 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.335871 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.336242 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.336606 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.336977 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.337337 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.337710 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.338070 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.338431 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.338788 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.339153 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.339523 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.339903 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.340365 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.340950 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.341327 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.342101 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.342690 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.343071 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.343842 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.344432 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.344883 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.345579 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.346160 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.346541 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.347302 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.347909 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.348299 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.349062 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.349431 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.349798 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.350157 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.350523 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 20:39:57.350879 140123427950784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.conv0.quantizer                : TensorQuantizer(8bit per-tensor amax=2.6387 calibrator=HistogramCalibrator quant)\n",
      "features.conv0.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.1544 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer1.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.2427 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer1.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0649 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer1.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0687 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer1.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0350 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer2.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.5290 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer2.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0556 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer2.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.1224 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer2.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0295 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer3.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.1763 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer3.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0524 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer3.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0878 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer3.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0293 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer4.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.1587 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer4.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0360 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer4.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0782 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer4.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0259 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer5.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.1288 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer5.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0332 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer5.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0596 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer5.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0227 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer6.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.1189 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer6.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0406 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer6.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0652 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer6.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0247 calibrator=HistogramCalibrator quant)\n",
      "features.transition1.conv.quantizer     : TensorQuantizer(8bit per-tensor amax=0.2048 calibrator=HistogramCalibrator quant)\n",
      "features.transition1.conv.quantizer_w   : TensorQuantizer(8bit per-tensor amax=0.0519 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer1.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.1336 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer1.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0308 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer1.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0577 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer1.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0197 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer2.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.1168 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer2.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0267 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer2.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0576 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer2.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0230 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer3.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.1018 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer3.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0284 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer3.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0610 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer3.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0236 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer4.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0989 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer4.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0266 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer4.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0666 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer4.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0222 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer5.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0818 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer5.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0265 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer5.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0637 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer5.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0243 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer6.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0897 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer6.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0289 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer6.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0637 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer6.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0233 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer7.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0748 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer7.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0265 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer7.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0536 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer7.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0243 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer8.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0764 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer8.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0293 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer8.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0563 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer8.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0282 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer9.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0719 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer9.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0266 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer9.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0504 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer9.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0246 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer10.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0761 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer10.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0283 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer10.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0576 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer10.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0228 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer11.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0861 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer11.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0305 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer11.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0589 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer11.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0218 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer12.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0721 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer12.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0251 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer12.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0531 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer12.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0244 calibrator=HistogramCalibrator quant)\n",
      "features.transition2.conv.quantizer     : TensorQuantizer(8bit per-tensor amax=0.1105 calibrator=HistogramCalibrator quant)\n",
      "features.transition2.conv.quantizer_w   : TensorQuantizer(8bit per-tensor amax=0.0310 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer1.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0631 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer1.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0260 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer1.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0390 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer1.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0208 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer2.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0635 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer2.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0287 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer2.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0408 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer2.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0174 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer3.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0607 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer3.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0251 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer3.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0542 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer3.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0235 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer4.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0620 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer4.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0288 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer4.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0510 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer4.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0217 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer5.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0563 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer5.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0237 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer5.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0325 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer5.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0202 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer6.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0586 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer6.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0209 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer6.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0374 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer6.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0242 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer7.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0484 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer7.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0236 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer7.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0281 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer7.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0146 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer8.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0513 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer8.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0249 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer8.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0434 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer8.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0200 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer9.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0507 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer9.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0181 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer9.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0239 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer9.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0135 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer10.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0555 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer10.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0194 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer10.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0253 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer10.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0137 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer11.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0473 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer11.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0220 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer11.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0373 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer11.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0175 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer12.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0472 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer12.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0202 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer12.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0271 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer12.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0133 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer13.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0532 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer13.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0203 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer13.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0292 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer13.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0139 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer14.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0488 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer14.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0191 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer14.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0265 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer14.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0110 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer15.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0481 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer15.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0187 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer15.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0282 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer15.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0125 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer16.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0486 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer16.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0191 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer16.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0376 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer16.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0150 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer17.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0412 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer17.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0214 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer17.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0615 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer17.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0173 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer18.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0442 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer18.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0196 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer18.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0797 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer18.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0235 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer19.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0445 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer19.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0197 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer19.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0477 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer19.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0163 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer20.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0343 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer20.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0219 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer20.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0614 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer20.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0224 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer21.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0771 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer21.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0192 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer21.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0486 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer21.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0210 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer22.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0909 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer22.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0218 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer22.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0603 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer22.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0200 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer23.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0864 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer23.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0194 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer23.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.1056 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer23.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0256 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer24.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0688 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer24.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0184 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer24.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0699 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer24.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0201 calibrator=HistogramCalibrator quant)\n",
      "features.transition3.conv.quantizer     : TensorQuantizer(8bit per-tensor amax=0.8180 calibrator=HistogramCalibrator quant)\n",
      "features.transition3.conv.quantizer_w   : TensorQuantizer(8bit per-tensor amax=0.0380 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer1.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0986 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer1.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0102 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer1.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.3773 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer1.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0348 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer2.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.1127 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer2.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0118 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer2.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.4744 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer2.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0350 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer3.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.1240 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer3.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0116 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer3.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.4674 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer3.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0352 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer4.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0417 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer4.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0117 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer4.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.2096 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer4.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0349 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer5.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0479 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer5.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0119 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer5.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.2926 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer5.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0382 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer6.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0495 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer6.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0104 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer6.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.2173 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer6.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0333 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer7.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0871 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer7.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0112 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer7.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.2549 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer7.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0326 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer8.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0644 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer8.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0100 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer8.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.3675 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer8.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0319 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer9.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0645 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer9.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0100 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer9.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.2257 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer9.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0332 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer10.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0758 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer10.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0108 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer10.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.3975 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer10.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0354 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer11.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0542 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer11.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0092 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer11.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.2579 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer11.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0376 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer12.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0550 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer12.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0095 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer12.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.2977 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer12.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0363 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer13.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0698 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer13.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0101 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer13.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.2855 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer13.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0405 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer14.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0541 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer14.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0102 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer14.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.2855 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer14.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0362 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer15.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0756 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer15.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0090 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer15.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.4441 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer15.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0319 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer16.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.1427 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer16.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0108 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer16.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.7068 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer16.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0330 calibrator=HistogramCalibrator quant)\n"
     ]
    }
   ],
   "source": [
    "from pytorch_quantization import nn as quant_nn\n",
    "from pytorch_quantization import calib\n",
    "\n",
    "def collect_stats(model, data_loader, num_batches):\n",
    "     \"\"\"Feed data to the network and collect statistic\"\"\"\n",
    "\n",
    "     # Enable calibrators\n",
    "     for name, module in model.named_modules():\n",
    "         if isinstance(module, quant_nn.TensorQuantizer):\n",
    "             if module._calibrator is not None:\n",
    "                 module.disable_quant()\n",
    "                 module.enable_calib()\n",
    "             else:\n",
    "                 module.disable()\n",
    "\n",
    "     for i, (image, _) in tqdm(enumerate(data_loader), total=num_batches):\n",
    "         model(image.cpu())\n",
    "         if i >= num_batches:\n",
    "             break\n",
    "\n",
    "     # Disable calibrators\n",
    "     for name, module in model.named_modules():\n",
    "         if isinstance(module, quant_nn.TensorQuantizer):\n",
    "             if module._calibrator is not None:\n",
    "                 module.enable_quant()\n",
    "                 module.disable_calib()\n",
    "             else:\n",
    "                 module.enable()\n",
    "\n",
    "def compute_amax(model, **kwargs):\n",
    " # Load calib result\n",
    " for name, module in model.named_modules():\n",
    "     if isinstance(module, quant_nn.TensorQuantizer):\n",
    "         if module._calibrator is not None:\n",
    "             if isinstance(module._calibrator, calib.MaxCalibrator):\n",
    "                 module.load_calib_amax()\n",
    "             else:\n",
    "                 module.load_calib_amax(**kwargs)\n",
    "         print(F\"{name:40}: {module}\")\n",
    " model.cpu()\n",
    "\n",
    "# It is a bit slow since we collect histograms on CPU\n",
    "with torch.no_grad():\n",
    "    stats = collect_stats(model, data_t, num_batches=2)\n",
    "    amax = compute_amax(model, method=\"percentile\", percentile=99.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0b99b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 4/7 [38:12<28:42, 574.01s/it]"
     ]
    }
   ],
   "source": [
    "# Inference without retraining\n",
    "import timeit\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "model.eval()\n",
    "start_time = timeit.default_timer()\n",
    "with torch.no_grad():\n",
    "    for iteraction, (images, labels) in tqdm(enumerate(data), total=len(data)):\n",
    "        images, labels = images.to(\"cpu\"), labels.to(\"cpu\")\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(timeit.default_timer() - start_time)\n",
    "print('Accuracy of the network on the 10000 test images: %.4f %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4939a153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/PyInit_conv2d_approx1/build.ninja...\n",
      "Building extension module PyInit_conv2d_approx1...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/2] c++ -MMD -MF axx_conv2d.o.d -DTORCH_EXTENSION_NAME=PyInit_conv2d_approx1 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /root/miniconda3/lib/python3.8/site-packages/torch/include -isystem /root/miniconda3/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -isystem /root/miniconda3/lib/python3.8/site-packages/torch/include/TH -isystem /root/miniconda3/lib/python3.8/site-packages/torch/include/THC -isystem /root/miniconda3/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -DAXX_MULT=approx1 -march=native -fopenmp -O3 -c /root/autodl-tmp/adapt-main/adapt-main/adapt/cpu-kernels/axx_conv2d.cpp -o axx_conv2d.o \n",
      "FAILED: axx_conv2d.o \n",
      "c++ -MMD -MF axx_conv2d.o.d -DTORCH_EXTENSION_NAME=PyInit_conv2d_approx1 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /root/miniconda3/lib/python3.8/site-packages/torch/include -isystem /root/miniconda3/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -isystem /root/miniconda3/lib/python3.8/site-packages/torch/include/TH -isystem /root/miniconda3/lib/python3.8/site-packages/torch/include/THC -isystem /root/miniconda3/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -DAXX_MULT=approx1 -march=native -fopenmp -O3 -c /root/autodl-tmp/adapt-main/adapt-main/adapt/cpu-kernels/axx_conv2d.cpp -o axx_conv2d.o \n",
      "/root/autodl-tmp/adapt-main/adapt-main/adapt/cpu-kernels/axx_conv2d.cpp:20:42: fatal error: axx_mults/approx1.h: No such file or directory\n",
      " #include STR(axx_mults/EXPAND(AXX_MULT).h)\n",
      "                                          ^\n",
      "compilation terminated.\n",
      "ninja: build stopped: subcommand failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error building extension 'PyInit_conv2d_approx1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/utils/cpp_extension.py\u001b[0m in \u001b[0;36m_run_ninja_build\u001b[0;34m(build_directory, verbose, error_prefix)\u001b[0m\n\u001b[1;32m   1666\u001b[0m         \u001b[0mstdout_fileno\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1667\u001b[0;31m         subprocess.run(\n\u001b[0m\u001b[1;32m   1668\u001b[0m             \u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             raise CalledProcessError(retcode, process.args,\n\u001b[0m\u001b[1;32m    517\u001b[0m                                      output=stdout, stderr=stderr)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['ninja', '-v']' returned non-zero exit status 1.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3688/2425882306.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0maxx_mult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'approx1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdensenet121\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxx_mult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxx_mult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# for evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# It is a bit slow since we collect histograms on CPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/autodl-tmp/adapt-main/adapt-main/examples/models/densenet.py\u001b[0m in \u001b[0;36mdensenet121\u001b[0;34m(pretrained, progress, device, axx_mult, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0maxx_mult_global\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxx_mult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m     return _densenet(\n\u001b[0m\u001b[1;32m    208\u001b[0m         \u001b[0;34m\"densenet121\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     )\n",
      "\u001b[0;32m~/autodl-tmp/adapt-main/adapt-main/examples/models/densenet.py\u001b[0m in \u001b[0;36m_densenet\u001b[0;34m(arch, growth_rate, block_config, num_init_features, pretrained, progress, device, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m ):\n\u001b[0;32m--> 186\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDenseNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrowth_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_init_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mscript_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/autodl-tmp/adapt-main/adapt-main/examples/models/densenet.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, growth_rate, block_config, num_init_features, bn_size, drop_rate, num_classes)\u001b[0m\n\u001b[1;32m    115\u001b[0m                     (\n\u001b[1;32m    116\u001b[0m                         \u001b[0;34m\"conv0\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                         approxNN.AdaPT_Conv2d(3,\n\u001b[0m\u001b[1;32m    118\u001b[0m                         \u001b[0mnum_init_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                         \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/autodl-tmp/adapt-main/adapt-main/adapt/approx_layers/axx_layers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, padding_mode, axx_mult, device, dtype)\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;31m#Jit compilation method for cpp extention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;31m#set PyInit_ prefix to comply with the python module name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxx_conv2d_kernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'PyInit_conv2d_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0maxx_mult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msources\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"/root/autodl-tmp/adapt-main/adapt-main/adapt/cpu-kernels/axx_conv2d.cpp\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra_cflags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'-DAXX_MULT='\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0maxx_mult\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' -march=native -fopenmp -O3'\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra_ldflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'-lgomp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgroups\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgroups\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0min_channels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/utils/cpp_extension.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, sources, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_python_module, is_standalone, keep_intermediates)\u001b[0m\n\u001b[1;32m   1077\u001b[0m                 verbose=True)\n\u001b[1;32m   1078\u001b[0m     '''\n\u001b[0;32m-> 1079\u001b[0;31m     return _jit_compile(\n\u001b[0m\u001b[1;32m   1080\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0msources\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msources\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/utils/cpp_extension.py\u001b[0m in \u001b[0;36m_jit_compile\u001b[0;34m(name, sources, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_python_module, is_standalone, keep_intermediates)\u001b[0m\n\u001b[1;32m   1290\u001b[0m                             \u001b[0mclean_ctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclean_ctx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m                         )\n\u001b[0;32m-> 1292\u001b[0;31m                     _write_ninja_file_and_build_library(\n\u001b[0m\u001b[1;32m   1293\u001b[0m                         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m                         \u001b[0msources\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/utils/cpp_extension.py\u001b[0m in \u001b[0;36m_write_ninja_file_and_build_library\u001b[0;34m(name, sources, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_standalone)\u001b[0m\n\u001b[1;32m   1402\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1403\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Building extension module {name}...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1404\u001b[0;31m     _run_ninja_build(\n\u001b[0m\u001b[1;32m   1405\u001b[0m         \u001b[0mbuild_directory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/utils/cpp_extension.py\u001b[0m in \u001b[0;36m_run_ninja_build\u001b[0;34m(build_directory, verbose, error_prefix)\u001b[0m\n\u001b[1;32m   1681\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'output'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1682\u001b[0m             \u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\": {error.output.decode()}\"\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1683\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error building extension 'PyInit_conv2d_approx1'"
     ]
    }
   ],
   "source": [
    "axx_mult = 'appro1'\n",
    "model = densenet121(pretrained=True, axx_mult = axx_mult)\n",
    "model.eval() # for evaluation\n",
    "# It is a bit slow since we collect histograms on CPU\n",
    "with torch.no_grad():\n",
    "    stats = collect_stats(model, data_t, num_batches=2)\n",
    "    amax = compute_amax(model, method=\"percentile\", percentile=99.99)\n",
    "    \n",
    "# Inference without retraining\n",
    "import timeit\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "model.eval()\n",
    "start_time = timeit.default_timer()\n",
    "with torch.no_grad():\n",
    "    for iteraction, (images, labels) in tqdm(enumerate(data), total=len(data)):\n",
    "        images, labels = images.to(\"cpu\"), labels.to(\"cpu\")\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(timeit.default_timer() - start_time)\n",
    "print('Accuracy of the network on the 10000 test images: %.4f %%' % (\n",
    "    100 * correct / total))\n",
    "\n",
    "# Retraining for 15 epochs\n",
    "from adapt.references.classification.train import evaluate, train_one_epoch, load_data\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "for epoch in range(15):\n",
    "    print(f\"Epoch {epoch + 1}/15\")\n",
    "    train_one_epoch(model, criterion, optimizer, data_t, \"cpu\", epoch, 1)\n",
    "    lr_scheduler.step()\n",
    "\n",
    "# Inference after retraining\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "model.eval()\n",
    "start_time = timeit.default_timer()\n",
    "with torch.no_grad():\n",
    "    for iteraction, (images, labels) in tqdm(enumerate(data), total=len(data)):\n",
    "        images, labels = images.to(\"cpu\"), labels.to(\"cpu\")\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(timeit.default_timer() - start_time)\n",
    "print('Accuracy of the network on the 10000 test images: %.4f %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b591026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "Epoch: [0]  [ 0/40]  eta: 0:11:05  lr: 0.0001  img/s: 7.704564943027618  loss: 1.1255 (1.1255)  acc1: 63.2812 (63.2812)  acc5: 88.2812 (88.2812)  time: 16.6266  data: 0.0131\n",
      "Epoch: [0]  [ 1/40]  eta: 0:10:11  lr: 0.0001  img/s: 8.70813929275048  loss: 0.8878 (1.0066)  acc1: 63.2812 (67.5781)  acc5: 88.2812 (92.9688)  time: 15.6701  data: 0.0139\n",
      "Epoch: [0]  [ 2/40]  eta: 0:10:30  lr: 0.0001  img/s: 6.96266787160591  loss: 0.9254 (0.9796)  acc1: 70.3125 (68.4896)  acc5: 96.8750 (94.2708)  time: 16.5797  data: 0.0143\n",
      "Epoch: [0]  [ 3/40]  eta: 0:09:47  lr: 0.0001  img/s: 9.294946305056063  loss: 0.9140 (0.9632)  acc1: 70.3125 (68.9453)  acc5: 94.5312 (94.3359)  time: 15.8813  data: 0.0145\n",
      "Epoch: [0]  [ 4/40]  eta: 0:09:16  lr: 0.0001  img/s: 9.295206367358771  loss: 0.9140 (0.9414)  acc1: 70.3125 (70.1562)  acc5: 94.5312 (93.9062)  time: 15.4620  data: 0.0145\n",
      "Epoch: [0]  [ 5/40]  eta: 0:09:10  lr: 0.0001  img/s: 7.4865759633279865  loss: 0.9140 (0.9376)  acc1: 70.3125 (70.1823)  acc5: 92.1875 (93.6198)  time: 15.7371  data: 0.0146\n",
      "Epoch: [0]  [ 6/40]  eta: 0:08:45  lr: 0.0001  img/s: 9.246678688287304  loss: 0.9140 (0.9089)  acc1: 70.3125 (71.3170)  acc5: 93.7500 (93.6384)  time: 15.4689  data: 0.0149\n",
      "Epoch: [0]  [ 7/40]  eta: 0:08:35  lr: 0.0001  img/s: 7.707254433914056  loss: 0.8878 (0.9027)  acc1: 70.3125 (71.4844)  acc5: 93.7500 (94.2383)  time: 15.6133  data: 0.0151\n",
      "Epoch: [0]  [ 8/40]  eta: 0:08:12  lr: 0.0001  img/s: 9.459422788894294  loss: 0.8878 (0.8982)  acc1: 71.8750 (71.8750)  acc5: 93.7500 (94.0972)  time: 15.3838  data: 0.0153\n",
      "Epoch: [0]  [ 9/40]  eta: 0:07:59  lr: 0.0001  img/s: 7.838393213151752  loss: 0.8878 (0.9090)  acc1: 70.3125 (71.2500)  acc5: 93.7500 (94.2969)  time: 15.4800  data: 0.0153\n",
      "Epoch: [0]  [10/40]  eta: 0:07:41  lr: 0.0001  img/s: 8.876181797689696  loss: 0.8878 (0.9005)  acc1: 71.8750 (71.5909)  acc5: 94.5312 (94.6023)  time: 15.3854  data: 0.0156\n",
      "Epoch: [0]  [11/40]  eta: 0:07:29  lr: 0.0001  img/s: 7.737583111283124  loss: 0.8624 (0.8928)  acc1: 71.0938 (71.5495)  acc5: 94.5312 (94.6615)  time: 15.4831  data: 0.0156\n",
      "Epoch: [0]  [12/40]  eta: 0:07:14  lr: 0.0001  img/s: 8.09760988711857  loss: 0.8878 (0.8952)  acc1: 71.0938 (71.2740)  acc5: 94.5312 (94.5913)  time: 15.5091  data: 0.0155\n",
      "Epoch: [0]  [13/40]  eta: 0:07:05  lr: 0.0001  img/s: 6.729498088745316  loss: 0.8878 (0.9104)  acc1: 70.3125 (70.8147)  acc5: 94.5312 (94.5871)  time: 15.7612  data: 0.0156\n",
      "Epoch: [0]  [14/40]  eta: 0:06:47  lr: 0.0001  img/s: 9.040981948631675  loss: 0.8878 (0.9043)  acc1: 71.0938 (71.0938)  acc5: 94.5312 (94.5312)  time: 15.6553  data: 0.0156\n",
      "Epoch: [0]  [15/40]  eta: 0:06:33  lr: 0.0001  img/s: 7.531483305911395  loss: 0.8624 (0.8947)  acc1: 71.0938 (71.3379)  acc5: 94.5312 (94.5801)  time: 15.7400  data: 0.0155\n",
      "Epoch: [0]  [16/40]  eta: 0:06:14  lr: 0.0001  img/s: 9.440364794474727  loss: 0.8878 (0.8963)  acc1: 71.0938 (71.2776)  acc5: 94.5312 (94.5772)  time: 15.6126  data: 0.0155\n",
      "Epoch: [0]  [17/40]  eta: 0:06:00  lr: 0.0001  img/s: 7.622753201676563  loss: 0.8624 (0.8879)  acc1: 71.0938 (71.5712)  acc5: 94.5312 (94.7049)  time: 15.6790  data: 0.0155\n",
      "Epoch: [0]  [18/40]  eta: 0:05:42  lr: 0.0001  img/s: 9.39454736056837  loss: 0.8624 (0.8842)  acc1: 71.8750 (71.7105)  acc5: 94.5312 (94.7368)  time: 15.5716  data: 0.0155\n",
      "Epoch: [0]  [19/40]  eta: 0:05:27  lr: 0.0001  img/s: 7.804978307846593  loss: 0.8592 (0.8731)  acc1: 71.8750 (72.1484)  acc5: 94.5312 (94.7656)  time: 15.6138  data: 0.0154\n",
      "Epoch: [0]  [20/40]  eta: 0:05:10  lr: 0.0001  img/s: 9.346590437949526  loss: 0.8545 (0.8681)  acc1: 72.6562 (72.3958)  acc5: 95.3125 (94.7917)  time: 15.4681  data: 0.0156\n",
      "Epoch: [0]  [21/40]  eta: 0:04:55  lr: 0.0001  img/s: 7.802868940002395  loss: 0.8192 (0.8611)  acc1: 74.2188 (72.5497)  acc5: 95.3125 (94.9574)  time: 15.5533  data: 0.0156\n",
      "Epoch: [0]  [22/40]  eta: 0:04:38  lr: 0.0001  img/s: 9.353910519292297  loss: 0.8176 (0.8537)  acc1: 75.0000 (72.7582)  acc5: 95.3125 (94.9728)  time: 15.3183  data: 0.0156\n",
      "Epoch: [0]  [23/40]  eta: 0:04:22  lr: 0.0001  img/s: 9.038695254173666  loss: 0.8149 (0.8406)  acc1: 75.0000 (73.2747)  acc5: 95.3125 (95.0846)  time: 15.3379  data: 0.0157\n",
      "Epoch: [0]  [24/40]  eta: 0:04:07  lr: 0.0001  img/s: 7.8752550685270855  loss: 0.8149 (0.8443)  acc1: 75.0000 (73.2188)  acc5: 95.3125 (94.9688)  time: 15.4621  data: 0.0157\n",
      "Epoch: [0]  [25/40]  eta: 0:03:50  lr: 0.0001  img/s: 9.290727636985565  loss: 0.8078 (0.8419)  acc1: 75.0000 (73.4375)  acc5: 95.3125 (94.9519)  time: 15.2960  data: 0.0157\n",
      "Epoch: [0]  [26/40]  eta: 0:03:36  lr: 0.0001  img/s: 7.875702389643094  loss: 0.8078 (0.8348)  acc1: 75.0000 (73.5822)  acc5: 95.3125 (94.9942)  time: 15.4165  data: 0.0156\n",
      "Epoch: [0]  [27/40]  eta: 0:03:19  lr: 0.0001  img/s: 9.20068930246839  loss: 0.7811 (0.8320)  acc1: 75.0000 (73.7165)  acc5: 95.3125 (95.1172)  time: 15.2816  data: 0.0155\n",
      "Epoch: [0]  [28/40]  eta: 0:03:05  lr: 0.0001  img/s: 7.601919661635785  loss: 0.7684 (0.8295)  acc1: 75.0000 (73.7069)  acc5: 95.3125 (95.1239)  time: 15.4468  data: 0.0154\n",
      "Epoch: [0]  [29/40]  eta: 0:02:49  lr: 0.0001  img/s: 9.368835599525408  loss: 0.7684 (0.8291)  acc1: 75.0000 (73.6719)  acc5: 95.3125 (95.2344)  time: 15.3134  data: 0.0154\n",
      "Epoch: [0]  [30/40]  eta: 0:02:34  lr: 0.0001  img/s: 7.797558997960688  loss: 0.7598 (0.8225)  acc1: 75.0000 (73.7903)  acc5: 95.3125 (95.3377)  time: 15.4129  data: 0.0152\n",
      "Epoch: [0]  [31/40]  eta: 0:02:18  lr: 0.0001  img/s: 9.132844341601242  loss: 0.7598 (0.8231)  acc1: 75.0000 (73.7549)  acc5: 95.3125 (95.3857)  time: 15.2866  data: 0.0152\n",
      "Epoch: [0]  [32/40]  eta: 0:02:03  lr: 0.0001  img/s: 7.855351627895843  loss: 0.7598 (0.8218)  acc1: 75.0000 (73.7453)  acc5: 95.3125 (95.3835)  time: 15.3109  data: 0.0152\n",
      "Epoch: [0]  [33/40]  eta: 0:01:47  lr: 0.0001  img/s: 9.557993055952789  loss: 0.7598 (0.8239)  acc1: 75.0000 (73.6903)  acc5: 95.3125 (95.3814)  time: 15.0294  data: 0.0151\n",
      "Epoch: [0]  [34/40]  eta: 0:01:32  lr: 0.0001  img/s: 8.051002301370756  loss: 0.7598 (0.8230)  acc1: 75.0000 (73.5938)  acc5: 95.3125 (95.4464)  time: 15.1164  data: 0.0150\n",
      "Epoch: [0]  [35/40]  eta: 0:01:16  lr: 0.0001  img/s: 9.639037916955862  loss: 0.7598 (0.8203)  acc1: 74.2188 (73.5894)  acc5: 95.3125 (95.4644)  time: 14.9305  data: 0.0150\n",
      "Epoch: [0]  [36/40]  eta: 0:01:00  lr: 0.0001  img/s: 9.546677227794886  loss: 0.7561 (0.8158)  acc1: 75.7812 (73.7120)  acc5: 96.0938 (95.5448)  time: 14.9230  data: 0.0150\n",
      "Epoch: [0]  [37/40]  eta: 0:00:45  lr: 0.0001  img/s: 7.979278612432158  loss: 0.7561 (0.8116)  acc1: 75.7812 (73.9515)  acc5: 96.0938 (95.5592)  time: 14.8855  data: 0.0150\n",
      "Epoch: [0]  [38/40]  eta: 0:00:30  lr: 0.0001  img/s: 9.483382073504309  loss: 0.7276 (0.8075)  acc1: 77.3438 (74.0986)  acc5: 96.0938 (95.6130)  time: 14.8791  data: 0.0150\n",
      "Epoch: [0]  [39/40]  eta: 0:00:14  lr: 0.0001  img/s: 6.065754579627324  loss: 0.7561 (0.8119)  acc1: 75.7812 (74.1000)  acc5: 96.0938 (95.6200)  time: 14.1244  data: 0.0144\n",
      "Epoch: [0] Total time: 0:09:54\n",
      "Epoch 2/15\n",
      "Epoch: [1]  [ 0/40]  eta: 0:10:56  lr: 1e-05  img/s: 7.806538805238508  loss: 0.7656 (0.7656)  acc1: 78.1250 (78.1250)  acc5: 93.7500 (93.7500)  time: 16.4173  data: 0.0208\n",
      "Epoch: [1]  [ 1/40]  eta: 0:09:37  lr: 1e-05  img/s: 9.694482164806855  loss: 0.5769 (0.6713)  acc1: 78.1250 (78.9062)  acc5: 93.7500 (96.0938)  time: 14.8192  data: 0.0193\n",
      "Epoch: [1]  [ 2/40]  eta: 0:09:37  lr: 1e-05  img/s: 8.019537841662174  loss: 0.7441 (0.6955)  acc1: 78.1250 (78.1250)  acc5: 98.4375 (96.8750)  time: 15.2046  data: 0.0176\n",
      "Epoch: [1]  [ 3/40]  eta: 0:09:04  lr: 1e-05  img/s: 9.68535681604272  loss: 0.6207 (0.6768)  acc1: 78.1250 (78.3203)  acc5: 97.6562 (97.0703)  time: 14.7117  data: 0.0175\n",
      "Epoch: [1]  [ 4/40]  eta: 0:08:59  lr: 1e-05  img/s: 7.995639318241959  loss: 0.6207 (0.6601)  acc1: 78.9062 (79.0625)  acc5: 98.4375 (97.5000)  time: 14.9739  data: 0.0168\n",
      "Epoch: [1]  [ 5/40]  eta: 0:08:33  lr: 1e-05  img/s: 9.705140658951304  loss: 0.6207 (0.6675)  acc1: 78.9062 (79.2969)  acc5: 97.6562 (97.3958)  time: 14.6788  data: 0.0164\n",
      "Epoch: [1]  [ 6/40]  eta: 0:08:13  lr: 1e-05  img/s: 9.458445865838849  loss: 0.6207 (0.6557)  acc1: 78.9062 (79.2411)  acc5: 97.6562 (97.4330)  time: 14.5172  data: 0.0162\n",
      "Epoch: [1]  [ 7/40]  eta: 0:08:03  lr: 1e-05  img/s: 8.24865234239542  loss: 0.6207 (0.6569)  acc1: 78.9062 (79.1016)  acc5: 97.6562 (97.5586)  time: 14.6440  data: 0.0159\n",
      "Epoch: [1]  [ 8/40]  eta: 0:07:46  lr: 1e-05  img/s: 9.207428974702637  loss: 0.6207 (0.6502)  acc1: 78.9062 (79.4271)  acc5: 97.6562 (97.3090)  time: 14.5631  data: 0.0157\n",
      "Epoch: [1]  [ 9/40]  eta: 0:07:42  lr: 1e-05  img/s: 7.079604876776223  loss: 0.6207 (0.6625)  acc1: 78.9062 (79.0625)  acc5: 97.6562 (97.2656)  time: 14.9163  data: 0.0156\n",
      "Epoch: [1]  [10/40]  eta: 0:07:23  lr: 1e-05  img/s: 9.449934083622596  loss: 0.6450 (0.6609)  acc1: 78.9062 (79.1193)  acc5: 97.6562 (97.1591)  time: 14.7930  data: 0.0156\n",
      "Epoch: [1]  [11/40]  eta: 0:07:11  lr: 1e-05  img/s: 8.092269593261609  loss: 0.6207 (0.6508)  acc1: 78.9062 (79.2318)  acc5: 96.8750 (97.0703)  time: 14.8796  data: 0.0155\n",
      "Epoch: [1]  [12/40]  eta: 0:06:53  lr: 1e-05  img/s: 9.574336634081485  loss: 0.6207 (0.6451)  acc1: 78.9062 (79.2067)  acc5: 97.6562 (97.1755)  time: 14.7645  data: 0.0154\n",
      "Epoch: [1]  [13/40]  eta: 0:06:41  lr: 1e-05  img/s: 7.950974793600008  loss: 0.6207 (0.6472)  acc1: 78.9062 (78.7946)  acc5: 96.8750 (96.9308)  time: 14.8609  data: 0.0154\n",
      "Epoch: [1]  [14/40]  eta: 0:06:24  lr: 1e-05  img/s: 9.41182202568776  loss: 0.6218 (0.6455)  acc1: 78.9062 (78.8542)  acc5: 97.6562 (97.0312)  time: 14.7778  data: 0.0153\n",
      "Epoch: [1]  [15/40]  eta: 0:06:10  lr: 1e-05  img/s: 8.165817450675274  loss: 0.6207 (0.6392)  acc1: 78.9062 (79.1016)  acc5: 97.6562 (97.1191)  time: 14.8347  data: 0.0152\n",
      "Epoch: [1]  [16/40]  eta: 0:05:54  lr: 1e-05  img/s: 9.13001516082979  loss: 0.6218 (0.6451)  acc1: 78.9062 (78.8603)  acc5: 97.6562 (97.1967)  time: 14.7877  data: 0.0152\n",
      "Epoch: [1]  [17/40]  eta: 0:05:38  lr: 1e-05  img/s: 9.42725542449066  loss: 0.6207 (0.6399)  acc1: 78.9062 (79.1233)  acc5: 97.6562 (97.3090)  time: 14.7213  data: 0.0152\n",
      "Epoch: [1]  [18/40]  eta: 0:05:25  lr: 1e-05  img/s: 7.779236575156018  loss: 0.6218 (0.6427)  acc1: 78.9062 (79.1118)  acc5: 97.6562 (97.3273)  time: 14.8133  data: 0.0152\n",
      "Epoch: [1]  [19/40]  eta: 0:05:10  lr: 1e-05  img/s: 9.285368746916996  loss: 0.6207 (0.6394)  acc1: 78.9062 (79.2188)  acc5: 97.6562 (97.2656)  time: 14.7626  data: 0.0152\n",
      "Epoch: [1]  [20/40]  eta: 0:04:56  lr: 1e-05  img/s: 7.866352269035912  loss: 0.6207 (0.6386)  acc1: 79.6875 (79.2411)  acc5: 97.6562 (97.1726)  time: 14.7561  data: 0.0149\n",
      "Epoch: [1]  [21/40]  eta: 0:04:41  lr: 1e-05  img/s: 8.786275084914319  loss: 0.6207 (0.6362)  acc1: 79.6875 (79.2969)  acc5: 97.6562 (97.1946)  time: 14.8242  data: 0.0147\n",
      "Epoch: [1]  [22/40]  eta: 0:04:27  lr: 1e-05  img/s: 8.019353725164741  loss: 0.6207 (0.6368)  acc1: 79.6875 (79.2799)  acc5: 97.6562 (97.1128)  time: 14.8243  data: 0.0148\n",
      "Epoch: [1]  [23/40]  eta: 0:04:11  lr: 1e-05  img/s: 9.482071440973751  loss: 0.5966 (0.6349)  acc1: 79.6875 (79.4922)  acc5: 96.8750 (97.0378)  time: 14.8383  data: 0.0147\n",
      "Epoch: [1]  [24/40]  eta: 0:03:58  lr: 1e-05  img/s: 7.763043852989625  loss: 0.6218 (0.6382)  acc1: 79.6875 (79.4375)  acc5: 96.8750 (97.0938)  time: 14.8624  data: 0.0147\n",
      "Epoch: [1]  [25/40]  eta: 0:03:42  lr: 1e-05  img/s: 9.528470995282042  loss: 0.6218 (0.6378)  acc1: 78.9062 (79.3870)  acc5: 97.6562 (97.1154)  time: 14.8746  data: 0.0147\n",
      "Epoch: [1]  [26/40]  eta: 0:03:28  lr: 1e-05  img/s: 7.9106277254484185  loss: 0.6218 (0.6355)  acc1: 78.9062 (79.3403)  acc5: 97.6562 (97.1644)  time: 15.0070  data: 0.0148\n",
      "Epoch: [1]  [27/40]  eta: 0:03:12  lr: 1e-05  img/s: 9.718705123905089  loss: 0.6218 (0.6402)  acc1: 78.9062 (79.2411)  acc5: 96.8750 (97.0982)  time: 14.8897  data: 0.0148\n",
      "Epoch: [1]  [28/40]  eta: 0:02:57  lr: 1e-05  img/s: 9.497546920848402  loss: 0.6227 (0.6424)  acc1: 78.9062 (79.0948)  acc5: 97.6562 (97.1175)  time: 14.8685  data: 0.0149\n",
      "Epoch: [1]  [29/40]  eta: 0:02:42  lr: 1e-05  img/s: 7.982804740380559  loss: 0.6227 (0.6442)  acc1: 78.9062 (79.1146)  acc5: 97.6562 (97.1094)  time: 14.7662  data: 0.0149\n",
      "Epoch: [1]  [30/40]  eta: 0:02:27  lr: 1e-05  img/s: 9.399459837051912  loss: 0.6218 (0.6410)  acc1: 78.9062 (79.3347)  acc5: 97.6562 (97.1522)  time: 14.7700  data: 0.0150\n",
      "Epoch: [1]  [31/40]  eta: 0:02:13  lr: 1e-05  img/s: 7.985173337029905  loss: 0.6227 (0.6419)  acc1: 78.9062 (79.2969)  acc5: 97.6562 (97.1436)  time: 14.7806  data: 0.0150\n",
      "Epoch: [1]  [32/40]  eta: 0:01:58  lr: 1e-05  img/s: 9.541948538046826  loss: 0.6288 (0.6448)  acc1: 78.9062 (79.2377)  acc5: 97.6562 (97.1354)  time: 14.7829  data: 0.0150\n",
      "Epoch: [1]  [33/40]  eta: 0:01:43  lr: 1e-05  img/s: 7.735035565586145  loss: 0.6288 (0.6465)  acc1: 78.9062 (79.2509)  acc5: 97.6562 (97.1507)  time: 14.8053  data: 0.0150\n",
      "Epoch: [1]  [34/40]  eta: 0:01:28  lr: 1e-05  img/s: 9.327545065785342  loss: 0.6437 (0.6464)  acc1: 78.9062 (79.3973)  acc5: 97.6562 (97.1429)  time: 14.8115  data: 0.0150\n",
      "Epoch: [1]  [35/40]  eta: 0:01:14  lr: 1e-05  img/s: 7.753521085773595  loss: 0.6437 (0.6461)  acc1: 78.9062 (79.4705)  acc5: 96.8750 (97.1137)  time: 14.8534  data: 0.0153\n",
      "Epoch: [1]  [36/40]  eta: 0:00:59  lr: 1e-05  img/s: 9.402650177820245  loss: 0.6371 (0.6425)  acc1: 79.6875 (79.6664)  acc5: 96.8750 (97.1495)  time: 14.8332  data: 0.0154\n",
      "Epoch: [1]  [37/40]  eta: 0:00:44  lr: 1e-05  img/s: 7.859101410840031  loss: 0.6437 (0.6433)  acc1: 78.9062 (79.6464)  acc5: 96.8750 (97.1217)  time: 14.9686  data: 0.0153\n",
      "Epoch: [1]  [38/40]  eta: 0:00:29  lr: 1e-05  img/s: 9.519932030137882  loss: 0.6371 (0.6398)  acc1: 79.6875 (79.7676)  acc5: 96.8750 (97.1955)  time: 14.8183  data: 0.0154\n",
      "Epoch: [1]  [39/40]  eta: 0:00:14  lr: 1e-05  img/s: 6.380338906217793  loss: 0.6437 (0.6572)  acc1: 78.9062 (79.7200)  acc5: 96.8750 (97.2000)  time: 14.1911  data: 0.0148\n",
      "Epoch: [1] Total time: 0:09:39\n",
      "Epoch 3/15\n",
      "Epoch: [2]  [ 0/40]  eta: 0:10:32  lr: 1.0000000000000002e-06  img/s: 8.099010174621647  loss: 0.7338 (0.7338)  acc1: 75.0000 (75.0000)  acc5: 95.3125 (95.3125)  time: 15.8173  data: 0.0129\n",
      "Epoch: [2]  [ 1/40]  eta: 0:09:38  lr: 1.0000000000000002e-06  img/s: 9.253813220594212  loss: 0.7194 (0.7266)  acc1: 75.0000 (76.1719)  acc5: 95.3125 (95.3125)  time: 14.8339  data: 0.0156\n",
      "Epoch: [2]  [ 2/40]  eta: 0:09:04  lr: 1.0000000000000002e-06  img/s: 9.638707210030516  loss: 0.7194 (0.6982)  acc1: 77.3438 (77.0833)  acc5: 95.3125 (95.8333)  time: 14.3219  data: 0.0164\n",
      "Epoch: [2]  [ 3/40]  eta: 0:09:06  lr: 1.0000000000000002e-06  img/s: 7.95252825518698  loss: 0.6423 (0.6842)  acc1: 77.3438 (77.5391)  acc5: 95.3125 (96.2891)  time: 14.7691  data: 0.0161\n",
      "Epoch: [2]  [ 4/40]  eta: 0:08:42  lr: 1.0000000000000002e-06  img/s: 9.45562622013732  loss: 0.6423 (0.6529)  acc1: 78.9062 (79.0625)  acc5: 96.8750 (96.7188)  time: 14.5258  data: 0.0160\n",
      "Epoch: [2]  [ 5/40]  eta: 0:08:38  lr: 1.0000000000000002e-06  img/s: 7.890765314308165  loss: 0.6412 (0.6486)  acc1: 78.9062 (79.2969)  acc5: 96.0938 (96.6146)  time: 14.8108  data: 0.0158\n",
      "Epoch: [2]  [ 6/40]  eta: 0:08:16  lr: 1.0000000000000002e-06  img/s: 9.602229941385843  loss: 0.6412 (0.6376)  acc1: 78.9062 (79.6875)  acc5: 96.8750 (96.7634)  time: 14.6017  data: 0.0159\n",
      "Epoch: [2]  [ 7/40]  eta: 0:08:06  lr: 1.0000000000000002e-06  img/s: 8.12527531744964  loss: 0.6412 (0.6393)  acc1: 78.9062 (79.7852)  acc5: 96.8750 (96.9727)  time: 14.7474  data: 0.0157\n",
      "Epoch: [2]  [ 8/40]  eta: 0:07:47  lr: 1.0000000000000002e-06  img/s: 9.573857377390253  loss: 0.6412 (0.6290)  acc1: 80.4688 (80.0347)  acc5: 97.6562 (97.0486)  time: 14.5960  data: 0.0156\n",
      "Epoch: [2]  [ 9/40]  eta: 0:07:38  lr: 1.0000000000000002e-06  img/s: 7.708512223628977  loss: 0.6412 (0.6454)  acc1: 78.9062 (79.4531)  acc5: 97.6562 (97.1875)  time: 14.7984  data: 0.0155\n",
      "Epoch: [2]  [10/40]  eta: 0:07:21  lr: 1.0000000000000002e-06  img/s: 9.3226729706099  loss: 0.6412 (0.6411)  acc1: 80.4688 (79.8295)  acc5: 97.6562 (97.2301)  time: 14.7027  data: 0.0156\n",
      "Epoch: [2]  [11/40]  eta: 0:07:04  lr: 1.0000000000000002e-06  img/s: 9.237748162003195  loss: 0.6276 (0.6376)  acc1: 80.4688 (79.8828)  acc5: 97.6562 (97.3307)  time: 14.6334  data: 0.0155\n",
      "Epoch: [2]  [12/40]  eta: 0:06:53  lr: 1.0000000000000002e-06  img/s: 7.885045348884208  loss: 0.6412 (0.6384)  acc1: 80.4688 (79.7476)  acc5: 97.6562 (97.3558)  time: 14.7577  data: 0.0156\n",
      "Epoch: [2]  [13/40]  eta: 0:06:36  lr: 1.0000000000000002e-06  img/s: 9.410532251487094  loss: 0.6412 (0.6544)  acc1: 78.9062 (79.1853)  acc5: 97.6562 (97.1540)  time: 14.6762  data: 0.0155\n",
      "Epoch: [2]  [14/40]  eta: 0:06:24  lr: 1.0000000000000002e-06  img/s: 7.966526134715365  loss: 0.6412 (0.6535)  acc1: 78.9062 (79.1667)  acc5: 97.6562 (97.1875)  time: 14.7700  data: 0.0155\n",
      "Epoch: [2]  [15/40]  eta: 0:06:09  lr: 1.0000000000000002e-06  img/s: 8.626604285445234  loss: 0.6405 (0.6492)  acc1: 78.9062 (79.5898)  acc5: 97.6562 (97.3145)  time: 14.7752  data: 0.0156\n",
      "Epoch: [2]  [16/40]  eta: 0:05:56  lr: 1.0000000000000002e-06  img/s: 7.893161063245376  loss: 0.6412 (0.6488)  acc1: 78.9062 (79.5496)  acc5: 97.6562 (97.3346)  time: 14.8609  data: 0.0155\n",
      "Epoch: [2]  [17/40]  eta: 0:05:40  lr: 1.0000000000000002e-06  img/s: 9.518378051420216  loss: 0.6412 (0.6504)  acc1: 78.9062 (79.3837)  acc5: 97.6562 (97.3958)  time: 14.7832  data: 0.0155\n",
      "Epoch: [2]  [18/40]  eta: 0:05:26  lr: 1.0000000000000002e-06  img/s: 7.907497795162941  loss: 0.6412 (0.6490)  acc1: 78.9062 (79.3586)  acc5: 97.6562 (97.4507)  time: 14.8580  data: 0.0155\n",
      "Epoch: [2]  [19/40]  eta: 0:05:10  lr: 1.0000000000000002e-06  img/s: 9.613034986145127  loss: 0.6412 (0.6493)  acc1: 78.9062 (79.2969)  acc5: 97.6562 (97.4219)  time: 14.7816  data: 0.0155\n",
      "Epoch: [2]  [20/40]  eta: 0:04:56  lr: 1.0000000000000002e-06  img/s: 7.913571394437394  loss: 0.6405 (0.6480)  acc1: 78.9062 (79.3899)  acc5: 97.6562 (97.3586)  time: 14.8002  data: 0.0156\n",
      "Epoch: [2]  [21/40]  eta: 0:04:40  lr: 1.0000000000000002e-06  img/s: 9.89804337583786  loss: 0.6276 (0.6404)  acc1: 78.9062 (79.7230)  acc5: 97.6562 (97.4077)  time: 14.7550  data: 0.0154\n",
      "Epoch: [2]  [22/40]  eta: 0:04:24  lr: 1.0000000000000002e-06  img/s: 9.703768018757899  loss: 0.6276 (0.6417)  acc1: 79.6875 (79.7215)  acc5: 97.6562 (97.3505)  time: 14.7504  data: 0.0152\n",
      "Epoch: [2]  [23/40]  eta: 0:04:10  lr: 1.0000000000000002e-06  img/s: 7.971080676829373  loss: 0.6241 (0.6391)  acc1: 80.4688 (79.7852)  acc5: 97.6562 (97.2656)  time: 14.7485  data: 0.0152\n",
      "Epoch: [2]  [24/40]  eta: 0:03:55  lr: 1.0000000000000002e-06  img/s: 9.685165318439296  loss: 0.6276 (0.6397)  acc1: 79.6875 (79.7812)  acc5: 97.6562 (97.2500)  time: 14.7324  data: 0.0152\n",
      "Epoch: [2]  [25/40]  eta: 0:03:41  lr: 1.0000000000000002e-06  img/s: 7.8839160353428275  loss: 0.6241 (0.6371)  acc1: 79.6875 (79.8377)  acc5: 97.6562 (97.2957)  time: 14.7331  data: 0.0152\n",
      "Epoch: [2]  [26/40]  eta: 0:03:25  lr: 1.0000000000000002e-06  img/s: 9.503592823561364  loss: 0.6241 (0.6352)  acc1: 79.6875 (79.8322)  acc5: 97.6562 (97.3090)  time: 14.7399  data: 0.0151\n",
      "Epoch: [2]  [27/40]  eta: 0:03:11  lr: 1.0000000000000002e-06  img/s: 7.831238218956538  loss: 0.6241 (0.6422)  acc1: 79.6875 (79.6317)  acc5: 97.6562 (97.3493)  time: 14.7696  data: 0.0152\n",
      "Epoch: [2]  [28/40]  eta: 0:02:56  lr: 1.0000000000000002e-06  img/s: 9.519602018293343  loss: 0.6405 (0.6454)  acc1: 78.9062 (79.5528)  acc5: 97.6562 (97.3060)  time: 14.7735  data: 0.0152\n",
      "Epoch: [2]  [29/40]  eta: 0:02:42  lr: 1.0000000000000002e-06  img/s: 8.048143125774333  loss: 0.6405 (0.6464)  acc1: 78.9062 (79.5312)  acc5: 97.6562 (97.3177)  time: 14.7384  data: 0.0152\n",
      "Epoch: [2]  [30/40]  eta: 0:02:27  lr: 1.0000000000000002e-06  img/s: 9.869222536113632  loss: 0.6405 (0.6399)  acc1: 78.9062 (79.7631)  acc5: 97.6562 (97.3034)  time: 14.7003  data: 0.0151\n",
      "Epoch: [2]  [31/40]  eta: 0:02:12  lr: 1.0000000000000002e-06  img/s: 9.402545115615801  loss: 0.6423 (0.6447)  acc1: 78.9062 (79.5410)  acc5: 97.6562 (97.2168)  time: 14.6881  data: 0.0151\n",
      "Epoch: [2]  [32/40]  eta: 0:01:57  lr: 1.0000000000000002e-06  img/s: 8.034384659620619  loss: 0.6405 (0.6442)  acc1: 78.9062 (79.5455)  acc5: 97.6562 (97.2301)  time: 14.6729  data: 0.0150\n",
      "Epoch: [2]  [33/40]  eta: 0:01:42  lr: 1.0000000000000002e-06  img/s: 9.755623952199395  loss: 0.6405 (0.6461)  acc1: 78.9062 (79.4347)  acc5: 97.6562 (97.2656)  time: 14.6489  data: 0.0150\n",
      "Epoch: [2]  [34/40]  eta: 0:01:28  lr: 1.0000000000000002e-06  img/s: 7.805918957725264  loss: 0.6423 (0.6469)  acc1: 79.6875 (79.4866)  acc5: 97.6562 (97.2321)  time: 14.6653  data: 0.0149\n",
      "Epoch: [2]  [35/40]  eta: 0:01:13  lr: 1.0000000000000002e-06  img/s: 9.695975280407032  loss: 0.6423 (0.6463)  acc1: 79.6875 (79.5356)  acc5: 96.8750 (97.2005)  time: 14.5834  data: 0.0148\n",
      "Epoch: [2]  [36/40]  eta: 0:00:58  lr: 1.0000000000000002e-06  img/s: 8.036853146282619  loss: 0.6298 (0.6450)  acc1: 79.6875 (79.6453)  acc5: 96.8750 (97.2551)  time: 14.5689  data: 0.0148\n",
      "Epoch: [2]  [37/40]  eta: 0:00:44  lr: 1.0000000000000002e-06  img/s: 9.560447257288692  loss: 0.6254 (0.6443)  acc1: 79.6875 (79.6875)  acc5: 96.8750 (97.3067)  time: 14.5659  data: 0.0148\n",
      "Epoch: [2]  [38/40]  eta: 0:00:29  lr: 1.0000000000000002e-06  img/s: 8.009375818752789  loss: 0.6254 (0.6413)  acc1: 79.6875 (79.8277)  acc5: 96.8750 (97.3558)  time: 14.5556  data: 0.0148\n",
      "Epoch: [2]  [39/40]  eta: 0:00:14  lr: 1.0000000000000002e-06  img/s: 5.83152304824457  loss: 0.6254 (0.6647)  acc1: 79.6875 (79.7800)  acc5: 97.6562 (97.3600)  time: 13.9578  data: 0.0141\n",
      "Epoch: [2] Total time: 0:09:34\n",
      "Epoch 4/15\n",
      "Epoch: [3]  [ 0/40]  eta: 0:08:52  lr: 1.0000000000000002e-07  img/s: 9.632670736901378  loss: 0.7696 (0.7696)  acc1: 77.3438 (77.3438)  acc5: 95.3125 (95.3125)  time: 13.3020  data: 0.0139\n",
      "Epoch: [3]  [ 1/40]  eta: 0:09:43  lr: 1.0000000000000002e-07  img/s: 7.703409025687648  loss: 0.7559 (0.7627)  acc1: 75.7812 (76.5625)  acc5: 95.3125 (96.0938)  time: 14.9664  data: 0.0143\n",
      "Epoch: [3]  [ 2/40]  eta: 0:09:11  lr: 1.0000000000000002e-07  img/s: 9.432202838942937  loss: 0.7559 (0.7388)  acc1: 77.3438 (77.3438)  acc5: 96.0938 (96.0938)  time: 14.5060  data: 0.0145\n",
      "Epoch: [3]  [ 3/40]  eta: 0:08:50  lr: 1.0000000000000002e-07  img/s: 9.233550593517803  loss: 0.6908 (0.6949)  acc1: 77.3438 (78.7109)  acc5: 96.0938 (96.6797)  time: 14.3487  data: 0.0145\n",
      "Epoch: [3]  [ 4/40]  eta: 0:08:52  lr: 1.0000000000000002e-07  img/s: 7.757320191787413  loss: 0.6908 (0.6879)  acc1: 78.9062 (78.9062)  acc5: 96.8750 (96.7188)  time: 14.7820  data: 0.0145\n",
      "Epoch: [3]  [ 5/40]  eta: 0:08:30  lr: 1.0000000000000002e-07  img/s: 9.462009392729561  loss: 0.6601 (0.6658)  acc1: 78.9062 (79.8177)  acc5: 96.0938 (96.6146)  time: 14.5754  data: 0.0145\n",
      "Epoch: [3]  [ 6/40]  eta: 0:08:23  lr: 1.0000000000000002e-07  img/s: 7.931623543716975  loss: 0.6601 (0.6558)  acc1: 79.6875 (79.7991)  acc5: 96.8750 (96.9866)  time: 14.8007  data: 0.0145\n",
      "Epoch: [3]  [ 7/40]  eta: 0:08:03  lr: 1.0000000000000002e-07  img/s: 9.437712032615012  loss: 0.6601 (0.6751)  acc1: 78.9062 (79.0039)  acc5: 96.8750 (96.9727)  time: 14.6479  data: 0.0146\n",
      "Epoch: [3]  [ 8/40]  eta: 0:07:53  lr: 1.0000000000000002e-07  img/s: 8.043902760682455  loss: 0.6601 (0.6606)  acc1: 79.6875 (79.1667)  acc5: 96.8750 (97.1354)  time: 14.7901  data: 0.0147\n",
      "Epoch: [3]  [ 9/40]  eta: 0:07:33  lr: 1.0000000000000002e-07  img/s: 9.625086633162356  loss: 0.6601 (0.6617)  acc1: 78.9062 (79.0625)  acc5: 96.8750 (97.1094)  time: 14.6424  data: 0.0147\n",
      "Epoch: [3]  [10/40]  eta: 0:07:23  lr: 1.0000000000000002e-07  img/s: 7.888011148483774  loss: 0.6601 (0.6539)  acc1: 79.6875 (79.4034)  acc5: 96.8750 (97.0170)  time: 14.7881  data: 0.0149\n",
      "Epoch: [3]  [11/40]  eta: 0:07:06  lr: 1.0000000000000002e-07  img/s: 9.218005537902254  loss: 0.6240 (0.6514)  acc1: 79.6875 (79.4922)  acc5: 96.8750 (97.0703)  time: 14.7141  data: 0.0149\n",
      "Epoch: [3]  [12/40]  eta: 0:06:55  lr: 1.0000000000000002e-07  img/s: 7.828352961050121  loss: 0.6601 (0.6552)  acc1: 79.6875 (79.1466)  acc5: 96.8750 (96.9952)  time: 14.8411  data: 0.0149\n",
      "Epoch: [3]  [13/40]  eta: 0:06:37  lr: 1.0000000000000002e-07  img/s: 9.6441091742848  loss: 0.6601 (0.6753)  acc1: 78.9062 (78.4598)  acc5: 96.8750 (96.8192)  time: 14.7302  data: 0.0149\n",
      "Epoch: [3]  [14/40]  eta: 0:06:20  lr: 1.0000000000000002e-07  img/s: 9.440717391311612  loss: 0.6721 (0.6767)  acc1: 79.6875 (78.5417)  acc5: 96.8750 (96.9792)  time: 14.6531  data: 0.0149\n",
      "Epoch: [3]  [15/40]  eta: 0:06:08  lr: 1.0000000000000002e-07  img/s: 8.014610699907342  loss: 0.6601 (0.6716)  acc1: 79.6875 (78.7598)  acc5: 96.8750 (97.0703)  time: 14.7363  data: 0.0149\n",
      "Epoch: [3]  [16/40]  eta: 0:05:51  lr: 1.0000000000000002e-07  img/s: 9.49694713832051  loss: 0.6601 (0.6686)  acc1: 79.6875 (78.9522)  acc5: 96.8750 (97.0588)  time: 14.6631  data: 0.0149\n",
      "Epoch: [3]  [17/40]  eta: 0:05:38  lr: 1.0000000000000002e-07  img/s: 8.004770738041978  loss: 0.6240 (0.6610)  acc1: 79.6875 (79.2535)  acc5: 96.8750 (97.1354)  time: 14.7377  data: 0.0148\n",
      "Epoch: [3]  [18/40]  eta: 0:05:23  lr: 1.0000000000000002e-07  img/s: 9.078422884452218  loss: 0.6601 (0.6641)  acc1: 79.6875 (79.2763)  acc5: 96.8750 (97.0806)  time: 14.7048  data: 0.0148\n",
      "Epoch: [3]  [19/40]  eta: 0:05:10  lr: 1.0000000000000002e-07  img/s: 7.694646801162449  loss: 0.6460 (0.6632)  acc1: 79.6875 (79.1406)  acc5: 96.8750 (97.1094)  time: 14.8020  data: 0.0148\n",
      "Epoch: [3]  [20/40]  eta: 0:04:54  lr: 1.0000000000000002e-07  img/s: 9.710152643947971  loss: 0.6240 (0.6606)  acc1: 79.6875 (79.2783)  acc5: 96.8750 (97.0610)  time: 14.7970  data: 0.0150\n",
      "Epoch: [3]  [21/40]  eta: 0:04:41  lr: 1.0000000000000002e-07  img/s: 7.925809459210983  loss: 0.6204 (0.6565)  acc1: 79.6875 (79.4744)  acc5: 96.8750 (97.0526)  time: 14.7737  data: 0.0150\n",
      "Epoch: [3]  [22/40]  eta: 0:04:24  lr: 1.0000000000000002e-07  img/s: 9.82570376458498  loss: 0.6204 (0.6562)  acc1: 80.4688 (79.5516)  acc5: 96.8750 (97.0109)  time: 14.7465  data: 0.0150\n",
      "Epoch: [3]  [23/40]  eta: 0:04:09  lr: 1.0000000000000002e-07  img/s: 9.412707320895263  loss: 0.6204 (0.6536)  acc1: 80.4688 (79.5898)  acc5: 96.8750 (97.0378)  time: 14.7333  data: 0.0150\n",
      "Epoch: [3]  [24/40]  eta: 0:03:55  lr: 1.0000000000000002e-07  img/s: 8.075775304024136  loss: 0.6204 (0.6564)  acc1: 80.4688 (79.6562)  acc5: 96.8750 (97.0625)  time: 14.7008  data: 0.0150\n",
      "Epoch: [3]  [25/40]  eta: 0:03:40  lr: 1.0000000000000002e-07  img/s: 9.516281060624564  loss: 0.6204 (0.6534)  acc1: 80.4688 (79.5974)  acc5: 96.8750 (97.0252)  time: 14.6971  data: 0.0152\n",
      "Epoch: [3]  [26/40]  eta: 0:03:26  lr: 1.0000000000000002e-07  img/s: 7.673065836728207  loss: 0.6204 (0.6519)  acc1: 80.4688 (79.5428)  acc5: 96.8750 (96.9907)  time: 14.7243  data: 0.0152\n",
      "Epoch: [3]  [27/40]  eta: 0:03:11  lr: 1.0000000000000002e-07  img/s: 9.54285220751604  loss: 0.6122 (0.6470)  acc1: 80.4688 (79.6596)  acc5: 96.8750 (96.9587)  time: 14.7167  data: 0.0151\n",
      "Epoch: [3]  [28/40]  eta: 0:02:57  lr: 1.0000000000000002e-07  img/s: 7.539437396332385  loss: 0.6204 (0.6465)  acc1: 80.4688 (79.6336)  acc5: 96.8750 (96.9558)  time: 14.7699  data: 0.0151\n",
      "Epoch: [3]  [29/40]  eta: 0:02:41  lr: 1.0000000000000002e-07  img/s: 9.657776383829354  loss: 0.6204 (0.6510)  acc1: 80.4688 (79.5312)  acc5: 96.0938 (96.8750)  time: 14.7677  data: 0.0151\n",
      "Epoch: [3]  [30/40]  eta: 0:02:27  lr: 1.0000000000000002e-07  img/s: 7.563346131798158  loss: 0.6204 (0.6486)  acc1: 80.4688 (79.6371)  acc5: 96.8750 (96.9002)  time: 14.8024  data: 0.0149\n",
      "Epoch: [3]  [31/40]  eta: 0:02:12  lr: 1.0000000000000002e-07  img/s: 9.69680537702812  loss: 0.6204 (0.6543)  acc1: 79.6875 (79.4678)  acc5: 96.0938 (96.8506)  time: 14.7681  data: 0.0150\n",
      "Epoch: [3]  [32/40]  eta: 0:01:58  lr: 1.0000000000000002e-07  img/s: 8.079012417613761  loss: 0.6204 (0.6549)  acc1: 80.4688 (79.5218)  acc5: 96.8750 (96.8750)  time: 14.7428  data: 0.0150\n",
      "Epoch: [3]  [33/40]  eta: 0:01:43  lr: 1.0000000000000002e-07  img/s: 9.37897041591218  loss: 0.6204 (0.6606)  acc1: 80.4688 (79.3658)  acc5: 96.8750 (96.8980)  time: 14.7615  data: 0.0149\n",
      "Epoch: [3]  [34/40]  eta: 0:01:29  lr: 1.0000000000000002e-07  img/s: 7.034969874702875  loss: 0.6204 (0.6643)  acc1: 80.4688 (79.2857)  acc5: 96.8750 (96.9420)  time: 14.9933  data: 0.0149\n",
      "Epoch: [3]  [35/40]  eta: 0:01:14  lr: 1.0000000000000002e-07  img/s: 9.291764938736277  loss: 0.6326 (0.6664)  acc1: 79.6875 (79.1884)  acc5: 96.8750 (96.8967)  time: 14.8836  data: 0.0150\n",
      "Epoch: [3]  [36/40]  eta: 0:00:59  lr: 1.0000000000000002e-07  img/s: 9.50001136028635  loss: 0.6326 (0.6652)  acc1: 79.6875 (79.2441)  acc5: 96.0938 (96.8750)  time: 14.8834  data: 0.0150\n",
      "Epoch: [3]  [37/40]  eta: 0:00:44  lr: 1.0000000000000002e-07  img/s: 7.963269731063482  loss: 0.6326 (0.6622)  acc1: 79.6875 (79.3586)  acc5: 96.0938 (96.8544)  time: 14.8879  data: 0.0153\n",
      "Epoch: [3]  [38/40]  eta: 0:00:29  lr: 1.0000000000000002e-07  img/s: 9.71813724585124  loss: 0.6222 (0.6582)  acc1: 80.4688 (79.5473)  acc5: 96.0938 (96.9151)  time: 14.8415  data: 0.0153\n",
      "Epoch: [3]  [39/40]  eta: 0:00:14  lr: 1.0000000000000002e-07  img/s: 6.810970892882552  loss: 0.6222 (0.6801)  acc1: 80.4688 (79.5000)  acc5: 96.0938 (96.9000)  time: 14.0679  data: 0.0147\n",
      "Epoch: [3] Total time: 0:09:37\n",
      "Epoch 5/15\n",
      "Epoch: [4]  [ 0/40]  eta: 0:10:47  lr: 1.0000000000000004e-08  img/s: 7.909122281316573  loss: 0.6212 (0.6212)  acc1: 78.1250 (78.1250)  acc5: 94.5312 (94.5312)  time: 16.1981  data: 0.0142\n",
      "Epoch: [4]  [ 1/40]  eta: 0:09:37  lr: 1.0000000000000004e-08  img/s: 9.548302444291556  loss: 0.6212 (0.6318)  acc1: 78.1250 (78.5156)  acc5: 94.5312 (96.8750)  time: 14.8089  data: 0.0142\n",
      "Epoch: [4]  [ 2/40]  eta: 0:09:39  lr: 1.0000000000000004e-08  img/s: 7.952822409468084  loss: 0.6423 (0.6567)  acc1: 78.1250 (77.3438)  acc5: 97.6562 (97.1354)  time: 15.2423  data: 0.0142\n",
      "Epoch: [4]  [ 3/40]  eta: 0:09:05  lr: 1.0000000000000004e-08  img/s: 9.677028605883883  loss: 0.6212 (0.6414)  acc1: 78.1250 (78.3203)  acc5: 97.6562 (97.8516)  time: 14.7421  data: 0.0142\n",
      "Epoch: [4]  [ 4/40]  eta: 0:09:00  lr: 1.0000000000000004e-08  img/s: 7.932742069458237  loss: 0.6423 (0.6446)  acc1: 78.1250 (78.2812)  acc5: 97.6562 (97.8125)  time: 15.0245  data: 0.0151\n",
      "Epoch: [4]  [ 5/40]  eta: 0:08:37  lr: 1.0000000000000004e-08  img/s: 9.43922036430578  loss: 0.6423 (0.6452)  acc1: 78.1250 (78.6458)  acc5: 97.6562 (97.7865)  time: 14.7828  data: 0.0149\n",
      "Epoch: [4]  [ 6/40]  eta: 0:08:16  lr: 1.0000000000000004e-08  img/s: 9.491428328567384  loss: 0.6423 (0.6322)  acc1: 78.9062 (79.1295)  acc5: 97.6562 (97.7679)  time: 14.5996  data: 0.0149\n",
      "Epoch: [4]  [ 7/40]  eta: 0:08:08  lr: 1.0000000000000004e-08  img/s: 7.949350844589144  loss: 0.6423 (0.6431)  acc1: 78.1250 (78.5156)  acc5: 97.6562 (97.8516)  time: 14.7893  data: 0.0149\n",
      "Epoch: [4]  [ 8/40]  eta: 0:07:48  lr: 1.0000000000000004e-08  img/s: 9.591550431376064  loss: 0.6423 (0.6332)  acc1: 78.9062 (78.9062)  acc5: 97.6562 (97.9167)  time: 14.6303  data: 0.0147\n",
      "Epoch: [4]  [ 9/40]  eta: 0:07:37  lr: 1.0000000000000004e-08  img/s: 7.983447775890799  loss: 0.6423 (0.6387)  acc1: 78.1250 (78.6719)  acc5: 97.6562 (97.7344)  time: 14.7722  data: 0.0148\n",
      "Epoch: [4]  [10/40]  eta: 0:07:19  lr: 1.0000000000000004e-08  img/s: 9.635865381336387  loss: 0.6423 (0.6319)  acc1: 78.9062 (78.9773)  acc5: 97.6562 (97.9403)  time: 14.6381  data: 0.0147\n",
      "Epoch: [4]  [11/40]  eta: 0:07:11  lr: 1.0000000000000004e-08  img/s: 7.380141019605571  loss: 0.6212 (0.6199)  acc1: 78.9062 (79.4922)  acc5: 97.6562 (97.9818)  time: 14.8648  data: 0.0147\n",
      "Epoch: [4]  [12/40]  eta: 0:06:54  lr: 1.0000000000000004e-08  img/s: 9.165505144689494  loss: 0.6423 (0.6252)  acc1: 78.9062 (79.3870)  acc5: 97.6562 (97.7163)  time: 14.7967  data: 0.0147\n",
      "Epoch: [4]  [13/40]  eta: 0:06:42  lr: 1.0000000000000004e-08  img/s: 7.958179420909613  loss: 0.6423 (0.6375)  acc1: 78.1250 (79.2969)  acc5: 97.6562 (97.6004)  time: 14.8897  data: 0.0147\n",
      "Epoch: [4]  [14/40]  eta: 0:06:24  lr: 1.0000000000000004e-08  img/s: 9.54418105494212  loss: 0.6423 (0.6361)  acc1: 78.9062 (79.4271)  acc5: 97.6562 (97.4479)  time: 14.7921  data: 0.0146\n",
      "Epoch: [4]  [15/40]  eta: 0:06:12  lr: 1.0000000000000004e-08  img/s: 7.686610298955043  loss: 0.6212 (0.6260)  acc1: 78.9062 (79.8340)  acc5: 97.6562 (97.4609)  time: 14.9093  data: 0.0146\n",
      "Epoch: [4]  [16/40]  eta: 0:05:56  lr: 1.0000000000000004e-08  img/s: 9.293259954665336  loss: 0.6423 (0.6277)  acc1: 80.4688 (79.9173)  acc5: 97.6562 (97.5643)  time: 14.8433  data: 0.0146\n",
      "Epoch: [4]  [17/40]  eta: 0:05:39  lr: 1.0000000000000004e-08  img/s: 9.532942745016205  loss: 0.6212 (0.6258)  acc1: 80.4688 (79.9913)  acc5: 97.6562 (97.4826)  time: 14.7656  data: 0.0148\n",
      "Epoch: [4]  [18/40]  eta: 0:05:26  lr: 1.0000000000000004e-08  img/s: 7.893630151084092  loss: 0.6423 (0.6292)  acc1: 80.4688 (79.9342)  acc5: 97.6562 (97.5329)  time: 14.8427  data: 0.0147\n",
      "Epoch: [4]  [19/40]  eta: 0:05:10  lr: 1.0000000000000004e-08  img/s: 9.527580868503884  loss: 0.6212 (0.6273)  acc1: 80.4688 (80.0000)  acc5: 97.6562 (97.5000)  time: 14.7733  data: 0.0150\n",
      "Epoch: [4]  [20/40]  eta: 0:04:56  lr: 1.0000000000000004e-08  img/s: 7.898252055514141  loss: 0.6423 (0.6284)  acc1: 81.2500 (80.1339)  acc5: 97.6562 (97.4702)  time: 14.7744  data: 0.0150\n",
      "Epoch: [4]  [21/40]  eta: 0:04:41  lr: 1.0000000000000004e-08  img/s: 9.320054865790535  loss: 0.6168 (0.6223)  acc1: 81.2500 (80.2557)  acc5: 97.6562 (97.4432)  time: 14.7909  data: 0.0151\n",
      "Epoch: [4]  [22/40]  eta: 0:04:28  lr: 1.0000000000000004e-08  img/s: 7.444579073811461  loss: 0.6168 (0.6221)  acc1: 81.2500 (80.2310)  acc5: 97.6562 (97.4524)  time: 14.8459  data: 0.0151\n",
      "Epoch: [4]  [23/40]  eta: 0:04:12  lr: 1.0000000000000004e-08  img/s: 9.526336762166402  loss: 0.6168 (0.6213)  acc1: 81.2500 (80.2734)  acc5: 97.6562 (97.2982)  time: 14.8563  data: 0.0151\n",
      "Epoch: [4]  [24/40]  eta: 0:03:58  lr: 1.0000000000000004e-08  img/s: 8.114503004652361  loss: 0.6168 (0.6242)  acc1: 81.2500 (80.1250)  acc5: 97.6562 (97.3438)  time: 14.8380  data: 0.0149\n",
      "Epoch: [4]  [25/40]  eta: 0:03:42  lr: 1.0000000000000004e-08  img/s: 9.546005196444725  loss: 0.6168 (0.6244)  acc1: 81.2500 (80.0781)  acc5: 97.6562 (97.3558)  time: 14.8305  data: 0.0149\n",
      "Epoch: [4]  [26/40]  eta: 0:03:28  lr: 1.0000000000000004e-08  img/s: 7.841049274237176  loss: 0.6177 (0.6244)  acc1: 81.2500 (80.0637)  acc5: 96.8750 (97.3090)  time: 14.9724  data: 0.0149\n",
      "Epoch: [4]  [27/40]  eta: 0:03:12  lr: 1.0000000000000004e-08  img/s: 9.48264958206521  loss: 0.6168 (0.6205)  acc1: 81.2500 (80.1897)  acc5: 96.8750 (97.2935)  time: 14.8423  data: 0.0150\n",
      "Epoch: [4]  [28/40]  eta: 0:02:57  lr: 1.0000000000000004e-08  img/s: 9.441466331953915  loss: 0.6177 (0.6215)  acc1: 81.2500 (80.1185)  acc5: 96.8750 (97.1983)  time: 14.8530  data: 0.0151\n",
      "Epoch: [4]  [29/40]  eta: 0:02:43  lr: 1.0000000000000004e-08  img/s: 7.656320319693007  loss: 0.6177 (0.6223)  acc1: 81.2500 (80.2344)  acc5: 96.8750 (97.1615)  time: 14.8872  data: 0.0151\n",
      "Epoch: [4]  [30/40]  eta: 0:02:27  lr: 1.0000000000000004e-08  img/s: 9.613427625832287  loss: 0.6177 (0.6163)  acc1: 81.2500 (80.4688)  acc5: 96.8750 (97.1774)  time: 14.8888  data: 0.0151\n",
      "Epoch: [4]  [31/40]  eta: 0:02:13  lr: 1.0000000000000004e-08  img/s: 7.938408687635867  loss: 0.6254 (0.6214)  acc1: 81.2500 (80.2734)  acc5: 96.8750 (97.2412)  time: 14.8278  data: 0.0151\n",
      "Epoch: [4]  [32/40]  eta: 0:01:58  lr: 1.0000000000000004e-08  img/s: 9.573886059732624  loss: 0.6181 (0.6213)  acc1: 81.2500 (80.2320)  acc5: 96.8750 (97.2301)  time: 14.7980  data: 0.0151\n",
      "Epoch: [4]  [33/40]  eta: 0:01:43  lr: 1.0000000000000004e-08  img/s: 7.74611630757935  loss: 0.6181 (0.6252)  acc1: 81.2500 (80.0781)  acc5: 96.8750 (97.2197)  time: 14.8202  data: 0.0152\n",
      "Epoch: [4]  [34/40]  eta: 0:01:28  lr: 1.0000000000000004e-08  img/s: 9.661884155812865  loss: 0.6254 (0.6259)  acc1: 79.6875 (80.0446)  acc5: 96.8750 (97.2545)  time: 14.8120  data: 0.0153\n",
      "Epoch: [4]  [35/40]  eta: 0:01:14  lr: 1.0000000000000004e-08  img/s: 7.684459154951108  loss: 0.6254 (0.6243)  acc1: 79.6875 (80.0998)  acc5: 96.8750 (97.1571)  time: 14.8123  data: 0.0153\n",
      "Epoch: [4]  [36/40]  eta: 0:00:59  lr: 1.0000000000000004e-08  img/s: 9.207248172449148  loss: 0.6181 (0.6197)  acc1: 79.6875 (80.2576)  acc5: 96.8750 (97.1284)  time: 14.8187  data: 0.0152\n",
      "Epoch: [4]  [37/40]  eta: 0:00:44  lr: 1.0000000000000004e-08  img/s: 7.710244874071695  loss: 0.6254 (0.6204)  acc1: 79.6875 (80.2426)  acc5: 96.8750 (97.0806)  time: 14.9772  data: 0.0151\n",
      "Epoch: [4]  [38/40]  eta: 0:00:29  lr: 1.0000000000000004e-08  img/s: 9.338002413575662  loss: 0.6254 (0.6220)  acc1: 79.6875 (80.1082)  acc5: 96.8750 (97.1154)  time: 14.8519  data: 0.0151\n",
      "Epoch: [4]  [39/40]  eta: 0:00:14  lr: 1.0000000000000004e-08  img/s: 6.8246422671660305  loss: 0.6308 (0.6238)  acc1: 79.6875 (80.0800)  acc5: 96.8750 (97.1200)  time: 14.2378  data: 0.0142\n",
      "Epoch: [4] Total time: 0:09:40\n",
      "Epoch 6/15\n",
      "Epoch: [5]  [ 0/40]  eta: 0:10:56  lr: 1.0000000000000005e-09  img/s: 7.800630820295665  loss: 0.6956 (0.6956)  acc1: 74.2188 (74.2188)  acc5: 96.0938 (96.0938)  time: 16.4233  data: 0.0144\n",
      "Epoch: [5]  [ 1/40]  eta: 0:09:43  lr: 1.0000000000000005e-09  img/s: 9.482317460532771  loss: 0.6956 (0.7090)  acc1: 74.2188 (75.0000)  acc5: 96.0938 (97.6562)  time: 14.9709  data: 0.0170\n",
      "Epoch: [5]  [ 2/40]  eta: 0:09:10  lr: 1.0000000000000005e-09  img/s: 9.487829366042934  loss: 0.6990 (0.7057)  acc1: 74.2188 (74.7396)  acc5: 97.6562 (97.6562)  time: 14.4826  data: 0.0163\n",
      "Epoch: [5]  [ 3/40]  eta: 0:09:13  lr: 1.0000000000000005e-09  img/s: 7.810840722014517  loss: 0.6956 (0.6701)  acc1: 74.2188 (76.7578)  acc5: 97.6562 (97.8516)  time: 14.9624  data: 0.0159\n",
      "Epoch: [5]  [ 4/40]  eta: 0:08:48  lr: 1.0000000000000005e-09  img/s: 9.477695130568106  loss: 0.6956 (0.6476)  acc1: 75.7812 (77.5000)  acc5: 98.4375 (97.9688)  time: 14.6742  data: 0.0158\n",
      "Epoch: [5]  [ 5/40]  eta: 0:08:41  lr: 1.0000000000000005e-09  img/s: 8.00423798835579  loss: 0.6628 (0.6501)  acc1: 75.7812 (77.8646)  acc5: 97.6562 (97.9167)  time: 14.8963  data: 0.0157\n",
      "Epoch: [5]  [ 6/40]  eta: 0:08:19  lr: 1.0000000000000005e-09  img/s: 9.451065806676944  loss: 0.6628 (0.6411)  acc1: 79.6875 (78.2366)  acc5: 97.6562 (97.8795)  time: 14.7054  data: 0.0159\n",
      "Epoch: [5]  [ 7/40]  eta: 0:08:11  lr: 1.0000000000000005e-09  img/s: 7.849098842029256  loss: 0.6628 (0.6447)  acc1: 75.7812 (77.9297)  acc5: 97.6562 (97.9492)  time: 14.9074  data: 0.0156\n",
      "Epoch: [5]  [ 8/40]  eta: 0:07:51  lr: 1.0000000000000005e-09  img/s: 9.68741327161635  loss: 0.6628 (0.6393)  acc1: 79.6875 (78.2118)  acc5: 97.6562 (97.9167)  time: 14.7209  data: 0.0157\n",
      "Epoch: [5]  [ 9/40]  eta: 0:07:39  lr: 1.0000000000000005e-09  img/s: 8.14667393296082  loss: 0.6628 (0.6489)  acc1: 75.7812 (77.9688)  acc5: 97.6562 (97.7344)  time: 14.8216  data: 0.0157\n",
      "Epoch: [5]  [10/40]  eta: 0:07:21  lr: 1.0000000000000005e-09  img/s: 9.45613568424652  loss: 0.6628 (0.6403)  acc1: 79.6875 (78.2670)  acc5: 97.6562 (97.7983)  time: 14.7063  data: 0.0158\n",
      "Epoch: [5]  [11/40]  eta: 0:07:03  lr: 1.0000000000000005e-09  img/s: 9.599792179791946  loss: 0.5960 (0.6333)  acc1: 79.6875 (78.4505)  acc5: 97.6562 (97.7214)  time: 14.5931  data: 0.0157\n",
      "Epoch: [5]  [12/40]  eta: 0:06:51  lr: 1.0000000000000005e-09  img/s: 7.9764656405712655  loss: 0.5960 (0.6295)  acc1: 80.4688 (78.7260)  acc5: 97.6562 (97.7764)  time: 14.7061  data: 0.0156\n",
      "Epoch: [5]  [13/40]  eta: 0:06:34  lr: 1.0000000000000005e-09  img/s: 9.641680580125504  loss: 0.5960 (0.6473)  acc1: 79.6875 (78.2924)  acc5: 97.6562 (97.6562)  time: 14.6049  data: 0.0155\n",
      "Epoch: [5]  [14/40]  eta: 0:06:22  lr: 1.0000000000000005e-09  img/s: 7.807995681154733  loss: 0.6289 (0.6461)  acc1: 79.6875 (78.3333)  acc5: 97.6562 (97.7083)  time: 14.7251  data: 0.0154\n",
      "Epoch: [5]  [15/40]  eta: 0:06:06  lr: 1.0000000000000005e-09  img/s: 9.19938076194054  loss: 0.5966 (0.6430)  acc1: 79.6875 (78.6133)  acc5: 97.6562 (97.5586)  time: 14.6757  data: 0.0158\n",
      "Epoch: [5]  [16/40]  eta: 0:05:56  lr: 1.0000000000000005e-09  img/s: 7.162959773125339  loss: 0.6289 (0.6482)  acc1: 79.6875 (78.1710)  acc5: 97.6562 (97.5643)  time: 14.8645  data: 0.0157\n",
      "Epoch: [5]  [17/40]  eta: 0:05:39  lr: 1.0000000000000005e-09  img/s: 9.761638065796351  loss: 0.5966 (0.6401)  acc1: 79.6875 (78.3420)  acc5: 97.6562 (97.6997)  time: 14.7679  data: 0.0156\n",
      "Epoch: [5]  [18/40]  eta: 0:05:26  lr: 1.0000000000000005e-09  img/s: 7.791618228858046  loss: 0.6289 (0.6466)  acc1: 79.6875 (78.2895)  acc5: 97.6562 (97.6974)  time: 14.8560  data: 0.0155\n",
      "Epoch: [5]  [19/40]  eta: 0:05:10  lr: 1.0000000000000005e-09  img/s: 9.39979111688595  loss: 0.6289 (0.6459)  acc1: 78.9062 (78.3203)  acc5: 97.6562 (97.7344)  time: 14.7949  data: 0.0155\n",
      "Epoch: [5]  [20/40]  eta: 0:04:57  lr: 1.0000000000000005e-09  img/s: 7.813194658326086  loss: 0.6289 (0.6471)  acc1: 79.6875 (78.4226)  acc5: 97.6562 (97.6935)  time: 14.7937  data: 0.0156\n",
      "Epoch: [5]  [21/40]  eta: 0:04:41  lr: 1.0000000000000005e-09  img/s: 9.609542060196258  loss: 0.5966 (0.6412)  acc1: 80.4688 (78.6577)  acc5: 97.6562 (97.7983)  time: 14.7845  data: 0.0154\n",
      "Epoch: [5]  [22/40]  eta: 0:04:28  lr: 1.0000000000000005e-09  img/s: 7.35371598716946  loss: 0.5966 (0.6401)  acc1: 80.4688 (78.8723)  acc5: 97.6562 (97.8261)  time: 14.9802  data: 0.0154\n",
      "Epoch: [5]  [23/40]  eta: 0:04:12  lr: 1.0000000000000005e-09  img/s: 9.58872794586942  loss: 0.5966 (0.6351)  acc1: 80.4688 (79.0365)  acc5: 97.6562 (97.7865)  time: 14.8283  data: 0.0154\n",
      "Epoch: [5]  [24/40]  eta: 0:03:56  lr: 1.0000000000000005e-09  img/s: 9.312653900637498  loss: 0.6179 (0.6408)  acc1: 80.4688 (78.8438)  acc5: 97.6562 (97.7188)  time: 14.8402  data: 0.0153\n",
      "Epoch: [5]  [25/40]  eta: 0:03:42  lr: 1.0000000000000005e-09  img/s: 7.975998505678338  loss: 0.6179 (0.6410)  acc1: 80.4688 (78.9062)  acc5: 97.6562 (97.6562)  time: 14.8431  data: 0.0154\n",
      "Epoch: [5]  [26/40]  eta: 0:03:26  lr: 1.0000000000000005e-09  img/s: 9.88486012524128  loss: 0.6179 (0.6380)  acc1: 80.4688 (79.0799)  acc5: 97.6562 (97.6273)  time: 14.8133  data: 0.0153\n",
      "Epoch: [5]  [27/40]  eta: 0:03:13  lr: 1.0000000000000005e-09  img/s: 7.758057903622764  loss: 0.6179 (0.6392)  acc1: 80.4688 (78.9621)  acc5: 97.6562 (97.6842)  time: 14.8231  data: 0.0155\n",
      "Epoch: [5]  [28/40]  eta: 0:02:57  lr: 1.0000000000000005e-09  img/s: 9.402460639401966  loss: 0.6179 (0.6363)  acc1: 80.4688 (79.0679)  acc5: 97.6562 (97.7101)  time: 14.8430  data: 0.0154\n",
      "Epoch: [5]  [29/40]  eta: 0:02:43  lr: 1.0000000000000005e-09  img/s: 7.733496279970902  loss: 0.6179 (0.6384)  acc1: 80.4688 (78.9583)  acc5: 97.6562 (97.6302)  time: 14.8849  data: 0.0153\n",
      "Epoch: [5]  [30/40]  eta: 0:02:28  lr: 1.0000000000000005e-09  img/s: 8.659840985155851  loss: 0.6179 (0.6314)  acc1: 80.4688 (79.2339)  acc5: 97.6562 (97.6815)  time: 14.9470  data: 0.0152\n",
      "Epoch: [5]  [31/40]  eta: 0:02:14  lr: 1.0000000000000005e-09  img/s: 7.74781749084109  loss: 0.6289 (0.6352)  acc1: 80.4688 (79.1016)  acc5: 97.6562 (97.7051)  time: 15.1065  data: 0.0153\n",
      "Epoch: [5]  [32/40]  eta: 0:01:59  lr: 1.0000000000000005e-09  img/s: 8.847584607901975  loss: 0.6322 (0.6381)  acc1: 78.9062 (79.0956)  acc5: 97.6562 (97.6562)  time: 15.0275  data: 0.0153\n",
      "Epoch: [5]  [33/40]  eta: 0:01:44  lr: 1.0000000000000005e-09  img/s: 7.811618200726658  loss: 0.6322 (0.6417)  acc1: 78.9062 (79.0211)  acc5: 97.6562 (97.6333)  time: 15.1830  data: 0.0153\n",
      "Epoch: [5]  [34/40]  eta: 0:01:29  lr: 1.0000000000000005e-09  img/s: 9.937300951685554  loss: 0.6476 (0.6437)  acc1: 78.9062 (78.9732)  acc5: 97.6562 (97.6339)  time: 15.0074  data: 0.0154\n",
      "Epoch: [5]  [35/40]  eta: 0:01:14  lr: 1.0000000000000005e-09  img/s: 7.810208829153533  loss: 0.6476 (0.6420)  acc1: 78.9062 (79.0365)  acc5: 97.6562 (97.6562)  time: 15.1308  data: 0.0150\n",
      "Epoch: [5]  [36/40]  eta: 0:00:59  lr: 1.0000000000000005e-09  img/s: 9.212212311250521  loss: 0.6322 (0.6406)  acc1: 80.4688 (79.0963)  acc5: 97.6562 (97.6774)  time: 14.9320  data: 0.0150\n",
      "Epoch: [5]  [37/40]  eta: 0:00:44  lr: 1.0000000000000005e-09  img/s: 9.519136495731432  loss: 0.6322 (0.6386)  acc1: 79.6875 (79.1118)  acc5: 97.6562 (97.6562)  time: 14.9488  data: 0.0151\n",
      "Epoch: [5]  [38/40]  eta: 0:00:29  lr: 1.0000000000000005e-09  img/s: 7.5140158824586925  loss: 0.6322 (0.6402)  acc1: 79.6875 (79.1066)  acc5: 97.6562 (97.6562)  time: 14.9792  data: 0.0151\n",
      "Epoch: [5]  [39/40]  eta: 0:00:14  lr: 1.0000000000000005e-09  img/s: 7.572056371083304  loss: 0.6476 (0.6485)  acc1: 79.6875 (79.0800)  acc5: 97.6562 (97.6600)  time: 14.3505  data: 0.0144\n",
      "Epoch: [5] Total time: 0:09:42\n",
      "Epoch 7/15\n",
      "Epoch: [6]  [ 0/40]  eta: 0:09:01  lr: 1.0000000000000006e-10  img/s: 9.464291245893634  loss: 0.7455 (0.7455)  acc1: 81.2500 (81.2500)  acc5: 92.9688 (92.9688)  time: 13.5381  data: 0.0136\n",
      "Epoch: [6]  [ 1/40]  eta: 0:09:44  lr: 1.0000000000000006e-10  img/s: 7.790387887581732  loss: 0.6339 (0.6897)  acc1: 78.9062 (80.0781)  acc5: 92.9688 (96.0938)  time: 14.9914  data: 0.0139\n",
      "Epoch: [6]  [ 2/40]  eta: 0:09:11  lr: 1.0000000000000006e-10  img/s: 9.428717196112524  loss: 0.7102 (0.6965)  acc1: 78.9062 (79.4271)  acc5: 96.0938 (96.0938)  time: 14.5243  data: 0.0141\n",
      "Epoch: [6]  [ 3/40]  eta: 0:09:20  lr: 1.0000000000000006e-10  img/s: 7.504828701216442  loss: 0.7102 (0.7014)  acc1: 78.9062 (79.4922)  acc5: 96.0938 (96.6797)  time: 15.1611  data: 0.0145\n",
      "Epoch: [6]  [ 4/40]  eta: 0:08:52  lr: 1.0000000000000006e-10  img/s: 9.582575495696684  loss: 0.7102 (0.6808)  acc1: 79.6875 (79.5312)  acc5: 98.4375 (97.1875)  time: 14.8033  data: 0.0145\n",
      "Epoch: [6]  [ 5/40]  eta: 0:08:47  lr: 1.0000000000000006e-10  img/s: 7.832905002388371  loss: 0.6339 (0.6627)  acc1: 79.6875 (79.8177)  acc5: 96.8750 (97.1354)  time: 15.0620  data: 0.0145\n",
      "Epoch: [6]  [ 6/40]  eta: 0:08:24  lr: 1.0000000000000006e-10  img/s: 9.447759733673125  loss: 0.6339 (0.6432)  acc1: 79.6875 (80.3571)  acc5: 97.6562 (97.2098)  time: 14.8482  data: 0.0148\n",
      "Epoch: [6]  [ 7/40]  eta: 0:08:18  lr: 1.0000000000000006e-10  img/s: 7.52721988921222  loss: 0.6339 (0.6471)  acc1: 79.6875 (80.0781)  acc5: 96.8750 (96.9727)  time: 15.1195  data: 0.0147\n",
      "Epoch: [6]  [ 8/40]  eta: 0:07:57  lr: 1.0000000000000006e-10  img/s: 9.5328702973227  loss: 0.6339 (0.6365)  acc1: 79.6875 (80.7292)  acc5: 96.8750 (96.8750)  time: 14.9331  data: 0.0147\n",
      "Epoch: [6]  [ 9/40]  eta: 0:07:47  lr: 1.0000000000000006e-10  img/s: 7.82951345446962  loss: 0.6339 (0.6452)  acc1: 79.6875 (80.1562)  acc5: 96.0938 (96.6406)  time: 15.0760  data: 0.0146\n",
      "Epoch: [6]  [10/40]  eta: 0:07:29  lr: 1.0000000000000006e-10  img/s: 9.234561823649736  loss: 0.6674 (0.6472)  acc1: 79.6875 (79.9006)  acc5: 96.8750 (96.8040)  time: 14.9673  data: 0.0150\n",
      "Epoch: [6]  [11/40]  eta: 0:07:10  lr: 1.0000000000000006e-10  img/s: 9.586284538697601  loss: 0.6394 (0.6466)  acc1: 78.9062 (79.6875)  acc5: 96.8750 (97.0052)  time: 14.8339  data: 0.0149\n",
      "Epoch: [6]  [12/40]  eta: 0:06:58  lr: 1.0000000000000006e-10  img/s: 7.848161753924856  loss: 0.6674 (0.6503)  acc1: 78.9062 (79.6274)  acc5: 97.6562 (97.0553)  time: 14.9486  data: 0.0150\n",
      "Epoch: [6]  [13/40]  eta: 0:06:40  lr: 1.0000000000000006e-10  img/s: 9.646228048611286  loss: 0.6674 (0.6630)  acc1: 78.9062 (79.0179)  acc5: 96.8750 (96.9308)  time: 14.8296  data: 0.0149\n",
      "Epoch: [6]  [14/40]  eta: 0:06:27  lr: 1.0000000000000006e-10  img/s: 7.954224211549439  loss: 0.6674 (0.6622)  acc1: 78.9062 (78.9062)  acc5: 96.8750 (96.9271)  time: 14.9148  data: 0.0149\n",
      "Epoch: [6]  [15/40]  eta: 0:06:11  lr: 1.0000000000000006e-10  img/s: 9.251339974715885  loss: 0.6508 (0.6510)  acc1: 78.9062 (79.5410)  acc5: 96.8750 (97.0703)  time: 14.8483  data: 0.0149\n",
      "Epoch: [6]  [16/40]  eta: 0:05:58  lr: 1.0000000000000006e-10  img/s: 7.821767902154333  loss: 0.6508 (0.6498)  acc1: 78.9062 (79.5496)  acc5: 97.6562 (97.1507)  time: 14.9384  data: 0.0149\n",
      "Epoch: [6]  [17/40]  eta: 0:05:41  lr: 1.0000000000000006e-10  img/s: 9.497154281537263  loss: 0.6394 (0.6476)  acc1: 78.9062 (79.7309)  acc5: 97.6562 (97.2222)  time: 14.8581  data: 0.0149\n",
      "Epoch: [6]  [18/40]  eta: 0:05:26  lr: 1.0000000000000006e-10  img/s: 8.653636572285015  loss: 0.6394 (0.6464)  acc1: 79.6875 (79.8109)  acc5: 97.6562 (97.2862)  time: 14.8553  data: 0.0149\n",
      "Epoch: [6]  [19/40]  eta: 0:05:13  lr: 1.0000000000000006e-10  img/s: 7.887481427456737  loss: 0.6339 (0.6403)  acc1: 78.9062 (79.7656)  acc5: 97.6562 (97.3438)  time: 14.9247  data: 0.0149\n",
      "Epoch: [6]  [20/40]  eta: 0:04:59  lr: 1.0000000000000006e-10  img/s: 8.070460243043  loss: 0.6339 (0.6413)  acc1: 78.9062 (79.7991)  acc5: 97.6562 (97.2470)  time: 15.0416  data: 0.0150\n",
      "Epoch: [6]  [21/40]  eta: 0:04:43  lr: 1.0000000000000006e-10  img/s: 9.594757293549945  loss: 0.6308 (0.6334)  acc1: 79.6875 (80.0426)  acc5: 97.6562 (97.2656)  time: 14.8871  data: 0.0150\n",
      "Epoch: [6]  [22/40]  eta: 0:04:26  lr: 1.0000000000000006e-10  img/s: 9.642613284393835  loss: 0.6308 (0.6336)  acc1: 79.6875 (79.9932)  acc5: 97.6562 (97.1807)  time: 14.8721  data: 0.0150\n",
      "Epoch: [6]  [23/40]  eta: 0:04:13  lr: 1.0000000000000006e-10  img/s: 7.81814951339831  loss: 0.6236 (0.6319)  acc1: 79.6875 (80.0781)  acc5: 97.6562 (97.2331)  time: 14.8379  data: 0.0150\n",
      "Epoch: [6]  [24/40]  eta: 0:03:57  lr: 1.0000000000000006e-10  img/s: 9.71981890646855  loss: 0.6308 (0.6346)  acc1: 78.9062 (80.0000)  acc5: 97.6562 (97.2500)  time: 14.8285  data: 0.0150\n",
      "Epoch: [6]  [25/40]  eta: 0:03:43  lr: 1.0000000000000006e-10  img/s: 7.979293792306649  loss: 0.6384 (0.6365)  acc1: 78.9062 (79.9880)  acc5: 97.6562 (97.3257)  time: 14.8135  data: 0.0150\n",
      "Epoch: [6]  [26/40]  eta: 0:03:27  lr: 1.0000000000000006e-10  img/s: 9.59184449300121  loss: 0.6384 (0.6355)  acc1: 78.9062 (80.0347)  acc5: 97.6562 (97.3380)  time: 14.8032  data: 0.0149\n",
      "Epoch: [6]  [27/40]  eta: 0:03:13  lr: 1.0000000000000006e-10  img/s: 8.10887450894963  loss: 0.6384 (0.6360)  acc1: 78.9062 (79.9386)  acc5: 97.6562 (97.4051)  time: 14.7423  data: 0.0150\n",
      "Epoch: [6]  [28/40]  eta: 0:02:57  lr: 1.0000000000000006e-10  img/s: 9.891473145295741  loss: 0.6384 (0.6354)  acc1: 78.9062 (79.9300)  acc5: 97.6562 (97.3869)  time: 14.7179  data: 0.0150\n",
      "Epoch: [6]  [29/40]  eta: 0:02:43  lr: 1.0000000000000006e-10  img/s: 8.030623988460883  loss: 0.6384 (0.6408)  acc1: 78.9062 (79.6875)  acc5: 97.6562 (97.2917)  time: 14.6975  data: 0.0150\n",
      "Epoch: [6]  [30/40]  eta: 0:02:27  lr: 1.0000000000000006e-10  img/s: 9.725688338512436  loss: 0.6308 (0.6337)  acc1: 79.6875 (79.8387)  acc5: 97.6562 (97.3790)  time: 14.6623  data: 0.0148\n",
      "Epoch: [6]  [31/40]  eta: 0:02:12  lr: 1.0000000000000006e-10  img/s: 9.373793392634882  loss: 0.6308 (0.6349)  acc1: 79.6875 (79.8096)  acc5: 97.6562 (97.2900)  time: 14.6774  data: 0.0148\n",
      "Epoch: [6]  [32/40]  eta: 0:01:58  lr: 1.0000000000000006e-10  img/s: 7.833033799730733  loss: 0.6308 (0.6366)  acc1: 79.6875 (79.7112)  acc5: 97.6562 (97.2538)  time: 14.6791  data: 0.0149\n",
      "Epoch: [6]  [33/40]  eta: 0:01:43  lr: 1.0000000000000006e-10  img/s: 9.571403454142791  loss: 0.6308 (0.6382)  acc1: 79.6875 (79.6645)  acc5: 97.6562 (97.2426)  time: 14.6843  data: 0.0149\n",
      "Epoch: [6]  [34/40]  eta: 0:01:28  lr: 1.0000000000000006e-10  img/s: 7.654166341844779  loss: 0.6308 (0.6385)  acc1: 79.6875 (79.6652)  acc5: 97.6562 (97.2321)  time: 14.7158  data: 0.0148\n",
      "Epoch: [6]  [35/40]  eta: 0:01:13  lr: 1.0000000000000006e-10  img/s: 9.499157131932128  loss: 0.6308 (0.6368)  acc1: 79.6875 (79.7743)  acc5: 97.6562 (97.2222)  time: 14.6977  data: 0.0148\n",
      "Epoch: [6]  [36/40]  eta: 0:00:59  lr: 1.0000000000000006e-10  img/s: 7.840287108206279  loss: 0.6236 (0.6344)  acc1: 79.6875 (79.8564)  acc5: 96.8750 (97.2128)  time: 14.6958  data: 0.0148\n",
      "Epoch: [6]  [37/40]  eta: 0:00:44  lr: 1.0000000000000006e-10  img/s: 9.443004597949376  loss: 0.6384 (0.6357)  acc1: 79.6875 (79.7697)  acc5: 96.8750 (97.1834)  time: 14.6996  data: 0.0148\n",
      "Epoch: [6]  [38/40]  eta: 0:00:29  lr: 1.0000000000000006e-10  img/s: 7.174436039263677  loss: 0.6384 (0.6348)  acc1: 79.6875 (79.7676)  acc5: 96.8750 (97.2356)  time: 14.8521  data: 0.0148\n",
      "Epoch: [6]  [39/40]  eta: 0:00:14  lr: 1.0000000000000006e-10  img/s: 7.853737089272205  loss: 0.6489 (0.6603)  acc1: 79.6875 (79.7200)  acc5: 96.8750 (97.2000)  time: 14.0909  data: 0.0141\n",
      "Epoch: [6] Total time: 0:09:40\n",
      "Epoch 8/15\n",
      "Epoch: [7]  [ 0/40]  eta: 0:08:54  lr: 1.0000000000000006e-11  img/s: 9.58770752510985  loss: 0.6361 (0.6361)  acc1: 80.4688 (80.4688)  acc5: 93.7500 (93.7500)  time: 13.3635  data: 0.0131\n",
      "Epoch: [7]  [ 1/40]  eta: 0:09:29  lr: 1.0000000000000006e-11  img/s: 8.08875482534153  loss: 0.6361 (0.6555)  acc1: 78.9062 (79.6875)  acc5: 93.7500 (95.3125)  time: 14.6018  data: 0.0144\n",
      "Epoch: [7]  [ 2/40]  eta: 0:09:00  lr: 1.0000000000000006e-11  img/s: 9.539397714933706  loss: 0.6749 (0.7003)  acc1: 78.9062 (78.9062)  acc5: 96.0938 (95.5729)  time: 14.2122  data: 0.0145\n",
      "Epoch: [7]  [ 3/40]  eta: 0:08:39  lr: 1.0000000000000006e-11  img/s: 9.484900850385364  loss: 0.6361 (0.6597)  acc1: 78.9062 (79.8828)  acc5: 96.0938 (96.2891)  time: 14.0365  data: 0.0145\n",
      "Epoch: [7]  [ 4/40]  eta: 0:08:42  lr: 1.0000000000000006e-11  img/s: 7.80315598294381  loss: 0.6361 (0.6288)  acc1: 80.4688 (80.7812)  acc5: 96.8750 (96.8750)  time: 14.5131  data: 0.0147\n",
      "Epoch: [7]  [ 5/40]  eta: 0:08:22  lr: 1.0000000000000006e-11  img/s: 9.489883137485402  loss: 0.6361 (0.6367)  acc1: 78.9062 (80.3385)  acc5: 96.0938 (96.7448)  time: 14.3446  data: 0.0146\n",
      "Epoch: [7]  [ 6/40]  eta: 0:08:16  lr: 1.0000000000000006e-11  img/s: 7.91283984938702  loss: 0.6361 (0.6292)  acc1: 80.4688 (80.4688)  acc5: 96.0938 (96.6518)  time: 14.6083  data: 0.0146\n",
      "Epoch: [7]  [ 7/40]  eta: 0:07:57  lr: 1.0000000000000006e-11  img/s: 9.560546343684452  loss: 0.6157 (0.6275)  acc1: 79.6875 (80.3711)  acc5: 96.0938 (96.6797)  time: 14.4576  data: 0.0146\n",
      "Epoch: [7]  [ 8/40]  eta: 0:07:49  lr: 1.0000000000000006e-11  img/s: 7.873002253085262  loss: 0.6157 (0.6198)  acc1: 80.4688 (80.6424)  acc5: 96.8750 (96.8750)  time: 14.6593  data: 0.0146\n",
      "Epoch: [7]  [ 9/40]  eta: 0:07:31  lr: 1.0000000000000006e-11  img/s: 9.405519052568874  loss: 0.6157 (0.6358)  acc1: 79.6875 (80.2344)  acc5: 96.0938 (96.5625)  time: 14.5558  data: 0.0146\n",
      "Epoch: [7]  [10/40]  eta: 0:07:21  lr: 1.0000000000000006e-11  img/s: 7.817300730117786  loss: 0.6361 (0.6434)  acc1: 79.6875 (79.9006)  acc5: 96.8750 (96.8750)  time: 14.7225  data: 0.0147\n",
      "Epoch: [7]  [11/40]  eta: 0:07:03  lr: 1.0000000000000006e-11  img/s: 9.543302070763445  loss: 0.6157 (0.6305)  acc1: 79.6875 (80.4036)  acc5: 96.8750 (97.0052)  time: 14.6145  data: 0.0147\n",
      "Epoch: [7]  [12/40]  eta: 0:06:53  lr: 1.0000000000000006e-11  img/s: 7.768881078843916  loss: 0.6157 (0.6256)  acc1: 80.4688 (80.5889)  acc5: 96.8750 (97.0553)  time: 14.7588  data: 0.0147\n",
      "Epoch: [7]  [13/40]  eta: 0:06:35  lr: 1.0000000000000006e-11  img/s: 9.670286288765242  loss: 0.6157 (0.6294)  acc1: 79.6875 (80.3013)  acc5: 96.8750 (96.9866)  time: 14.6511  data: 0.0147\n",
      "Epoch: [7]  [14/40]  eta: 0:06:19  lr: 1.0000000000000006e-11  img/s: 9.426184342471954  loss: 0.6361 (0.6355)  acc1: 79.6875 (80.0521)  acc5: 96.8750 (97.0833)  time: 14.5806  data: 0.0146\n",
      "Epoch: [7]  [15/40]  eta: 0:06:07  lr: 1.0000000000000006e-11  img/s: 7.857979401025597  loss: 0.6157 (0.6290)  acc1: 79.6875 (80.2246)  acc5: 96.8750 (97.2656)  time: 14.6883  data: 0.0146\n",
      "Epoch: [7]  [16/40]  eta: 0:05:50  lr: 1.0000000000000006e-11  img/s: 9.415261006087684  loss: 0.6361 (0.6351)  acc1: 79.6875 (79.8713)  acc5: 97.6562 (97.3346)  time: 14.6248  data: 0.0146\n",
      "Epoch: [7]  [17/40]  eta: 0:05:38  lr: 1.0000000000000006e-11  img/s: 7.820819441364247  loss: 0.6157 (0.6322)  acc1: 79.6875 (79.9479)  acc5: 97.6562 (97.3958)  time: 14.7224  data: 0.0146\n",
      "Epoch: [7]  [18/40]  eta: 0:05:22  lr: 1.0000000000000006e-11  img/s: 9.427835343742153  loss: 0.6361 (0.6383)  acc1: 79.6875 (79.7286)  acc5: 97.6562 (97.4095)  time: 14.6628  data: 0.0145\n",
      "Epoch: [7]  [19/40]  eta: 0:05:09  lr: 1.0000000000000006e-11  img/s: 8.019307847093074  loss: 0.6361 (0.6385)  acc1: 78.9062 (79.6875)  acc5: 97.6562 (97.4219)  time: 14.7285  data: 0.0146\n",
      "Epoch: [7]  [20/40]  eta: 0:04:53  lr: 1.0000000000000006e-11  img/s: 9.463139339629567  loss: 0.6417 (0.6426)  acc1: 78.9062 (79.6503)  acc5: 97.6562 (97.5074)  time: 14.7374  data: 0.0147\n",
      "Epoch: [7]  [21/40]  eta: 0:04:40  lr: 1.0000000000000006e-11  img/s: 7.828789147488351  loss: 0.6157 (0.6352)  acc1: 78.9062 (79.8651)  acc5: 98.4375 (97.5497)  time: 14.7637  data: 0.0147\n",
      "Epoch: [7]  [22/40]  eta: 0:04:24  lr: 1.0000000000000006e-11  img/s: 9.252054066425696  loss: 0.6157 (0.6369)  acc1: 78.9062 (79.7894)  acc5: 98.4375 (97.5543)  time: 14.7845  data: 0.0147\n",
      "Epoch: [7]  [23/40]  eta: 0:04:11  lr: 1.0000000000000006e-11  img/s: 7.710496461517619  loss: 0.6157 (0.6338)  acc1: 78.9062 (80.0456)  acc5: 97.6562 (97.4609)  time: 14.9399  data: 0.0148\n",
      "Epoch: [7]  [24/40]  eta: 0:03:55  lr: 1.0000000000000006e-11  img/s: 9.544426575107266  loss: 0.6359 (0.6338)  acc1: 78.9062 (80.0625)  acc5: 97.6562 (97.4688)  time: 14.7903  data: 0.0148\n",
      "Epoch: [7]  [25/40]  eta: 0:03:40  lr: 1.0000000000000006e-11  img/s: 9.24331287814584  loss: 0.6307 (0.6337)  acc1: 79.6875 (80.1082)  acc5: 97.6562 (97.5361)  time: 14.8083  data: 0.0149\n",
      "Epoch: [7]  [26/40]  eta: 0:03:26  lr: 1.0000000000000006e-11  img/s: 7.994031236720407  loss: 0.6307 (0.6336)  acc1: 78.9062 (80.0347)  acc5: 98.4375 (97.5984)  time: 14.8001  data: 0.0148\n",
      "Epoch: [7]  [27/40]  eta: 0:03:11  lr: 1.0000000000000006e-11  img/s: 9.376701346098494  loss: 0.6307 (0.6317)  acc1: 78.9062 (80.1060)  acc5: 98.4375 (97.5167)  time: 14.8132  data: 0.0148\n",
      "Epoch: [7]  [28/40]  eta: 0:02:57  lr: 1.0000000000000006e-11  img/s: 7.963132126955549  loss: 0.6359 (0.6321)  acc1: 78.9062 (80.1455)  acc5: 97.6562 (97.4677)  time: 14.8040  data: 0.0149\n",
      "Epoch: [7]  [29/40]  eta: 0:02:41  lr: 1.0000000000000006e-11  img/s: 9.663412469551806  loss: 0.6359 (0.6360)  acc1: 78.9062 (79.9219)  acc5: 97.6562 (97.4219)  time: 14.7858  data: 0.0148\n",
      "Epoch: [7]  [30/40]  eta: 0:02:27  lr: 1.0000000000000006e-11  img/s: 8.058105556065374  loss: 0.6307 (0.6302)  acc1: 80.4688 (80.1411)  acc5: 97.6562 (97.4294)  time: 14.7614  data: 0.0148\n",
      "Epoch: [7]  [31/40]  eta: 0:02:12  lr: 1.0000000000000006e-11  img/s: 9.113493569758552  loss: 0.6359 (0.6342)  acc1: 78.9062 (79.9805)  acc5: 97.6562 (97.3877)  time: 14.7931  data: 0.0149\n",
      "Epoch: [7]  [32/40]  eta: 0:01:58  lr: 1.0000000000000006e-11  img/s: 7.288473113430806  loss: 0.6417 (0.6361)  acc1: 78.9062 (79.9479)  acc5: 97.6562 (97.3248)  time: 14.8474  data: 0.0149\n",
      "Epoch: [7]  [33/40]  eta: 0:01:43  lr: 1.0000000000000006e-11  img/s: 9.160661228430179  loss: 0.6417 (0.6403)  acc1: 78.9062 (79.8254)  acc5: 97.6562 (97.3346)  time: 14.8842  data: 0.0149\n",
      "Epoch: [7]  [34/40]  eta: 0:01:28  lr: 1.0000000000000006e-11  img/s: 8.003227701153333  loss: 0.6417 (0.6440)  acc1: 78.9062 (79.7321)  acc5: 97.6562 (97.2768)  time: 15.0049  data: 0.0149\n",
      "Epoch: [7]  [35/40]  eta: 0:01:13  lr: 1.0000000000000006e-11  img/s: 9.57828878747655  loss: 0.6417 (0.6408)  acc1: 78.9062 (79.8394)  acc5: 97.6562 (97.2656)  time: 14.8587  data: 0.0150\n",
      "Epoch: [7]  [36/40]  eta: 0:00:59  lr: 1.0000000000000006e-11  img/s: 8.83528828586466  loss: 0.6359 (0.6367)  acc1: 78.9062 (79.9409)  acc5: 97.6562 (97.2762)  time: 14.9033  data: 0.0150\n",
      "Epoch: [7]  [37/40]  eta: 0:00:44  lr: 1.0000000000000006e-11  img/s: 9.414205358914238  loss: 0.6359 (0.6359)  acc1: 78.9062 (79.9342)  acc5: 97.6562 (97.2656)  time: 14.8388  data: 0.0889\n",
      "Epoch: [7]  [38/40]  eta: 0:00:29  lr: 1.0000000000000006e-11  img/s: 9.056212454702568  loss: 0.6307 (0.6342)  acc1: 79.6875 (79.9880)  acc5: 97.6562 (97.2756)  time: 14.8667  data: 0.0890\n",
      "Epoch: [7]  [39/40]  eta: 0:00:14  lr: 1.0000000000000006e-11  img/s: 6.768730618116298  loss: 0.6307 (0.6505)  acc1: 79.6875 (79.9400)  acc5: 97.6562 (97.2800)  time: 14.1270  data: 0.0883\n",
      "Epoch: [7] Total time: 0:09:37\n",
      "Epoch 9/15\n",
      "Epoch: [8]  [ 0/40]  eta: 0:10:49  lr: 1.0000000000000006e-12  img/s: 7.884640620741206  loss: 0.6539 (0.6539)  acc1: 78.9062 (78.9062)  acc5: 97.6562 (97.6562)  time: 16.2491  data: 0.0150\n",
      "Epoch: [8]  [ 1/40]  eta: 0:09:46  lr: 1.0000000000000006e-12  img/s: 9.26203912949988  loss: 0.6539 (0.6792)  acc1: 76.5625 (77.7344)  acc5: 97.6562 (97.6562)  time: 15.0415  data: 0.0145\n",
      "Epoch: [8]  [ 2/40]  eta: 0:09:54  lr: 1.0000000000000006e-12  img/s: 7.609027792058962  loss: 0.6742 (0.6775)  acc1: 78.9062 (78.1250)  acc5: 97.6562 (97.6562)  time: 15.6405  data: 0.0151\n",
      "Epoch: [8]  [ 3/40]  eta: 0:09:15  lr: 1.0000000000000006e-12  img/s: 9.778335258689685  loss: 0.6539 (0.6472)  acc1: 78.9062 (79.1016)  acc5: 97.6562 (97.6562)  time: 15.0066  data: 0.0150\n",
      "Epoch: [8]  [ 4/40]  eta: 0:09:08  lr: 1.0000000000000006e-12  img/s: 7.9042330189906655  loss: 0.6539 (0.6170)  acc1: 78.9062 (80.1562)  acc5: 97.6562 (97.6562)  time: 15.2471  data: 0.0150\n",
      "Epoch: [8]  [ 5/40]  eta: 0:08:41  lr: 1.0000000000000006e-12  img/s: 9.740300731977268  loss: 0.6539 (0.6411)  acc1: 78.9062 (79.4271)  acc5: 97.6562 (97.5260)  time: 14.8985  data: 0.0150\n",
      "Epoch: [8]  [ 6/40]  eta: 0:08:35  lr: 1.0000000000000006e-12  img/s: 7.673847608368489  loss: 0.6539 (0.6280)  acc1: 78.9062 (79.6875)  acc5: 97.6562 (97.8795)  time: 15.1554  data: 0.0152\n",
      "Epoch: [8]  [ 7/40]  eta: 0:08:12  lr: 1.0000000000000006e-12  img/s: 9.632864139525736  loss: 0.5690 (0.6206)  acc1: 78.9062 (79.9805)  acc5: 97.6562 (97.7539)  time: 14.9237  data: 0.0151\n",
      "Epoch: [8]  [ 8/40]  eta: 0:08:01  lr: 1.0000000000000006e-12  img/s: 7.989731634051001  loss: 0.5690 (0.6132)  acc1: 81.2500 (80.4688)  acc5: 97.6562 (97.8299)  time: 15.0471  data: 0.0149\n",
      "Epoch: [8]  [ 9/40]  eta: 0:07:41  lr: 1.0000000000000006e-12  img/s: 9.610083039446776  loss: 0.5690 (0.6290)  acc1: 78.9062 (79.7656)  acc5: 97.6562 (97.8906)  time: 14.8758  data: 0.0149\n",
      "Epoch: [8]  [10/40]  eta: 0:07:22  lr: 1.0000000000000006e-12  img/s: 9.601373201310428  loss: 0.6223 (0.6284)  acc1: 78.9062 (79.3324)  acc5: 97.6562 (97.7273)  time: 14.7368  data: 0.0149\n",
      "Epoch: [8]  [11/40]  eta: 0:07:10  lr: 1.0000000000000006e-12  img/s: 8.016962296657862  loss: 0.5690 (0.6209)  acc1: 78.9062 (79.5573)  acc5: 97.6562 (97.6562)  time: 14.8403  data: 0.0148\n",
      "Epoch: [8]  [12/40]  eta: 0:06:52  lr: 1.0000000000000006e-12  img/s: 9.494211276727768  loss: 0.6223 (0.6245)  acc1: 78.9062 (79.2067)  acc5: 97.6562 (97.5962)  time: 14.7370  data: 0.0148\n",
      "Epoch: [8]  [13/40]  eta: 0:06:40  lr: 1.0000000000000006e-12  img/s: 8.055795037586469  loss: 0.6223 (0.6328)  acc1: 78.9062 (78.8504)  acc5: 97.6562 (97.4330)  time: 14.8203  data: 0.0147\n",
      "Epoch: [8]  [14/40]  eta: 0:06:22  lr: 1.0000000000000006e-12  img/s: 9.546703201099973  loss: 0.6410 (0.6333)  acc1: 78.9062 (78.5938)  acc5: 97.6562 (97.5000)  time: 14.7272  data: 0.0148\n",
      "Epoch: [8]  [15/40]  eta: 0:06:09  lr: 1.0000000000000006e-12  img/s: 8.131450250556597  loss: 0.6223 (0.6274)  acc1: 78.9062 (78.6133)  acc5: 97.6562 (97.5098)  time: 14.7915  data: 0.0148\n",
      "Epoch: [8]  [16/40]  eta: 0:05:52  lr: 1.0000000000000006e-12  img/s: 9.583190419617612  loss: 0.6410 (0.6283)  acc1: 78.9062 (78.5846)  acc5: 97.6562 (97.6562)  time: 14.7079  data: 0.0148\n",
      "Epoch: [8]  [17/40]  eta: 0:05:40  lr: 1.0000000000000006e-12  img/s: 7.932525348037537  loss: 0.6410 (0.6349)  acc1: 78.1250 (78.4288)  acc5: 97.6562 (97.6997)  time: 14.7881  data: 0.0148\n",
      "Epoch: [8]  [18/40]  eta: 0:05:24  lr: 1.0000000000000006e-12  img/s: 9.146534899907822  loss: 0.6422 (0.6381)  acc1: 78.9062 (78.4539)  acc5: 97.6562 (97.7385)  time: 14.7472  data: 0.0148\n",
      "Epoch: [8]  [19/40]  eta: 0:05:08  lr: 1.0000000000000006e-12  img/s: 9.175686016682981  loss: 0.6422 (0.6384)  acc1: 78.1250 (78.3984)  acc5: 97.6562 (97.7344)  time: 14.7081  data: 0.0149\n",
      "Epoch: [8]  [20/40]  eta: 0:04:55  lr: 1.0000000000000006e-12  img/s: 7.731303563940992  loss: 0.6410 (0.6359)  acc1: 78.1250 (78.5342)  acc5: 97.6562 (97.6935)  time: 14.7242  data: 0.0149\n",
      "Epoch: [8]  [21/40]  eta: 0:04:39  lr: 1.0000000000000006e-12  img/s: 9.704927150624165  loss: 0.6223 (0.6290)  acc1: 78.9062 (78.8707)  acc5: 97.6562 (97.6918)  time: 14.6927  data: 0.0149\n",
      "Epoch: [8]  [22/40]  eta: 0:04:26  lr: 1.0000000000000006e-12  img/s: 8.002205264332273  loss: 0.6223 (0.6331)  acc1: 78.1250 (78.8383)  acc5: 97.6562 (97.6902)  time: 14.6513  data: 0.0148\n",
      "Epoch: [8]  [23/40]  eta: 0:04:10  lr: 1.0000000000000006e-12  img/s: 9.611035620581799  loss: 0.6223 (0.6309)  acc1: 78.1250 (79.0039)  acc5: 97.6562 (97.6562)  time: 14.6626  data: 0.0148\n",
      "Epoch: [8]  [24/40]  eta: 0:03:56  lr: 1.0000000000000006e-12  img/s: 8.146313224065764  loss: 0.6410 (0.6332)  acc1: 78.1250 (78.8750)  acc5: 97.6562 (97.6875)  time: 14.6386  data: 0.0148\n",
      "Epoch: [8]  [25/40]  eta: 0:03:40  lr: 1.0000000000000006e-12  img/s: 9.512905613892961  loss: 0.6410 (0.6357)  acc1: 78.1250 (78.8462)  acc5: 97.6562 (97.6863)  time: 14.6545  data: 0.0150\n",
      "Epoch: [8]  [26/40]  eta: 0:03:26  lr: 1.0000000000000006e-12  img/s: 7.881086935491519  loss: 0.6410 (0.6333)  acc1: 78.1250 (78.8773)  acc5: 97.6562 (97.7141)  time: 14.6325  data: 0.0149\n",
      "Epoch: [8]  [27/40]  eta: 0:03:11  lr: 1.0000000000000006e-12  img/s: 9.555538413548021  loss: 0.6410 (0.6296)  acc1: 78.1250 (79.1016)  acc5: 97.6562 (97.6842)  time: 14.6380  data: 0.0151\n",
      "Epoch: [8]  [28/40]  eta: 0:02:57  lr: 1.0000000000000006e-12  img/s: 7.935809788982938  loss: 0.6422 (0.6324)  acc1: 78.1250 (78.9871)  acc5: 97.6562 (97.6562)  time: 14.6435  data: 0.0151\n",
      "Epoch: [8]  [29/40]  eta: 0:02:41  lr: 1.0000000000000006e-12  img/s: 9.545911163736289  loss: 0.6422 (0.6387)  acc1: 78.1250 (78.7760)  acc5: 97.6562 (97.5781)  time: 14.6480  data: 0.0152\n",
      "Epoch: [8]  [30/40]  eta: 0:02:27  lr: 1.0000000000000006e-12  img/s: 8.41096869168824  loss: 0.6422 (0.6334)  acc1: 78.1250 (79.0323)  acc5: 97.6562 (97.5554)  time: 14.7423  data: 0.0152\n",
      "Epoch: [8]  [31/40]  eta: 0:02:13  lr: 1.0000000000000006e-12  img/s: 7.854842258350439  loss: 0.6427 (0.6367)  acc1: 78.1250 (79.0039)  acc5: 97.6562 (97.5342)  time: 14.7589  data: 0.0152\n",
      "Epoch: [8]  [32/40]  eta: 0:01:57  lr: 1.0000000000000006e-12  img/s: 9.648635177918413  loss: 0.6427 (0.6412)  acc1: 78.1250 (78.8116)  acc5: 97.6562 (97.4669)  time: 14.7482  data: 0.0154\n",
      "Epoch: [8]  [33/40]  eta: 0:01:43  lr: 1.0000000000000006e-12  img/s: 7.796357688214766  loss: 0.6427 (0.6453)  acc1: 78.1250 (78.8143)  acc5: 97.6562 (97.4265)  time: 14.7747  data: 0.0154\n",
      "Epoch: [8]  [34/40]  eta: 0:01:28  lr: 1.0000000000000006e-12  img/s: 8.899287642135034  loss: 0.6779 (0.6462)  acc1: 78.1250 (78.8839)  acc5: 97.6562 (97.4330)  time: 14.8233  data: 0.0153\n",
      "Epoch: [8]  [35/40]  eta: 0:01:14  lr: 1.0000000000000006e-12  img/s: 7.927145219215455  loss: 0.6779 (0.6467)  acc1: 78.1250 (78.8845)  acc5: 96.8750 (97.3524)  time: 14.8437  data: 0.0153\n",
      "Epoch: [8]  [36/40]  eta: 0:00:59  lr: 1.0000000000000006e-12  img/s: 9.30184706678454  loss: 0.6779 (0.6448)  acc1: 78.1250 (78.8640)  acc5: 96.8750 (97.3818)  time: 14.8639  data: 0.0153\n",
      "Epoch: [8]  [37/40]  eta: 0:00:44  lr: 1.0000000000000006e-12  img/s: 7.754382283045319  loss: 0.6625 (0.6442)  acc1: 78.9062 (78.9885)  acc5: 96.8750 (97.4095)  time: 14.8824  data: 0.0153\n",
      "Epoch: [8]  [38/40]  eta: 0:00:29  lr: 1.0000000000000006e-12  img/s: 9.448195354988055  loss: 0.6427 (0.6432)  acc1: 78.9062 (79.0264)  acc5: 96.8750 (97.3958)  time: 14.8600  data: 0.0153\n",
      "Epoch: [8]  [39/40]  eta: 0:00:14  lr: 1.0000000000000006e-12  img/s: 6.750378816226451  loss: 0.6625 (0.6603)  acc1: 78.9062 (78.9800)  acc5: 96.8750 (97.4000)  time: 14.2211  data: 0.0146\n",
      "Epoch: [8] Total time: 0:09:38\n",
      "Epoch 10/15\n",
      "Epoch: [9]  [ 0/40]  eta: 0:10:53  lr: 1.0000000000000007e-13  img/s: 7.842199674331692  loss: 0.7079 (0.7079)  acc1: 75.0000 (75.0000)  acc5: 92.9688 (92.9688)  time: 16.3341  data: 0.0121\n",
      "Epoch: [9]  [ 1/40]  eta: 0:09:47  lr: 1.0000000000000007e-13  img/s: 9.302371524585876  loss: 0.6583 (0.6831)  acc1: 75.0000 (77.7344)  acc5: 92.9688 (95.7031)  time: 15.0556  data: 0.0147\n",
      "Epoch: [9]  [ 2/40]  eta: 0:09:47  lr: 1.0000000000000007e-13  img/s: 7.87295422430489  loss: 0.6583 (0.6462)  acc1: 80.4688 (79.1667)  acc5: 96.8750 (96.0938)  time: 15.4615  data: 0.0148\n",
      "Epoch: [9]  [ 3/40]  eta: 0:09:21  lr: 1.0000000000000007e-13  img/s: 8.943362393700614  loss: 0.6458 (0.6461)  acc1: 80.4688 (80.2734)  acc5: 96.8750 (96.6797)  time: 15.1777  data: 0.0146\n",
      "Epoch: [9]  [ 4/40]  eta: 0:09:15  lr: 1.0000000000000007e-13  img/s: 7.804361090560177  loss: 0.6458 (0.6116)  acc1: 82.0312 (81.5625)  acc5: 98.4375 (97.1875)  time: 15.4253  data: 0.0146\n",
      "Epoch: [9]  [ 5/40]  eta: 0:08:51  lr: 1.0000000000000007e-13  img/s: 9.172490311939828  loss: 0.6458 (0.6220)  acc1: 80.4688 (81.3802)  acc5: 96.8750 (96.6146)  time: 15.1827  data: 0.0146\n",
      "Epoch: [9]  [ 6/40]  eta: 0:08:28  lr: 1.0000000000000007e-13  img/s: 9.48869011106328  loss: 0.6458 (0.6215)  acc1: 80.4688 (81.2500)  acc5: 97.6562 (96.7634)  time: 14.9430  data: 0.0147\n",
      "Epoch: [9]  [ 7/40]  eta: 0:08:18  lr: 1.0000000000000007e-13  img/s: 7.93162120011325  loss: 0.6448 (0.6244)  acc1: 80.4688 (81.3477)  acc5: 97.6562 (97.1680)  time: 15.0941  data: 0.0146\n",
      "Epoch: [9]  [ 8/40]  eta: 0:07:58  lr: 1.0000000000000007e-13  img/s: 9.185330507859957  loss: 0.6448 (0.6151)  acc1: 82.0312 (81.5972)  acc5: 97.6562 (97.2222)  time: 14.9670  data: 0.0147\n",
      "Epoch: [9]  [ 9/40]  eta: 0:07:48  lr: 1.0000000000000007e-13  img/s: 7.755575844641549  loss: 0.6448 (0.6321)  acc1: 80.4688 (80.8594)  acc5: 97.6562 (97.0312)  time: 15.1222  data: 0.0146\n",
      "Epoch: [9]  [10/40]  eta: 0:07:29  lr: 1.0000000000000007e-13  img/s: 9.374387868546115  loss: 0.6448 (0.6322)  acc1: 80.4688 (80.6818)  acc5: 97.6562 (97.0170)  time: 14.9900  data: 0.0145\n",
      "Epoch: [9]  [11/40]  eta: 0:07:18  lr: 1.0000000000000007e-13  img/s: 7.661389287918408  loss: 0.6448 (0.6359)  acc1: 80.4688 (80.2083)  acc5: 96.8750 (96.8750)  time: 15.1342  data: 0.0145\n",
      "Epoch: [9]  [12/40]  eta: 0:07:00  lr: 1.0000000000000007e-13  img/s: 9.44507955152987  loss: 0.6458 (0.6412)  acc1: 80.4688 (79.9279)  acc5: 97.6562 (96.9952)  time: 15.0137  data: 0.0146\n",
      "Epoch: [9]  [13/40]  eta: 0:06:48  lr: 1.0000000000000007e-13  img/s: 7.70337210749537  loss: 0.6458 (0.6571)  acc1: 80.4688 (79.1853)  acc5: 96.8750 (96.8750)  time: 15.1294  data: 0.0148\n",
      "Epoch: [9]  [14/40]  eta: 0:06:30  lr: 1.0000000000000007e-13  img/s: 9.346353200606535  loss: 0.6583 (0.6595)  acc1: 80.4688 (79.1667)  acc5: 96.8750 (96.8229)  time: 15.0348  data: 0.0148\n",
      "Epoch: [9]  [15/40]  eta: 0:06:17  lr: 1.0000000000000007e-13  img/s: 7.81374697253659  loss: 0.6458 (0.6561)  acc1: 80.4688 (79.3457)  acc5: 96.8750 (97.0215)  time: 15.1199  data: 0.0148\n",
      "Epoch: [9]  [16/40]  eta: 0:06:00  lr: 1.0000000000000007e-13  img/s: 9.333360175368894  loss: 0.6583 (0.6623)  acc1: 80.4688 (79.1820)  acc5: 97.6562 (97.0588)  time: 15.0382  data: 0.0149\n",
      "Epoch: [9]  [17/40]  eta: 0:05:48  lr: 1.0000000000000007e-13  img/s: 7.601061646505772  loss: 0.6458 (0.6580)  acc1: 78.9062 (79.1667)  acc5: 97.6562 (97.0920)  time: 15.1391  data: 0.0149\n",
      "Epoch: [9]  [18/40]  eta: 0:05:31  lr: 1.0000000000000007e-13  img/s: 9.261586153521808  loss: 0.6583 (0.6604)  acc1: 78.9062 (79.1118)  acc5: 97.6562 (97.1628)  time: 15.0705  data: 0.0149\n",
      "Epoch: [9]  [19/40]  eta: 0:05:15  lr: 1.0000000000000007e-13  img/s: 8.90447631329068  loss: 0.6458 (0.6522)  acc1: 78.9062 (79.3359)  acc5: 97.6562 (97.2266)  time: 15.0364  data: 0.0149\n",
      "Epoch: [9]  [20/40]  eta: 0:05:01  lr: 1.0000000000000007e-13  img/s: 7.920467645969226  loss: 0.6458 (0.6555)  acc1: 79.6875 (79.3527)  acc5: 97.6562 (97.3214)  time: 15.0286  data: 0.0151\n",
      "Epoch: [9]  [21/40]  eta: 0:04:45  lr: 1.0000000000000007e-13  img/s: 9.222736340183866  loss: 0.6448 (0.6497)  acc1: 79.6875 (79.5455)  acc5: 97.6562 (97.4077)  time: 15.0344  data: 0.0150\n",
      "Epoch: [9]  [22/40]  eta: 0:04:31  lr: 1.0000000000000007e-13  img/s: 7.7963556502999385  loss: 0.6458 (0.6512)  acc1: 78.9062 (79.4158)  acc5: 97.6562 (97.3845)  time: 15.0424  data: 0.0150\n",
      "Epoch: [9]  [23/40]  eta: 0:04:15  lr: 1.0000000000000007e-13  img/s: 9.293067883731588  loss: 0.6448 (0.6481)  acc1: 78.9062 (79.5247)  acc5: 97.6562 (97.3633)  time: 15.0155  data: 0.0150\n",
      "Epoch: [9]  [24/40]  eta: 0:04:01  lr: 1.0000000000000007e-13  img/s: 7.774193561466316  loss: 0.6742 (0.6524)  acc1: 78.9062 (79.3750)  acc5: 97.6562 (97.4375)  time: 15.0186  data: 0.0150\n",
      "Epoch: [9]  [25/40]  eta: 0:03:45  lr: 1.0000000000000007e-13  img/s: 9.420030216967385  loss: 0.6448 (0.6495)  acc1: 78.9062 (79.2969)  acc5: 97.6562 (97.4760)  time: 15.0003  data: 0.0150\n",
      "Epoch: [9]  [26/40]  eta: 0:03:31  lr: 1.0000000000000007e-13  img/s: 7.6708171574600605  loss: 0.6448 (0.6433)  acc1: 78.9062 (79.5428)  acc5: 97.6562 (97.5405)  time: 15.1602  data: 0.0150\n",
      "Epoch: [9]  [27/40]  eta: 0:03:15  lr: 1.0000000000000007e-13  img/s: 9.148801656764102  loss: 0.6775 (0.6448)  acc1: 78.9062 (79.6038)  acc5: 97.6562 (97.5167)  time: 15.0528  data: 0.0150\n",
      "Epoch: [9]  [28/40]  eta: 0:03:01  lr: 1.0000000000000007e-13  img/s: 7.900716417034502  loss: 0.6775 (0.6459)  acc1: 78.9062 (79.6067)  acc5: 97.6562 (97.5216)  time: 15.1661  data: 0.0149\n",
      "Epoch: [9]  [29/40]  eta: 0:02:45  lr: 1.0000000000000007e-13  img/s: 9.309504312004488  loss: 0.6766 (0.6460)  acc1: 78.9062 (79.6615)  acc5: 97.6562 (97.5260)  time: 15.0283  data: 0.0150\n",
      "Epoch: [9]  [30/40]  eta: 0:02:30  lr: 1.0000000000000007e-13  img/s: 7.936419464430741  loss: 0.6766 (0.6412)  acc1: 78.9062 (79.6875)  acc5: 97.6562 (97.5806)  time: 15.1520  data: 0.0150\n",
      "Epoch: [9]  [31/40]  eta: 0:02:15  lr: 1.0000000000000007e-13  img/s: 9.174799900683263  loss: 0.6766 (0.6427)  acc1: 78.9062 (79.5410)  acc5: 97.6562 (97.5830)  time: 15.0143  data: 0.0151\n",
      "Epoch: [9]  [32/40]  eta: 0:02:00  lr: 1.0000000000000007e-13  img/s: 9.354946329512792  loss: 0.6766 (0.6449)  acc1: 78.9062 (79.4034)  acc5: 97.6562 (97.4669)  time: 15.0209  data: 0.0151\n",
      "Epoch: [9]  [33/40]  eta: 0:01:45  lr: 1.0000000000000007e-13  img/s: 7.97535299682512  loss: 0.6495 (0.6446)  acc1: 79.6875 (79.5496)  acc5: 97.6562 (97.4494)  time: 14.9924  data: 0.0149\n",
      "Epoch: [9]  [34/40]  eta: 0:01:30  lr: 1.0000000000000007e-13  img/s: 9.348954682696544  loss: 0.6495 (0.6462)  acc1: 79.6875 (79.4643)  acc5: 97.6562 (97.4554)  time: 14.9922  data: 0.0149\n",
      "Epoch: [9]  [35/40]  eta: 0:01:15  lr: 1.0000000000000007e-13  img/s: 7.707162378855764  loss: 0.6495 (0.6455)  acc1: 79.6875 (79.4922)  acc5: 97.6562 (97.4826)  time: 15.0035  data: 0.0149\n",
      "Epoch: [9]  [36/40]  eta: 0:01:00  lr: 1.0000000000000007e-13  img/s: 8.637731235937586  loss: 0.6352 (0.6399)  acc1: 79.6875 (79.6664)  acc5: 97.6562 (97.5084)  time: 15.0585  data: 0.0147\n",
      "Epoch: [9]  [37/40]  eta: 0:00:45  lr: 1.0000000000000007e-13  img/s: 7.7425407596047355  loss: 0.6352 (0.6392)  acc1: 80.4688 (79.6875)  acc5: 97.6562 (97.5123)  time: 15.0432  data: 0.0148\n",
      "Epoch: [9]  [38/40]  eta: 0:00:30  lr: 1.0000000000000007e-13  img/s: 9.474744775124893  loss: 0.6212 (0.6387)  acc1: 80.4688 (79.6274)  acc5: 97.6562 (97.5160)  time: 15.0276  data: 0.0147\n",
      "Epoch: [9]  [39/40]  eta: 0:00:14  lr: 1.0000000000000007e-13  img/s: 6.213646773339624  loss: 0.6352 (0.6483)  acc1: 79.6875 (79.6000)  acc5: 97.6562 (97.5200)  time: 14.3726  data: 0.0141\n",
      "Epoch: [9] Total time: 0:09:48\n",
      "Epoch 11/15\n",
      "Epoch: [10]  [ 0/40]  eta: 0:11:09  lr: 1.0000000000000008e-14  img/s: 7.650022366377725  loss: 0.8302 (0.8302)  acc1: 77.3438 (77.3438)  acc5: 93.7500 (93.7500)  time: 16.7467  data: 0.0147\n",
      "Epoch: [10]  [ 1/40]  eta: 0:10:24  lr: 1.0000000000000008e-14  img/s: 8.392081666178418  loss: 0.7607 (0.7954)  acc1: 74.2188 (75.7812)  acc5: 93.7500 (95.3125)  time: 16.0069  data: 0.0147\n",
      "Epoch: [10]  [ 2/40]  eta: 0:10:10  lr: 1.0000000000000008e-14  img/s: 7.924002207210261  loss: 0.7607 (0.7747)  acc1: 76.5625 (76.0417)  acc5: 96.8750 (96.0938)  time: 16.0604  data: 0.0145\n",
      "Epoch: [10]  [ 3/40]  eta: 0:09:33  lr: 1.0000000000000008e-14  img/s: 9.27353348369167  loss: 0.7333 (0.7216)  acc1: 76.5625 (77.3438)  acc5: 96.8750 (96.8750)  time: 15.4995  data: 0.0143\n",
      "Epoch: [10]  [ 4/40]  eta: 0:09:28  lr: 1.0000000000000008e-14  img/s: 7.547486715935629  loss: 0.7333 (0.7101)  acc1: 77.3438 (77.5000)  acc5: 97.6562 (97.1875)  time: 15.7942  data: 0.0142\n",
      "Epoch: [10]  [ 5/40]  eta: 0:09:00  lr: 1.0000000000000008e-14  img/s: 9.339081165883158  loss: 0.6826 (0.7056)  acc1: 77.3438 (77.6042)  acc5: 96.8750 (96.8750)  time: 15.4485  data: 0.0142\n",
      "Epoch: [10]  [ 6/40]  eta: 0:08:49  lr: 1.0000000000000008e-14  img/s: 7.897379982777219  loss: 0.6826 (0.6866)  acc1: 78.1250 (77.7902)  acc5: 97.6562 (96.9866)  time: 15.5591  data: 0.0143\n",
      "Epoch: [10]  [ 7/40]  eta: 0:08:25  lr: 1.0000000000000008e-14  img/s: 9.350465717261116  loss: 0.6702 (0.6846)  acc1: 78.1250 (78.3203)  acc5: 97.6562 (97.2656)  time: 15.3272  data: 0.0143\n",
      "Epoch: [10]  [ 8/40]  eta: 0:08:04  lr: 1.0000000000000008e-14  img/s: 9.371837817468569  loss: 0.6702 (0.6725)  acc1: 78.1250 (79.1667)  acc5: 97.6562 (97.3090)  time: 15.1435  data: 0.0145\n",
      "Epoch: [10]  [ 9/40]  eta: 0:07:54  lr: 1.0000000000000008e-14  img/s: 7.705116602354615  loss: 0.6642 (0.6660)  acc1: 78.1250 (78.9844)  acc5: 97.6562 (97.1094)  time: 15.2924  data: 0.0151\n",
      "Epoch: [10]  [10/40]  eta: 0:07:33  lr: 1.0000000000000008e-14  img/s: 9.49047196204652  loss: 0.6702 (0.6869)  acc1: 78.1250 (78.3381)  acc5: 97.6562 (97.3011)  time: 15.1296  data: 0.0150\n",
      "Epoch: [10]  [11/40]  eta: 0:07:21  lr: 1.0000000000000008e-14  img/s: 7.841726485041137  loss: 0.6642 (0.6779)  acc1: 78.1250 (78.3854)  acc5: 97.6562 (97.3958)  time: 15.2302  data: 0.0150\n",
      "Epoch: [10]  [12/40]  eta: 0:07:04  lr: 1.0000000000000008e-14  img/s: 8.881630060148222  loss: 0.6702 (0.6777)  acc1: 78.1250 (78.1250)  acc5: 97.6562 (97.4760)  time: 15.1685  data: 0.0151\n",
      "Epoch: [10]  [13/40]  eta: 0:06:52  lr: 1.0000000000000008e-14  img/s: 7.683795087314339  loss: 0.6702 (0.6879)  acc1: 77.3438 (77.9576)  acc5: 97.6562 (97.2656)  time: 15.2760  data: 0.0151\n",
      "Epoch: [10]  [14/40]  eta: 0:06:34  lr: 1.0000000000000008e-14  img/s: 9.46038491033196  loss: 0.6702 (0.6853)  acc1: 77.3438 (77.8125)  acc5: 97.6562 (97.2396)  time: 15.1605  data: 0.0150\n",
      "Epoch: [10]  [15/40]  eta: 0:06:20  lr: 1.0000000000000008e-14  img/s: 7.8267484735701505  loss: 0.6642 (0.6745)  acc1: 77.3438 (78.0273)  acc5: 97.6562 (97.3145)  time: 15.2360  data: 0.0149\n",
      "Epoch: [10]  [16/40]  eta: 0:06:03  lr: 1.0000000000000008e-14  img/s: 9.494636247922505  loss: 0.6702 (0.6770)  acc1: 78.1250 (78.1710)  acc5: 97.6562 (97.3346)  time: 15.1337  data: 0.0149\n",
      "Epoch: [10]  [17/40]  eta: 0:05:49  lr: 1.0000000000000008e-14  img/s: 7.893821190954705  loss: 0.6642 (0.6721)  acc1: 78.1250 (78.2552)  acc5: 97.6562 (97.3090)  time: 15.1946  data: 0.0149\n",
      "Epoch: [10]  [18/40]  eta: 0:05:32  lr: 1.0000000000000008e-14  img/s: 9.298056258037414  loss: 0.6642 (0.6695)  acc1: 78.1250 (78.2072)  acc5: 97.6562 (97.3684)  time: 15.1202  data: 0.0149\n",
      "Epoch: [10]  [19/40]  eta: 0:05:19  lr: 1.0000000000000008e-14  img/s: 7.736784732497884  loss: 0.6487 (0.6683)  acc1: 78.1250 (78.2031)  acc5: 97.6562 (97.2656)  time: 15.1921  data: 0.0149\n",
      "Epoch: [10]  [20/40]  eta: 0:05:02  lr: 1.0000000000000008e-14  img/s: 8.959867897455506  loss: 0.6487 (0.6686)  acc1: 78.1250 (78.1622)  acc5: 97.6562 (97.2842)  time: 15.0698  data: 0.0149\n",
      "Epoch: [10]  [21/40]  eta: 0:04:48  lr: 1.0000000000000008e-14  img/s: 7.844972719086861  loss: 0.6461 (0.6597)  acc1: 78.1250 (78.4091)  acc5: 97.6562 (97.3366)  time: 15.1230  data: 0.0149\n",
      "Epoch: [10]  [22/40]  eta: 0:04:32  lr: 1.0000000000000008e-14  img/s: 9.474849952089324  loss: 0.6217 (0.6574)  acc1: 78.1250 (78.5666)  acc5: 97.6562 (97.3505)  time: 14.9909  data: 0.0150\n",
      "Epoch: [10]  [23/40]  eta: 0:04:16  lr: 1.0000000000000008e-14  img/s: 9.513968676879223  loss: 0.6217 (0.6536)  acc1: 78.1250 (78.7760)  acc5: 97.6562 (97.2982)  time: 14.9735  data: 0.0150\n",
      "Epoch: [10]  [24/40]  eta: 0:04:01  lr: 1.0000000000000008e-14  img/s: 7.960133066712006  loss: 0.6217 (0.6527)  acc1: 78.1250 (78.7188)  acc5: 97.6562 (97.3750)  time: 14.9295  data: 0.0150\n",
      "Epoch: [10]  [25/40]  eta: 0:03:45  lr: 1.0000000000000008e-14  img/s: 9.127803948834943  loss: 0.6143 (0.6512)  acc1: 78.9062 (78.7560)  acc5: 97.6562 (97.3558)  time: 14.9456  data: 0.0152\n",
      "Epoch: [10]  [26/40]  eta: 0:03:31  lr: 1.0000000000000008e-14  img/s: 7.832234798875067  loss: 0.6143 (0.6478)  acc1: 78.9062 (78.8194)  acc5: 97.6562 (97.3669)  time: 14.9523  data: 0.0152\n",
      "Epoch: [10]  [27/40]  eta: 0:03:15  lr: 1.0000000000000008e-14  img/s: 9.261973617005802  loss: 0.6072 (0.6427)  acc1: 78.9062 (79.0458)  acc5: 97.6562 (97.4609)  time: 14.9590  data: 0.0153\n",
      "Epoch: [10]  [28/40]  eta: 0:03:01  lr: 1.0000000000000008e-14  img/s: 7.6666502110599355  loss: 0.6143 (0.6443)  acc1: 78.1250 (78.9871)  acc5: 97.6562 (97.4677)  time: 15.1108  data: 0.0152\n",
      "Epoch: [10]  [29/40]  eta: 0:02:46  lr: 1.0000000000000008e-14  img/s: 8.790532288446073  loss: 0.6143 (0.6406)  acc1: 78.9062 (79.0104)  acc5: 97.6562 (97.4740)  time: 15.0079  data: 0.0149\n",
      "Epoch: [10]  [30/40]  eta: 0:02:31  lr: 1.0000000000000008e-14  img/s: 7.818560196680137  loss: 0.6068 (0.6361)  acc1: 79.6875 (79.2087)  acc5: 97.6562 (97.5050)  time: 15.1521  data: 0.0149\n",
      "Epoch: [10]  [31/40]  eta: 0:02:15  lr: 1.0000000000000008e-14  img/s: 9.4342969208786  loss: 0.6143 (0.6380)  acc1: 79.6875 (79.1992)  acc5: 97.6562 (97.4854)  time: 15.0144  data: 0.0150\n",
      "Epoch: [10]  [32/40]  eta: 0:02:01  lr: 1.0000000000000008e-14  img/s: 7.914651113699216  loss: 0.6143 (0.6376)  acc1: 79.6875 (79.1903)  acc5: 97.6562 (97.4432)  time: 15.1024  data: 0.0149\n",
      "Epoch: [10]  [33/40]  eta: 0:01:45  lr: 1.0000000000000008e-14  img/s: 9.35711338971423  loss: 0.6143 (0.6378)  acc1: 79.6875 (79.1360)  acc5: 97.6562 (97.4494)  time: 14.9534  data: 0.0149\n",
      "Epoch: [10]  [34/40]  eta: 0:01:30  lr: 1.0000000000000008e-14  img/s: 9.500826062745942  loss: 0.6143 (0.6415)  acc1: 79.6875 (79.1071)  acc5: 97.6562 (97.3884)  time: 14.9506  data: 0.0149\n",
      "Epoch: [10]  [35/40]  eta: 0:01:15  lr: 1.0000000000000008e-14  img/s: 7.544014126464593  loss: 0.6143 (0.6403)  acc1: 79.6875 (79.2318)  acc5: 97.6562 (97.3307)  time: 14.9812  data: 0.0149\n",
      "Epoch: [10]  [36/40]  eta: 0:01:00  lr: 1.0000000000000008e-14  img/s: 9.15327221807839  loss: 0.6068 (0.6386)  acc1: 79.6875 (79.3074)  acc5: 97.6562 (97.3606)  time: 15.0064  data: 0.0149\n",
      "Epoch: [10]  [37/40]  eta: 0:00:45  lr: 1.0000000000000008e-14  img/s: 7.940843324796033  loss: 0.6068 (0.6377)  acc1: 78.9062 (79.2969)  acc5: 97.6562 (97.3890)  time: 15.0016  data: 0.0150\n",
      "Epoch: [10]  [38/40]  eta: 0:00:30  lr: 1.0000000000000008e-14  img/s: 9.52540613491747  loss: 0.6042 (0.6365)  acc1: 78.9062 (79.2869)  acc5: 97.6562 (97.4159)  time: 14.9851  data: 0.0149\n",
      "Epoch: [10]  [39/40]  eta: 0:00:14  lr: 1.0000000000000008e-14  img/s: 6.398206069388961  loss: 0.6042 (0.6416)  acc1: 78.9062 (79.2800)  acc5: 97.6562 (97.4200)  time: 14.2198  data: 0.0143\n",
      "Epoch: [10] Total time: 0:09:48\n",
      "Epoch 12/15\n",
      "Epoch: [11]  [ 0/40]  eta: 0:11:25  lr: 1.0000000000000009e-15  img/s: 7.477145900808343  loss: 0.7068 (0.7068)  acc1: 78.1250 (78.1250)  acc5: 95.3125 (95.3125)  time: 17.1356  data: 0.0168\n",
      "Epoch: [11]  [ 1/40]  eta: 0:09:58  lr: 1.0000000000000009e-15  img/s: 9.448845702619932  loss: 0.6975 (0.7022)  acc1: 77.3438 (77.7344)  acc5: 95.3125 (96.0938)  time: 15.3482  data: 0.0154\n",
      "Epoch: [11]  [ 2/40]  eta: 0:09:59  lr: 1.0000000000000009e-15  img/s: 7.714834939786755  loss: 0.6975 (0.6559)  acc1: 78.1250 (78.6458)  acc5: 96.8750 (96.8750)  time: 15.7673  data: 0.0150\n",
      "Epoch: [11]  [ 3/40]  eta: 0:09:21  lr: 1.0000000000000009e-15  img/s: 9.553875879690665  loss: 0.5963 (0.6410)  acc1: 78.1250 (79.1016)  acc5: 96.8750 (97.2656)  time: 15.1787  data: 0.0151\n",
      "Epoch: [11]  [ 4/40]  eta: 0:09:16  lr: 1.0000000000000009e-15  img/s: 7.75815465393403  loss: 0.5963 (0.6280)  acc1: 78.9062 (79.0625)  acc5: 96.8750 (97.1875)  time: 15.4456  data: 0.0149\n",
      "Epoch: [11]  [ 5/40]  eta: 0:08:53  lr: 1.0000000000000009e-15  img/s: 9.055761057183204  loss: 0.5963 (0.6322)  acc1: 78.9062 (79.2969)  acc5: 96.8750 (97.0052)  time: 15.2297  data: 0.0150\n",
      "Epoch: [11]  [ 6/40]  eta: 0:08:41  lr: 1.0000000000000009e-15  img/s: 8.02515200071272  loss: 0.6531 (0.6354)  acc1: 79.6875 (79.3527)  acc5: 96.8750 (97.3214)  time: 15.3345  data: 0.0148\n",
      "Epoch: [11]  [ 7/40]  eta: 0:08:21  lr: 1.0000000000000009e-15  img/s: 8.955956061824233  loss: 0.6218 (0.6337)  acc1: 79.6875 (79.3945)  acc5: 96.8750 (97.1680)  time: 15.2061  data: 0.0148\n",
      "Epoch: [11]  [ 8/40]  eta: 0:08:10  lr: 1.0000000000000009e-15  img/s: 7.79087208598849  loss: 0.6218 (0.6271)  acc1: 79.6875 (79.6875)  acc5: 96.8750 (97.1354)  time: 15.3437  data: 0.0149\n",
      "Epoch: [11]  [ 9/40]  eta: 0:07:51  lr: 1.0000000000000009e-15  img/s: 9.194796555859455  loss: 0.6218 (0.6403)  acc1: 79.6875 (79.2969)  acc5: 96.8750 (96.7969)  time: 15.2029  data: 0.0149\n",
      "Epoch: [11]  [10/40]  eta: 0:07:31  lr: 1.0000000000000009e-15  img/s: 9.421140901085401  loss: 0.6218 (0.6353)  acc1: 79.6875 (79.5455)  acc5: 96.8750 (97.0170)  time: 15.0572  data: 0.0148\n",
      "Epoch: [11]  [11/40]  eta: 0:07:18  lr: 1.0000000000000009e-15  img/s: 8.017139598999448  loss: 0.5963 (0.6281)  acc1: 79.6875 (79.6875)  acc5: 96.8750 (96.9401)  time: 15.1342  data: 0.0148\n",
      "Epoch: [11]  [12/40]  eta: 0:07:00  lr: 1.0000000000000009e-15  img/s: 9.511966991597639  loss: 0.6085 (0.6266)  acc1: 80.4688 (79.9279)  acc5: 96.8750 (96.8750)  time: 15.0062  data: 0.0147\n",
      "Epoch: [11]  [13/40]  eta: 0:06:49  lr: 1.0000000000000009e-15  img/s: 7.458844502230196  loss: 0.6085 (0.6332)  acc1: 79.6875 (79.6317)  acc5: 96.0938 (96.7634)  time: 15.1612  data: 0.0147\n",
      "Epoch: [11]  [14/40]  eta: 0:06:31  lr: 1.0000000000000009e-15  img/s: 9.377406586171123  loss: 0.6218 (0.6353)  acc1: 79.6875 (79.5833)  acc5: 96.8750 (96.8750)  time: 15.0615  data: 0.0148\n",
      "Epoch: [11]  [15/40]  eta: 0:06:18  lr: 1.0000000000000009e-15  img/s: 7.886621926575641  loss: 0.6114 (0.6338)  acc1: 79.6875 (79.5898)  acc5: 96.8750 (97.0215)  time: 15.1356  data: 0.0150\n",
      "Epoch: [11]  [16/40]  eta: 0:06:01  lr: 1.0000000000000009e-15  img/s: 9.40341927830618  loss: 0.6200 (0.6330)  acc1: 79.6875 (79.5956)  acc5: 96.8750 (97.1048)  time: 15.0469  data: 0.0150\n",
      "Epoch: [11]  [17/40]  eta: 0:05:48  lr: 1.0000000000000009e-15  img/s: 7.418307312810587  loss: 0.6114 (0.6314)  acc1: 79.6875 (79.8611)  acc5: 96.8750 (97.0486)  time: 15.1704  data: 0.0150\n",
      "Epoch: [11]  [18/40]  eta: 0:05:31  lr: 1.0000000000000009e-15  img/s: 9.58128176191204  loss: 0.6200 (0.6325)  acc1: 79.6875 (79.7697)  acc5: 96.8750 (97.1217)  time: 15.0758  data: 0.0149\n",
      "Epoch: [11]  [19/40]  eta: 0:05:18  lr: 1.0000000000000009e-15  img/s: 7.6919236715235675  loss: 0.6200 (0.6325)  acc1: 79.6875 (79.8438)  acc5: 96.8750 (97.1094)  time: 15.1548  data: 0.0149\n",
      "Epoch: [11]  [20/40]  eta: 0:05:02  lr: 1.0000000000000009e-15  img/s: 9.139774350255196  loss: 0.6180 (0.6318)  acc1: 79.6875 (79.9107)  acc5: 96.8750 (97.1354)  time: 14.9989  data: 0.0148\n",
      "Epoch: [11]  [21/40]  eta: 0:04:48  lr: 1.0000000000000009e-15  img/s: 7.710249856938321  loss: 0.6114 (0.6248)  acc1: 80.4688 (80.0781)  acc5: 96.8750 (97.2301)  time: 15.1517  data: 0.0148\n",
      "Epoch: [11]  [22/40]  eta: 0:04:33  lr: 1.0000000000000009e-15  img/s: 8.330647002812494  loss: 0.6180 (0.6255)  acc1: 79.6875 (79.8913)  acc5: 96.8750 (97.2486)  time: 15.0904  data: 0.0148\n",
      "Epoch: [11]  [23/40]  eta: 0:04:18  lr: 1.0000000000000009e-15  img/s: 7.755534615552965  loss: 0.6180 (0.6168)  acc1: 79.6875 (80.1432)  acc5: 96.8750 (97.2656)  time: 15.2457  data: 0.0148\n",
      "Epoch: [11]  [24/40]  eta: 0:04:02  lr: 1.0000000000000009e-15  img/s: 9.558681752718158  loss: 0.6200 (0.6174)  acc1: 80.4688 (80.1562)  acc5: 97.6562 (97.3438)  time: 15.0903  data: 0.0148\n",
      "Epoch: [11]  [25/40]  eta: 0:03:46  lr: 1.0000000000000009e-15  img/s: 9.162384233087026  loss: 0.6200 (0.6175)  acc1: 79.6875 (80.0781)  acc5: 97.6562 (97.2957)  time: 15.0820  data: 0.0148\n",
      "Epoch: [11]  [26/40]  eta: 0:03:32  lr: 1.0000000000000009e-15  img/s: 8.046952502391584  loss: 0.6180 (0.6142)  acc1: 80.4688 (80.2373)  acc5: 97.6562 (97.3958)  time: 15.0799  data: 0.0148\n",
      "Epoch: [11]  [27/40]  eta: 0:03:16  lr: 1.0000000000000009e-15  img/s: 9.57485555568337  loss: 0.6114 (0.6136)  acc1: 81.2500 (80.3292)  acc5: 97.6562 (97.4330)  time: 15.0336  data: 0.0147\n",
      "Epoch: [11]  [28/40]  eta: 0:03:01  lr: 1.0000000000000009e-15  img/s: 8.097893252641427  loss: 0.6180 (0.6168)  acc1: 80.4688 (80.2263)  acc5: 97.6562 (97.4138)  time: 15.0025  data: 0.0147\n",
      "Epoch: [11]  [29/40]  eta: 0:02:45  lr: 1.0000000000000009e-15  img/s: 9.144094362083619  loss: 0.6180 (0.6186)  acc1: 80.4688 (80.1562)  acc5: 97.6562 (97.2917)  time: 15.0063  data: 0.0146\n",
      "Epoch: [11]  [30/40]  eta: 0:02:31  lr: 1.0000000000000009e-15  img/s: 7.903853547158706  loss: 0.6180 (0.6150)  acc1: 80.4688 (80.3427)  acc5: 97.6562 (97.2530)  time: 15.1367  data: 0.0147\n",
      "Epoch: [11]  [31/40]  eta: 0:02:15  lr: 1.0000000000000009e-15  img/s: 9.727255410999714  loss: 0.6200 (0.6212)  acc1: 79.6875 (80.0537)  acc5: 97.6562 (97.1680)  time: 14.9964  data: 0.0147\n",
      "Epoch: [11]  [32/40]  eta: 0:02:00  lr: 1.0000000000000009e-15  img/s: 7.8318043978701  loss: 0.6204 (0.6245)  acc1: 79.6875 (80.0426)  acc5: 97.6562 (97.1354)  time: 15.1408  data: 0.0147\n",
      "Epoch: [11]  [33/40]  eta: 0:01:45  lr: 1.0000000000000009e-15  img/s: 9.73855014862297  loss: 0.6204 (0.6319)  acc1: 79.6875 (79.8254)  acc5: 97.6562 (97.1278)  time: 14.9399  data: 0.0147\n",
      "Epoch: [11]  [34/40]  eta: 0:01:30  lr: 1.0000000000000009e-15  img/s: 7.870060532825369  loss: 0.6204 (0.6399)  acc1: 79.6875 (79.7098)  acc5: 97.6562 (97.1429)  time: 15.0706  data: 0.0147\n",
      "Epoch: [11]  [35/40]  eta: 0:01:15  lr: 1.0000000000000009e-15  img/s: 9.64479457019486  loss: 0.6204 (0.6390)  acc1: 79.6875 (79.8394)  acc5: 97.6562 (97.1571)  time: 14.9225  data: 0.0145\n",
      "Epoch: [11]  [36/40]  eta: 0:00:59  lr: 1.0000000000000009e-15  img/s: 8.951724480838129  loss: 0.6204 (0.6375)  acc1: 80.4688 (79.9409)  acc5: 96.8750 (97.1495)  time: 14.9567  data: 0.0144\n",
      "Epoch: [11]  [37/40]  eta: 0:00:45  lr: 1.0000000000000009e-15  img/s: 7.947347546360719  loss: 0.6204 (0.6367)  acc1: 80.4688 (79.9959)  acc5: 96.8750 (97.1217)  time: 14.8993  data: 0.0144\n",
      "Epoch: [11]  [38/40]  eta: 0:00:29  lr: 1.0000000000000009e-15  img/s: 9.81062336650164  loss: 0.6180 (0.6343)  acc1: 81.2500 (80.1082)  acc5: 96.8750 (97.1755)  time: 14.8836  data: 0.0144\n",
      "Epoch: [11]  [39/40]  eta: 0:00:14  lr: 1.0000000000000009e-15  img/s: 5.104605023454502  loss: 0.6180 (0.6416)  acc1: 80.4688 (80.1000)  acc5: 97.6562 (97.1800)  time: 14.1294  data: 0.0138\n",
      "Epoch: [11] Total time: 0:09:45\n",
      "Epoch 13/15\n",
      "Epoch: [12]  [ 0/40]  eta: 0:10:39  lr: 1.000000000000001e-16  img/s: 8.01758223062955  loss: 0.7890 (0.7890)  acc1: 73.4375 (73.4375)  acc5: 96.0938 (96.0938)  time: 15.9784  data: 0.0135\n",
      "Epoch: [12]  [ 1/40]  eta: 0:09:49  lr: 1.000000000000001e-16  img/s: 9.006206197736242  loss: 0.7890 (0.8339)  acc1: 71.8750 (72.6562)  acc5: 96.0938 (96.0938)  time: 15.1029  data: 0.0142\n",
      "Epoch: [12]  [ 2/40]  eta: 0:09:45  lr: 1.000000000000001e-16  img/s: 7.984544748391638  loss: 0.7890 (0.7832)  acc1: 73.4375 (73.9583)  acc5: 96.0938 (96.3542)  time: 15.4173  data: 0.0145\n",
      "Epoch: [12]  [ 3/40]  eta: 0:09:09  lr: 1.000000000000001e-16  img/s: 9.735033393028091  loss: 0.6818 (0.7341)  acc1: 73.4375 (75.7812)  acc5: 96.0938 (96.8750)  time: 14.8540  data: 0.0148\n",
      "Epoch: [12]  [ 4/40]  eta: 0:09:04  lr: 1.000000000000001e-16  img/s: 7.902510851600778  loss: 0.6818 (0.6876)  acc1: 76.5625 (76.7188)  acc5: 96.8750 (97.0312)  time: 15.1256  data: 0.0147\n",
      "Epoch: [12]  [ 5/40]  eta: 0:08:44  lr: 1.000000000000001e-16  img/s: 8.997518584887992  loss: 0.6818 (0.6883)  acc1: 76.5625 (76.9531)  acc5: 96.0938 (96.7448)  time: 14.9781  data: 0.0148\n",
      "Epoch: [12]  [ 6/40]  eta: 0:08:36  lr: 1.000000000000001e-16  img/s: 7.79401196109577  loss: 0.6818 (0.6643)  acc1: 78.1250 (77.4554)  acc5: 96.8750 (96.9866)  time: 15.1866  data: 0.0148\n",
      "Epoch: [12]  [ 7/40]  eta: 0:08:16  lr: 1.000000000000001e-16  img/s: 9.085150751022566  loss: 0.5868 (0.6412)  acc1: 78.1250 (78.6133)  acc5: 96.8750 (97.0703)  time: 15.0512  data: 0.0147\n",
      "Epoch: [12]  [ 8/40]  eta: 0:08:04  lr: 1.000000000000001e-16  img/s: 8.068445768435234  loss: 0.6067 (0.6374)  acc1: 80.4688 (78.8194)  acc5: 96.8750 (96.9618)  time: 15.1432  data: 0.0147\n",
      "Epoch: [12]  [ 9/40]  eta: 0:07:42  lr: 1.000000000000001e-16  img/s: 9.995397325351874  loss: 0.6067 (0.6462)  acc1: 78.1250 (78.4375)  acc5: 96.0938 (96.8750)  time: 14.9109  data: 0.0147\n",
      "Epoch: [12]  [10/40]  eta: 0:07:23  lr: 1.000000000000001e-16  img/s: 9.406742348701199  loss: 0.6229 (0.6441)  acc1: 79.6875 (78.5511)  acc5: 96.8750 (97.0170)  time: 14.7937  data: 0.0147\n",
      "Epoch: [12]  [11/40]  eta: 0:07:14  lr: 1.000000000000001e-16  img/s: 7.438975061261941  loss: 0.6067 (0.6403)  acc1: 79.6875 (78.8411)  acc5: 96.8750 (97.0703)  time: 14.9960  data: 0.0147\n",
      "Epoch: [12]  [12/40]  eta: 0:06:57  lr: 1.000000000000001e-16  img/s: 9.320419567643125  loss: 0.6229 (0.6456)  acc1: 79.6875 (78.9062)  acc5: 96.8750 (97.0553)  time: 14.9001  data: 0.0147\n",
      "Epoch: [12]  [13/40]  eta: 0:06:45  lr: 1.000000000000001e-16  img/s: 7.804333408836973  loss: 0.6229 (0.6538)  acc1: 79.6875 (78.4040)  acc5: 96.8750 (96.9866)  time: 15.0083  data: 0.0147\n",
      "Epoch: [12]  [14/40]  eta: 0:06:27  lr: 1.000000000000001e-16  img/s: 9.370986527547542  loss: 0.6568 (0.6540)  acc1: 79.6875 (78.4896)  acc5: 96.8750 (97.0833)  time: 14.9194  data: 0.0147\n",
      "Epoch: [12]  [15/40]  eta: 0:06:14  lr: 1.000000000000001e-16  img/s: 7.990465335798858  loss: 0.6229 (0.6470)  acc1: 79.6875 (78.7109)  acc5: 96.8750 (97.2168)  time: 14.9890  data: 0.0147\n",
      "Epoch: [12]  [16/40]  eta: 0:05:57  lr: 1.000000000000001e-16  img/s: 9.497152097498386  loss: 0.6568 (0.6514)  acc1: 79.6875 (78.5846)  acc5: 97.6562 (97.2426)  time: 14.9010  data: 0.0147\n",
      "Epoch: [12]  [17/40]  eta: 0:05:44  lr: 1.000000000000001e-16  img/s: 8.027480260957931  loss: 0.6229 (0.6469)  acc1: 79.6875 (78.9062)  acc5: 97.6562 (97.3524)  time: 14.9598  data: 0.0147\n",
      "Epoch: [12]  [18/40]  eta: 0:05:27  lr: 1.000000000000001e-16  img/s: 9.596488292204073  loss: 0.6535 (0.6473)  acc1: 79.6875 (78.9885)  acc5: 97.6562 (97.3273)  time: 14.8752  data: 0.0147\n",
      "Epoch: [12]  [19/40]  eta: 0:05:13  lr: 1.000000000000001e-16  img/s: 8.034262501463763  loss: 0.6229 (0.6392)  acc1: 79.6875 (79.2578)  acc5: 97.6562 (97.3438)  time: 14.9288  data: 0.0147\n",
      "Epoch: [12]  [20/40]  eta: 0:04:56  lr: 1.000000000000001e-16  img/s: 9.693893133961152  loss: 0.6229 (0.6410)  acc1: 79.6875 (79.2039)  acc5: 97.6562 (97.3214)  time: 14.7909  data: 0.0148\n",
      "Epoch: [12]  [21/40]  eta: 0:04:40  lr: 1.000000000000001e-16  img/s: 9.613646767494437  loss: 0.6067 (0.6369)  acc1: 79.6875 (79.2259)  acc5: 97.6562 (97.4077)  time: 14.7459  data: 0.0148\n",
      "Epoch: [12]  [22/40]  eta: 0:04:27  lr: 1.000000000000001e-16  img/s: 7.496564288825639  loss: 0.6067 (0.6412)  acc1: 79.6875 (79.2459)  acc5: 97.6562 (97.4185)  time: 14.7981  data: 0.0148\n",
      "Epoch: [12]  [23/40]  eta: 0:04:11  lr: 1.000000000000001e-16  img/s: 9.488154831041163  loss: 0.6229 (0.6431)  acc1: 79.6875 (79.2643)  acc5: 97.6562 (97.3307)  time: 14.8152  data: 0.0148\n",
      "Epoch: [12]  [24/40]  eta: 0:03:57  lr: 1.000000000000001e-16  img/s: 7.982271943397348  loss: 0.6229 (0.6420)  acc1: 79.6875 (79.1875)  acc5: 97.6562 (97.3125)  time: 14.8072  data: 0.0148\n",
      "Epoch: [12]  [25/40]  eta: 0:03:42  lr: 1.000000000000001e-16  img/s: 9.422720179735357  loss: 0.6229 (0.6423)  acc1: 79.6875 (79.1466)  acc5: 97.6562 (97.3558)  time: 14.7751  data: 0.0148\n",
      "Epoch: [12]  [26/40]  eta: 0:03:28  lr: 1.000000000000001e-16  img/s: 7.997486904206337  loss: 0.6229 (0.6383)  acc1: 79.6875 (79.3113)  acc5: 97.6562 (97.3380)  time: 14.7542  data: 0.0148\n",
      "Epoch: [12]  [27/40]  eta: 0:03:13  lr: 1.000000000000001e-16  img/s: 8.907962389404576  loss: 0.6327 (0.6381)  acc1: 79.6875 (79.2411)  acc5: 97.6562 (97.3493)  time: 14.7682  data: 0.0149\n",
      "Epoch: [12]  [28/40]  eta: 0:02:59  lr: 1.000000000000001e-16  img/s: 7.616528373339464  loss: 0.6327 (0.6366)  acc1: 79.6875 (79.2026)  acc5: 97.6562 (97.3599)  time: 14.8153  data: 0.0149\n",
      "Epoch: [12]  [29/40]  eta: 0:02:43  lr: 1.000000000000001e-16  img/s: 9.723017215164317  loss: 0.6327 (0.6406)  acc1: 79.6875 (79.2188)  acc5: 97.6562 (97.3177)  time: 14.8333  data: 0.0149\n",
      "Epoch: [12]  [30/40]  eta: 0:02:29  lr: 1.000000000000001e-16  img/s: 7.788763894063706  loss: 0.6327 (0.6372)  acc1: 79.6875 (79.3347)  acc5: 97.6562 (97.3790)  time: 14.9746  data: 0.0149\n",
      "Epoch: [12]  [31/40]  eta: 0:02:13  lr: 1.000000000000001e-16  img/s: 9.459957332205995  loss: 0.6502 (0.6408)  acc1: 79.6875 (79.1992)  acc5: 97.6562 (97.3145)  time: 14.7910  data: 0.0151\n",
      "Epoch: [12]  [32/40]  eta: 0:01:58  lr: 1.000000000000001e-16  img/s: 9.656080170851196  loss: 0.6327 (0.6403)  acc1: 79.6875 (79.2140)  acc5: 97.6562 (97.3011)  time: 14.7670  data: 0.0150\n",
      "Epoch: [12]  [33/40]  eta: 0:01:43  lr: 1.000000000000001e-16  img/s: 8.045004958308239  loss: 0.6327 (0.6405)  acc1: 79.6875 (79.1820)  acc5: 97.6562 (97.3346)  time: 14.7426  data: 0.0151\n",
      "Epoch: [12]  [34/40]  eta: 0:01:28  lr: 1.000000000000001e-16  img/s: 9.64757162820532  loss: 0.6327 (0.6433)  acc1: 79.6875 (79.0848)  acc5: 97.6562 (97.3214)  time: 14.7230  data: 0.0150\n",
      "Epoch: [12]  [35/40]  eta: 0:01:14  lr: 1.000000000000001e-16  img/s: 7.948850984563469  loss: 0.6473 (0.6435)  acc1: 79.6875 (79.1450)  acc5: 96.8750 (97.3090)  time: 14.7272  data: 0.0151\n",
      "Epoch: [12]  [36/40]  eta: 0:00:59  lr: 1.000000000000001e-16  img/s: 9.232391929719686  loss: 0.6327 (0.6381)  acc1: 79.6875 (79.2863)  acc5: 96.8750 (97.3818)  time: 14.7464  data: 0.0150\n",
      "Epoch: [12]  [37/40]  eta: 0:00:44  lr: 1.000000000000001e-16  img/s: 7.877070777547598  loss: 0.6327 (0.6373)  acc1: 79.6875 (79.2969)  acc5: 96.8750 (97.4507)  time: 14.7616  data: 0.0150\n",
      "Epoch: [12]  [38/40]  eta: 0:00:29  lr: 1.000000000000001e-16  img/s: 8.745259780622337  loss: 0.6259 (0.6342)  acc1: 79.6875 (79.4671)  acc5: 97.6562 (97.5160)  time: 14.8265  data: 0.0149\n",
      "Epoch: [12]  [39/40]  eta: 0:00:14  lr: 1.000000000000001e-16  img/s: 6.490055762621056  loss: 0.6327 (0.6430)  acc1: 79.6875 (79.4600)  acc5: 97.6562 (97.5200)  time: 14.0909  data: 0.0143\n",
      "Epoch: [12] Total time: 0:09:40\n",
      "Epoch 14/15\n",
      "Epoch: [13]  [ 0/40]  eta: 0:10:42  lr: 1.000000000000001e-17  img/s: 7.975915678489925  loss: 0.7816 (0.7816)  acc1: 77.3438 (77.3438)  acc5: 94.5312 (94.5312)  time: 16.0621  data: 0.0138\n",
      "Epoch: [13]  [ 1/40]  eta: 0:09:40  lr: 1.000000000000001e-17  img/s: 9.361101864001155  loss: 0.6511 (0.7164)  acc1: 77.3438 (77.7344)  acc5: 94.5312 (96.8750)  time: 14.8753  data: 0.0143\n",
      "Epoch: [13]  [ 2/40]  eta: 0:09:41  lr: 1.000000000000001e-17  img/s: 7.9279510591062765  loss: 0.6533 (0.6954)  acc1: 78.1250 (78.1250)  acc5: 97.6562 (97.1354)  time: 15.3048  data: 0.0157\n",
      "Epoch: [13]  [ 3/40]  eta: 0:09:05  lr: 1.000000000000001e-17  img/s: 9.776867592151014  loss: 0.6533 (0.6975)  acc1: 78.1250 (78.1250)  acc5: 95.3125 (96.6797)  time: 14.7555  data: 0.0156\n",
      "Epoch: [13]  [ 4/40]  eta: 0:09:00  lr: 1.000000000000001e-17  img/s: 7.966741644261929  loss: 0.6533 (0.6806)  acc1: 78.1250 (78.1250)  acc5: 97.6562 (97.0312)  time: 15.0207  data: 0.0154\n",
      "Epoch: [13]  [ 5/40]  eta: 0:08:41  lr: 1.000000000000001e-17  img/s: 9.003759478693492  loss: 0.6511 (0.6744)  acc1: 78.1250 (77.9948)  acc5: 95.3125 (96.6146)  time: 14.8890  data: 0.0152\n",
      "Epoch: [13]  [ 6/40]  eta: 0:08:20  lr: 1.000000000000001e-17  img/s: 9.295511025681119  loss: 0.6511 (0.6583)  acc1: 78.1250 (78.7946)  acc5: 95.3125 (96.4286)  time: 14.7313  data: 0.0152\n",
      "Epoch: [13]  [ 7/40]  eta: 0:08:14  lr: 1.000000000000001e-17  img/s: 7.642836059501812  loss: 0.6511 (0.6590)  acc1: 78.1250 (78.5156)  acc5: 95.3125 (96.5820)  time: 14.9851  data: 0.0151\n",
      "Epoch: [13]  [ 8/40]  eta: 0:07:53  lr: 1.000000000000001e-17  img/s: 9.621308368834104  loss: 0.6511 (0.6512)  acc1: 78.1250 (78.5590)  acc5: 97.6562 (96.7882)  time: 14.7999  data: 0.0150\n",
      "Epoch: [13]  [ 9/40]  eta: 0:07:43  lr: 1.000000000000001e-17  img/s: 7.883094004080753  loss: 0.6511 (0.6707)  acc1: 78.1250 (77.7344)  acc5: 96.8750 (96.7969)  time: 14.9451  data: 0.0150\n",
      "Epoch: [13]  [10/40]  eta: 0:07:24  lr: 1.000000000000001e-17  img/s: 9.510573810258679  loss: 0.6533 (0.6692)  acc1: 78.1250 (77.9830)  acc5: 97.6562 (96.8750)  time: 14.8113  data: 0.0149\n",
      "Epoch: [13]  [11/40]  eta: 0:07:12  lr: 1.000000000000001e-17  img/s: 8.024405918532995  loss: 0.6511 (0.6585)  acc1: 78.1250 (78.5156)  acc5: 97.6562 (96.9401)  time: 14.9076  data: 0.0149\n",
      "Epoch: [13]  [12/40]  eta: 0:06:54  lr: 1.000000000000001e-17  img/s: 9.592937100157977  loss: 0.6511 (0.6569)  acc1: 78.1250 (78.6058)  acc5: 97.6562 (96.9952)  time: 14.7884  data: 0.0149\n",
      "Epoch: [13]  [13/40]  eta: 0:06:40  lr: 1.000000000000001e-17  img/s: 8.177386863934588  loss: 0.6511 (0.6671)  acc1: 78.1250 (78.4598)  acc5: 97.6562 (96.7634)  time: 14.8512  data: 0.0149\n",
      "Epoch: [13]  [14/40]  eta: 0:06:23  lr: 1.000000000000001e-17  img/s: 9.590901708832567  loss: 0.6533 (0.6665)  acc1: 78.1250 (78.3854)  acc5: 97.6562 (96.7188)  time: 14.7518  data: 0.0149\n",
      "Epoch: [13]  [15/40]  eta: 0:06:11  lr: 1.000000000000001e-17  img/s: 7.650668398119033  loss: 0.6511 (0.6622)  acc1: 78.1250 (78.4668)  acc5: 97.6562 (96.7773)  time: 14.8764  data: 0.0149\n",
      "Epoch: [13]  [16/40]  eta: 0:05:55  lr: 1.000000000000001e-17  img/s: 9.417236398824187  loss: 0.6533 (0.6665)  acc1: 78.1250 (78.4467)  acc5: 97.6562 (96.9669)  time: 14.8019  data: 0.0150\n",
      "Epoch: [13]  [17/40]  eta: 0:05:38  lr: 1.000000000000001e-17  img/s: 9.468798301711026  loss: 0.6533 (0.6680)  acc1: 78.1250 (78.4722)  acc5: 97.6562 (96.9618)  time: 14.7314  data: 0.0150\n",
      "Epoch: [13]  [18/40]  eta: 0:05:25  lr: 1.000000000000001e-17  img/s: 7.866904977394793  loss: 0.6533 (0.6670)  acc1: 78.1250 (78.4128)  acc5: 97.6562 (96.9161)  time: 14.8132  data: 0.0150\n",
      "Epoch: [13]  [19/40]  eta: 0:05:10  lr: 1.000000000000001e-17  img/s: 9.1141439893437  loss: 0.6511 (0.6627)  acc1: 78.1250 (78.5938)  acc5: 96.8750 (96.8750)  time: 14.7755  data: 0.0150\n",
      "Epoch: [13]  [20/40]  eta: 0:04:57  lr: 1.000000000000001e-17  img/s: 7.793792004866586  loss: 0.6511 (0.6690)  acc1: 78.1250 (78.5342)  acc5: 96.8750 (96.8750)  time: 14.7944  data: 0.0152\n",
      "Epoch: [13]  [21/40]  eta: 0:04:41  lr: 1.000000000000001e-17  img/s: 9.443511872284436  loss: 0.6488 (0.6624)  acc1: 78.1250 (78.8352)  acc5: 96.8750 (96.9460)  time: 14.7884  data: 0.0152\n",
      "Epoch: [13]  [22/40]  eta: 0:04:27  lr: 1.000000000000001e-17  img/s: 7.89514293872671  loss: 0.6488 (0.6619)  acc1: 78.1250 (79.0082)  acc5: 96.8750 (96.9769)  time: 14.7916  data: 0.0150\n",
      "Epoch: [13]  [23/40]  eta: 0:04:11  lr: 1.000000000000001e-17  img/s: 9.335543218508045  loss: 0.6432 (0.6590)  acc1: 78.9062 (79.1667)  acc5: 96.8750 (96.9401)  time: 14.8225  data: 0.0149\n",
      "Epoch: [13]  [24/40]  eta: 0:03:57  lr: 1.000000000000001e-17  img/s: 8.005603182039565  loss: 0.6432 (0.6562)  acc1: 78.9062 (79.1875)  acc5: 96.8750 (97.0312)  time: 14.8186  data: 0.0149\n",
      "Epoch: [13]  [25/40]  eta: 0:03:42  lr: 1.000000000000001e-17  img/s: 9.405226418349162  loss: 0.6488 (0.6563)  acc1: 78.9062 (79.1767)  acc5: 96.8750 (96.9952)  time: 14.7882  data: 0.0149\n",
      "Epoch: [13]  [26/40]  eta: 0:03:27  lr: 1.000000000000001e-17  img/s: 8.028208787146477  loss: 0.6488 (0.6518)  acc1: 78.9062 (79.2535)  acc5: 97.6562 (97.0486)  time: 14.8969  data: 0.0149\n",
      "Epoch: [13]  [27/40]  eta: 0:03:12  lr: 1.000000000000001e-17  img/s: 9.672899060955466  loss: 0.6374 (0.6492)  acc1: 79.6875 (79.2969)  acc5: 97.6562 (97.1261)  time: 14.7212  data: 0.0149\n",
      "Epoch: [13]  [28/40]  eta: 0:02:56  lr: 1.000000000000001e-17  img/s: 9.871888745416857  loss: 0.6374 (0.6460)  acc1: 79.6875 (79.3373)  acc5: 96.8750 (97.1175)  time: 14.7043  data: 0.0149\n",
      "Epoch: [13]  [29/40]  eta: 0:02:42  lr: 1.000000000000001e-17  img/s: 7.907641985729593  loss: 0.6374 (0.6469)  acc1: 79.6875 (79.4271)  acc5: 96.8750 (97.0312)  time: 14.7018  data: 0.0149\n",
      "Epoch: [13]  [30/40]  eta: 0:02:27  lr: 1.000000000000001e-17  img/s: 9.83353197753408  loss: 0.5983 (0.6408)  acc1: 79.6875 (79.6875)  acc5: 96.8750 (97.0766)  time: 14.6797  data: 0.0150\n",
      "Epoch: [13]  [31/40]  eta: 0:02:12  lr: 1.000000000000001e-17  img/s: 7.996257864318984  loss: 0.6374 (0.6486)  acc1: 79.6875 (79.4678)  acc5: 96.8750 (96.9971)  time: 14.6828  data: 0.0153\n",
      "Epoch: [13]  [32/40]  eta: 0:01:58  lr: 1.000000000000001e-17  img/s: 8.984127488279212  loss: 0.6488 (0.6501)  acc1: 79.6875 (79.5218)  acc5: 96.8750 (96.9460)  time: 14.7280  data: 0.0153\n",
      "Epoch: [13]  [33/40]  eta: 0:01:43  lr: 1.000000000000001e-17  img/s: 8.00682626005475  loss: 0.6488 (0.6577)  acc1: 79.6875 (79.3199)  acc5: 96.8750 (96.9439)  time: 14.7447  data: 0.0153\n",
      "Epoch: [13]  [34/40]  eta: 0:01:28  lr: 1.000000000000001e-17  img/s: 9.140746779031696  loss: 0.6488 (0.6579)  acc1: 79.6875 (79.1964)  acc5: 96.8750 (96.9420)  time: 14.7776  data: 0.0153\n",
      "Epoch: [13]  [35/40]  eta: 0:01:14  lr: 1.000000000000001e-17  img/s: 7.993169660756503  loss: 0.6488 (0.6566)  acc1: 80.4688 (79.2752)  acc5: 96.8750 (96.9618)  time: 14.7417  data: 0.0153\n",
      "Epoch: [13]  [36/40]  eta: 0:00:59  lr: 1.000000000000001e-17  img/s: 9.644515964757327  loss: 0.6162 (0.6555)  acc1: 80.4688 (79.2652)  acc5: 96.8750 (96.9806)  time: 14.7256  data: 0.0152\n",
      "Epoch: [13]  [37/40]  eta: 0:00:44  lr: 1.000000000000001e-17  img/s: 7.979042027322112  loss: 0.6105 (0.6537)  acc1: 80.4688 (79.3791)  acc5: 96.8750 (96.9984)  time: 14.8518  data: 0.0152\n",
      "Epoch: [13]  [38/40]  eta: 0:00:29  lr: 1.000000000000001e-17  img/s: 9.250649743016501  loss: 0.5921 (0.6490)  acc1: 81.2500 (79.5072)  acc5: 96.8750 (97.0553)  time: 14.7301  data: 0.0152\n",
      "Epoch: [13]  [39/40]  eta: 0:00:14  lr: 1.000000000000001e-17  img/s: 6.437507350355397  loss: 0.6105 (0.6651)  acc1: 80.4688 (79.4600)  acc5: 97.6562 (97.0600)  time: 14.0894  data: 0.0145\n",
      "Epoch: [13] Total time: 0:09:37\n",
      "Epoch 15/15\n",
      "Epoch: [14]  [ 0/40]  eta: 0:11:26  lr: 1.000000000000001e-18  img/s: 7.460251608775994  loss: 0.8112 (0.8112)  acc1: 73.4375 (73.4375)  acc5: 95.3125 (95.3125)  time: 17.1703  data: 0.0127\n",
      "Epoch: [14]  [ 1/40]  eta: 0:10:04  lr: 1.000000000000001e-18  img/s: 9.253792166113827  loss: 0.7140 (0.7626)  acc1: 73.4375 (75.3906)  acc5: 95.3125 (96.4844)  time: 15.5098  data: 0.0149\n",
      "Epoch: [14]  [ 2/40]  eta: 0:09:25  lr: 1.000000000000001e-18  img/s: 9.417382261609092  loss: 0.7140 (0.7168)  acc1: 77.3438 (77.0833)  acc5: 97.6562 (96.8750)  time: 14.8762  data: 0.0157\n",
      "Epoch: [14]  [ 3/40]  eta: 0:09:19  lr: 1.000000000000001e-18  img/s: 8.097297841567853  loss: 0.6253 (0.6890)  acc1: 77.3438 (77.7344)  acc5: 97.6562 (97.2656)  time: 15.1129  data: 0.0155\n",
      "Epoch: [14]  [ 4/40]  eta: 0:08:53  lr: 1.000000000000001e-18  img/s: 9.372721333338617  loss: 0.6763 (0.6865)  acc1: 77.3438 (77.5000)  acc5: 97.6562 (97.3438)  time: 14.8251  data: 0.0159\n",
      "Epoch: [14]  [ 5/40]  eta: 0:08:45  lr: 1.000000000000001e-18  img/s: 7.984921913450387  loss: 0.6253 (0.6737)  acc1: 77.3438 (78.2552)  acc5: 97.6562 (97.6562)  time: 15.0283  data: 0.0156\n",
      "Epoch: [14]  [ 6/40]  eta: 0:08:24  lr: 1.000000000000001e-18  img/s: 9.415763487479083  loss: 0.6253 (0.6548)  acc1: 79.6875 (78.7946)  acc5: 97.6562 (97.6562)  time: 14.8255  data: 0.0154\n",
      "Epoch: [14]  [ 7/40]  eta: 0:08:13  lr: 1.000000000000001e-18  img/s: 8.064689736724462  loss: 0.6253 (0.6645)  acc1: 77.3438 (78.4180)  acc5: 97.6562 (97.7539)  time: 14.9583  data: 0.0155\n",
      "Epoch: [14]  [ 8/40]  eta: 0:07:54  lr: 1.000000000000001e-18  img/s: 9.247343798147917  loss: 0.6253 (0.6573)  acc1: 79.6875 (78.9062)  acc5: 97.6562 (97.6562)  time: 14.8359  data: 0.0155\n",
      "Epoch: [14]  [ 9/40]  eta: 0:07:45  lr: 1.000000000000001e-18  img/s: 7.640439890033185  loss: 0.6253 (0.6694)  acc1: 77.3438 (78.4375)  acc5: 97.6562 (97.4219)  time: 15.0291  data: 0.0154\n",
      "Epoch: [14]  [10/40]  eta: 0:07:26  lr: 1.000000000000001e-18  img/s: 9.493293797784787  loss: 0.6763 (0.6736)  acc1: 77.3438 (78.1960)  acc5: 97.6562 (97.4432)  time: 14.8899  data: 0.0154\n",
      "Epoch: [14]  [11/40]  eta: 0:07:14  lr: 1.000000000000001e-18  img/s: 8.069792804321963  loss: 0.6253 (0.6617)  acc1: 77.3438 (78.5156)  acc5: 97.6562 (97.5260)  time: 14.9723  data: 0.0155\n",
      "Epoch: [14]  [12/40]  eta: 0:06:57  lr: 1.000000000000001e-18  img/s: 9.013837182811882  loss: 0.6658 (0.6620)  acc1: 79.6875 (78.6659)  acc5: 97.6562 (97.3558)  time: 14.9140  data: 0.0154\n",
      "Epoch: [14]  [13/40]  eta: 0:06:39  lr: 1.000000000000001e-18  img/s: 9.521454263240885  loss: 0.6658 (0.6662)  acc1: 77.3438 (78.5156)  acc5: 97.6562 (97.2656)  time: 14.8100  data: 0.0153\n",
      "Epoch: [14]  [14/40]  eta: 0:06:26  lr: 1.000000000000001e-18  img/s: 8.054433336388852  loss: 0.6763 (0.6679)  acc1: 77.3438 (78.3333)  acc5: 97.6562 (97.2396)  time: 14.8831  data: 0.0153\n",
      "Epoch: [14]  [15/40]  eta: 0:06:09  lr: 1.000000000000001e-18  img/s: 9.69752245674294  loss: 0.6658 (0.6631)  acc1: 77.3438 (78.6133)  acc5: 97.6562 (97.3145)  time: 14.7788  data: 0.0152\n",
      "Epoch: [14]  [16/40]  eta: 0:05:56  lr: 1.000000000000001e-18  img/s: 8.041900436537526  loss: 0.6763 (0.6658)  acc1: 78.9062 (78.6305)  acc5: 97.6562 (97.2886)  time: 14.8468  data: 0.0154\n",
      "Epoch: [14]  [17/40]  eta: 0:05:40  lr: 1.000000000000001e-18  img/s: 9.274216721114687  loss: 0.6658 (0.6606)  acc1: 78.1250 (78.6024)  acc5: 97.6562 (97.2222)  time: 14.7896  data: 0.0154\n",
      "Epoch: [14]  [18/40]  eta: 0:05:27  lr: 1.000000000000001e-18  img/s: 7.908815622290681  loss: 0.6763 (0.6628)  acc1: 78.1250 (78.4539)  acc5: 97.6562 (97.2451)  time: 14.8638  data: 0.0154\n",
      "Epoch: [14]  [19/40]  eta: 0:05:11  lr: 1.000000000000001e-18  img/s: 9.229925359056072  loss: 0.6658 (0.6558)  acc1: 78.1250 (78.7109)  acc5: 97.6562 (97.2656)  time: 14.8152  data: 0.0158\n",
      "Epoch: [14]  [20/40]  eta: 0:04:58  lr: 1.000000000000001e-18  img/s: 7.704918110896843  loss: 0.6296 (0.6546)  acc1: 78.9062 (78.7574)  acc5: 97.6562 (97.2842)  time: 14.7880  data: 0.0159\n",
      "Epoch: [14]  [21/40]  eta: 0:04:41  lr: 1.000000000000001e-18  img/s: 9.474305030279712  loss: 0.6253 (0.6504)  acc1: 79.6875 (78.9773)  acc5: 97.6562 (97.3722)  time: 14.7718  data: 0.0158\n",
      "Epoch: [14]  [22/40]  eta: 0:04:27  lr: 1.000000000000001e-18  img/s: 8.178366726560315  loss: 0.6296 (0.6518)  acc1: 79.6875 (79.1440)  acc5: 97.6562 (97.3845)  time: 14.8746  data: 0.0156\n",
      "Epoch: [14]  [23/40]  eta: 0:04:11  lr: 1.000000000000001e-18  img/s: 9.428101570934318  loss: 0.6296 (0.6496)  acc1: 79.6875 (79.1667)  acc5: 97.6562 (97.3958)  time: 14.7630  data: 0.0156\n",
      "Epoch: [14]  [24/40]  eta: 0:03:56  lr: 1.000000000000001e-18  img/s: 9.701653421900188  loss: 0.6296 (0.6538)  acc1: 79.6875 (79.0312)  acc5: 97.6562 (97.4062)  time: 14.7397  data: 0.0155\n",
      "Epoch: [14]  [25/40]  eta: 0:03:42  lr: 1.000000000000001e-18  img/s: 7.8734770306323005  loss: 0.6296 (0.6495)  acc1: 79.6875 (79.1466)  acc5: 97.6562 (97.4760)  time: 14.7512  data: 0.0156\n",
      "Epoch: [14]  [26/40]  eta: 0:03:27  lr: 1.000000000000001e-18  img/s: 8.978427861505908  loss: 0.6296 (0.6478)  acc1: 79.6875 (79.3403)  acc5: 97.6562 (97.4826)  time: 14.7843  data: 0.0156\n",
      "Epoch: [14]  [27/40]  eta: 0:03:13  lr: 1.000000000000001e-18  img/s: 7.857452440774189  loss: 0.6228 (0.6469)  acc1: 79.6875 (79.4364)  acc5: 97.6562 (97.5446)  time: 14.8051  data: 0.0155\n",
      "Epoch: [14]  [28/40]  eta: 0:02:57  lr: 1.000000000000001e-18  img/s: 9.198575012301127  loss: 0.6228 (0.6444)  acc1: 79.6875 (79.5259)  acc5: 97.6562 (97.5754)  time: 14.8088  data: 0.0155\n",
      "Epoch: [14]  [29/40]  eta: 0:02:43  lr: 1.000000000000001e-18  img/s: 7.993483610430964  loss: 0.6228 (0.6474)  acc1: 79.6875 (79.3750)  acc5: 97.6562 (97.5521)  time: 14.7718  data: 0.0155\n",
      "Epoch: [14]  [30/40]  eta: 0:02:28  lr: 1.000000000000001e-18  img/s: 9.457645412658687  loss: 0.6036 (0.6445)  acc1: 80.4688 (79.5111)  acc5: 97.6562 (97.5554)  time: 14.7744  data: 0.0155\n",
      "Epoch: [14]  [31/40]  eta: 0:02:13  lr: 1.000000000000001e-18  img/s: 8.015515558452519  loss: 0.6228 (0.6479)  acc1: 79.6875 (79.3701)  acc5: 97.6562 (97.4609)  time: 14.7798  data: 0.0155\n",
      "Epoch: [14]  [32/40]  eta: 0:01:58  lr: 1.000000000000001e-18  img/s: 9.348451167289367  loss: 0.6228 (0.6488)  acc1: 79.6875 (79.2614)  acc5: 97.6562 (97.4905)  time: 14.7544  data: 0.0156\n",
      "Epoch: [14]  [33/40]  eta: 0:01:43  lr: 1.000000000000001e-18  img/s: 7.933082236962553  loss: 0.6228 (0.6519)  acc1: 79.6875 (79.1131)  acc5: 97.6562 (97.5414)  time: 14.8890  data: 0.0155\n",
      "Epoch: [14]  [34/40]  eta: 0:01:29  lr: 1.000000000000001e-18  img/s: 8.683945035753379  loss: 0.6228 (0.6522)  acc1: 79.6875 (79.0848)  acc5: 97.6562 (97.5446)  time: 14.8314  data: 0.0156\n",
      "Epoch: [14]  [35/40]  eta: 0:01:14  lr: 1.000000000000001e-18  img/s: 7.993569183250747  loss: 0.6228 (0.6503)  acc1: 79.6875 (79.2101)  acc5: 97.6562 (97.4826)  time: 14.9721  data: 0.0156\n",
      "Epoch: [14]  [36/40]  eta: 0:00:59  lr: 1.000000000000001e-18  img/s: 9.600737571159332  loss: 0.6036 (0.6475)  acc1: 79.6875 (79.3708)  acc5: 97.6562 (97.5084)  time: 14.8427  data: 0.0154\n",
      "Epoch: [14]  [37/40]  eta: 0:00:44  lr: 1.000000000000001e-18  img/s: 9.125586207050118  loss: 0.6036 (0.6462)  acc1: 80.4688 (79.3997)  acc5: 97.6562 (97.5329)  time: 14.8539  data: 0.0154\n",
      "Epoch: [14]  [38/40]  eta: 0:00:29  lr: 1.000000000000001e-18  img/s: 8.063474229991487  loss: 0.6036 (0.6460)  acc1: 80.4688 (79.3269)  acc5: 97.6562 (97.5761)  time: 14.8383  data: 0.0153\n",
      "Epoch: [14]  [39/40]  eta: 0:00:14  lr: 1.000000000000001e-18  img/s: 7.061080697176467  loss: 0.6228 (0.6524)  acc1: 79.6875 (79.3000)  acc5: 97.6562 (97.5800)  time: 14.2005  data: 0.0143\n",
      "Epoch: [14] Total time: 0:09:40\n"
     ]
    }
   ],
   "source": [
    "axx_mult = 'appro2'\n",
    "model = DensNet121(pretrained=True, axx_mult = axx_mult)\n",
    "model.eval() # for evaluation\n",
    "# It is a bit slow since we collect histograms on CPU\n",
    "with torch.no_grad():\n",
    "    stats = collect_stats(model, data_t, num_batches=2)\n",
    "    amax = compute_amax(model, method=\"percentile\", percentile=99.99)\n",
    "    \n",
    "# Inference without retraining\n",
    "import timeit\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "model.eval()\n",
    "start_time = timeit.default_timer()\n",
    "with torch.no_grad():\n",
    "    for iteraction, (images, labels) in tqdm(enumerate(data), total=len(data)):\n",
    "        images, labels = images.to(\"cpu\"), labels.to(\"cpu\")\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(timeit.default_timer() - start_time)\n",
    "print('Accuracy of the network on the 10000 test images: %.4f %%' % (\n",
    "    100 * correct / total))\n",
    "\n",
    "# Retraining for 15 epochs\n",
    "from adapt.references.classification.train import evaluate, train_one_epoch, load_data\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "for epoch in range(15):\n",
    "    print(f\"Epoch {epoch + 1}/15\")\n",
    "    train_one_epoch(model, criterion, optimizer, data_t, \"cpu\", epoch, 1)\n",
    "    lr_scheduler.step()\n",
    "\n",
    "# Inference after retraining\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "model.eval()\n",
    "start_time = timeit.default_timer()\n",
    "with torch.no_grad():\n",
    "    for iteraction, (images, labels) in tqdm(enumerate(data), total=len(data)):\n",
    "        images, labels = images.to(\"cpu\"), labels.to(\"cpu\")\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(timeit.default_timer() - start_time)\n",
    "print('Accuracy of the network on the 10000 test images: %.4f %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ca82eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "axx_mult = 'appro3'\n",
    "model = DensNet121(pretrained=True, axx_mult = axx_mult)\n",
    "model.eval() # for evaluation\n",
    "# It is a bit slow since we collect histograms on CPU\n",
    "with torch.no_grad():\n",
    "    stats = collect_stats(model, data_t, num_batches=2)\n",
    "    amax = compute_amax(model, method=\"percentile\", percentile=99.99)\n",
    "    \n",
    "# Inference without retraining\n",
    "import timeit\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "model.eval()\n",
    "start_time = timeit.default_timer()\n",
    "with torch.no_grad():\n",
    "    for iteraction, (images, labels) in tqdm(enumerate(data), total=len(data)):\n",
    "        images, labels = images.to(\"cpu\"), labels.to(\"cpu\")\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(timeit.default_timer() - start_time)\n",
    "print('Accuracy of the network on the 10000 test images: %.4f %%' % (\n",
    "    100 * correct / total))\n",
    "\n",
    "# Retraining for 15 epochs\n",
    "from adapt.references.classification.train import evaluate, train_one_epoch, load_data\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "for epoch in range(15):\n",
    "    print(f\"Epoch {epoch + 1}/15\")\n",
    "    train_one_epoch(model, criterion, optimizer, data_t, \"cpu\", epoch, 1)\n",
    "    lr_scheduler.step()\n",
    "\n",
    "# Inference after retraining\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "model.eval()\n",
    "start_time = timeit.default_timer()\n",
    "with torch.no_grad():\n",
    "    for iteraction, (images, labels) in tqdm(enumerate(data), total=len(data)):\n",
    "        images, labels = images.to(\"cpu\"), labels.to(\"cpu\")\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(timeit.default_timer() - start_time)\n",
    "print('Accuracy of the network on the 10000 test images: %.4f %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3dffea-b09b-4f99-84ff-785ff168f2f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
