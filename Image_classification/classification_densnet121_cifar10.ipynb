{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb626d36",
   "metadata": {},
   "source": [
    "Name: Ziwei Liu\n",
    "Student ID: 3766789"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "458eb16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import torch\n",
    "\n",
    "import requests\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms as T\n",
    "from torchvision.datasets import CIFAR10\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ab55c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.models.densenet import densenet121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f4cd1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "threads = 16\n",
    "torch.set_num_threads(threads)\n",
    "\n",
    "#maybe better performance (Jupyter)\n",
    "#%env OMP_PLACES=cores\n",
    "#%env OMP_PROC_BIND=close\n",
    "#%env OMP_WAIT_POLICY=active\n",
    "\n",
    "#WSL\n",
    "os.environ['OMP_PLACES'] = 'cores'\n",
    "os.environ['OMP_PROC_BIND'] = 'close'\n",
    "os.environ['OMP_WAIT_POLICY'] = 'active'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3cbd963",
   "metadata": {},
   "outputs": [],
   "source": [
    "#axx_mult = 'mul8s_acc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fac997d5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_exact, skipping build step...\n",
      "Loading extension module PyInit_conv2d_exact...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DenseNet(\n",
       "  (features): Sequential(\n",
       "    (conv0): AdaPT_Conv2d(\n",
       "      3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "      (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "    )\n",
       "    (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu0): ReLU(inplace=True)\n",
       "    (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (denseblock1): _DenseBlock(\n",
       "      (denselayer1): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer2): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer3): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer4): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer5): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer6): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (transition1): _Transition(\n",
       "      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv): AdaPT_Conv2d(\n",
       "        256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    )\n",
       "    (denseblock2): _DenseBlock(\n",
       "      (denselayer1): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer2): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer3): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer4): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer5): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer6): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer7): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer8): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer9): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer10): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer11): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer12): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (transition2): _Transition(\n",
       "      (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv): AdaPT_Conv2d(\n",
       "        512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    )\n",
       "    (denseblock3): _DenseBlock(\n",
       "      (denselayer1): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer2): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer3): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer4): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer5): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer6): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer7): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer8): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer9): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer10): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer11): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer12): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer13): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer14): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer15): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer16): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer17): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer18): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer19): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer20): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer21): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer22): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer23): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer24): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (transition3): _Transition(\n",
       "      (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv): AdaPT_Conv2d(\n",
       "        1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      )\n",
       "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    )\n",
       "    (denseblock4): _DenseBlock(\n",
       "      (denselayer1): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer2): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer3): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer4): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer5): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer6): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer7): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer8): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer9): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer10): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer11): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer12): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer13): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer14): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer15): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "      (denselayer16): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): AdaPT_Conv2d(\n",
       "          992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): AdaPT_Conv2d(\n",
       "          128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=1024, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "axx_mult = 'exact'\n",
    "model = densenet121(pretrained=True, axx_mult = axx_mult)\n",
    "model.eval() # for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47008795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "def val_dataloader(mean = (0.4914, 0.4822, 0.4465), std = (0.2471, 0.2435, 0.2616)):\n",
    "\n",
    "    transform = T.Compose(\n",
    "        [\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean, std),\n",
    "        ]\n",
    "    )\n",
    "    dataset = CIFAR10(root=\"datasets/cifar10_data\", train=False, download=True, transform=transform)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=1024,\n",
    "        num_workers=0,\n",
    "        drop_last=True,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "transform = T.Compose(\n",
    "        [\n",
    "            T.RandomCrop(32, padding=4),\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean = (0.4914, 0.4822, 0.4465), std = (0.2471, 0.2435, 0.2616)),\n",
    "        ]\n",
    "    )\n",
    "dataset = CIFAR10(root=\"datasets/cifar10_data\", train=True, download=True, transform=transform)\n",
    "\n",
    "evens = list(range(0, len(dataset), 10))\n",
    "trainset_1 = torch.utils.data.Subset(dataset, evens)\n",
    "\n",
    "data = val_dataloader()\n",
    "\n",
    "# data_t is used for calibration purposes and is a subset of train-set\n",
    "data_t = DataLoader(trainset_1, batch_size=128,\n",
    "                                            shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8b3320f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [00:09<00:00,  4.90s/it]\n",
      "W0222 02:23:02.289330 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.289930 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.290439 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.290885 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.291328 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.291781 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.292217 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.292624 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.293027 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.293437 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.293871 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.294309 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.294726 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.295136 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.295551 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.295946 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.296352 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.296755 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.297162 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.297555 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.297943 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.298187 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.298480 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.298746 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.298992 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.299226 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.299467 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.299706 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.299957 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.300182 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.300410 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.300630 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.300856 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.301076 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.301308 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.301526 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.301749 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.301967 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.302188 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.302406 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.302630 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.302842 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.303061 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.303276 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.303496 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.303719 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.303939 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.304158 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.304388 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.304607 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.304831 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.305056 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.305280 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.305504 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.305726 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.305967 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.306293 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.306617 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.306960 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.307290 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.307631 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.307947 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.308290 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.308625 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.308951 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.309288 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.309627 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.309963 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.310293 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.310631 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.310976 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.311298 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.311631 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.311961 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.312301 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.312619 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.312947 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.313267 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.313599 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.314046 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.314521 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.314944 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.315362 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.315806 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.316252 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.316619 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.317029 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.317425 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.317860 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.318217 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.318622 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.319006 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.319411 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.319802 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.320218 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.320631 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.321041 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.321444 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.321835 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.322233 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.322638 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.322969 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.323196 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.323416 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.323654 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.323877 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.324105 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.324324 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.324551 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.324768 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.324993 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.325211 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.325434 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.325648 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.325867 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.326080 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.326299 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.326515 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.326739 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.326955 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.327176 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.327393 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.327622 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.327838 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.328056 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.328272 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.328494 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.328711 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.328933 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.329149 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.329373 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.329597 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.329825 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.330046 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.330273 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.330492 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.330714 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.330930 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.331154 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.331370 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.331596 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.331813 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.332034 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.332251 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.332469 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.332686 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.332906 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.333121 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.333338 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.333550 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.333779 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.333994 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.334214 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.334428 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.334648 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.334865 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.335086 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.335302 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.335534 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.335762 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.335985 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.336206 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.336431 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.336650 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.336875 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.337095 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.337320 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.337540 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.337761 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.337984 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.338207 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.338423 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.338643 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.338859 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.339086 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.339303 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.339536 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.339759 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.339981 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.340197 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.340418 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.340634 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.340852 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.341065 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.341496 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.341721 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.341944 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.342167 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.342393 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.342607 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.342827 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.343042 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.343265 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.343481 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.343707 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.343921 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.344142 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.344362 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.344586 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.344803 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.345031 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.345248 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.345469 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.345690 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.345916 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.346133 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.346363 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.346580 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.346802 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.347016 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.347237 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.347453 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.347683 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.347908 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.348247 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.348569 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.348924 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.349241 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.349562 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.349896 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.350232 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.350557 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.350881 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.351207 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.351549 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.351883 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.352220 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.352555 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.352893 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.353227 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.353566 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.353886 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.354216 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.354547 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.354884 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.355210 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.355577 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.355902 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.356136 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.356351 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:23:02.369845 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.370563 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.371083 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.371664 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.372352 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.373044 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.373767 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.374424 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.375112 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.375775 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.376657 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.377511 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.378370 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.379224 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.379891 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.380870 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.381682 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.382481 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.383305 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.384130 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.384940 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.385725 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.386526 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.387302 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.388111 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.388885 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.389687 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.390453 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.391260 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.392059 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.392857 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.393635 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.394435 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.395198 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.395988 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.396756 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.397555 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.398317 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.399098 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.399900 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.400691 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.401466 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.402249 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.403009 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.403813 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.404570 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.405360 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.406118 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.406900 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.407662 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.408468 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.409228 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.410018 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.410777 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.411559 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.412320 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.413106 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.413884 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.414675 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.415435 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.416235 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.416991 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.417778 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.418541 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.419332 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.420104 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.420895 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.421656 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.422446 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.423404 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.424226 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.424990 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.425788 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.426556 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.427357 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.428136 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.428925 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.429685 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.430477 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.431241 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.432034 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.432803 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.433586 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.434369 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.435150 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.435930 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.436717 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.437492 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.438270 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.439029 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.439827 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.440590 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.441385 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.442152 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.442947 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.443754 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.444565 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.445336 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.446114 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.446878 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.447674 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.448450 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.449231 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.449990 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.450772 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.451547 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.452343 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.453117 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.453908 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.454701 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.455490 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.456272 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.457052 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.457817 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.458602 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.459368 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.460164 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.460934 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.461729 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.462492 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.463266 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.464064 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.464864 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.465632 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.466415 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.467181 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.467964 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.468742 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.469531 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.470298 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.471077 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.471863 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.472645 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.473410 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.474214 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.474991 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.475780 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.476559 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.477344 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.478108 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.478895 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.479673 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.480469 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.481247 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.482028 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.482802 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.483595 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.484401 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.485188 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.485950 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.486735 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.487491 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.488216 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.488875 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.489464 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.489979 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.490484 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.490962 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.491453 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.491963 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.492475 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.492952 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.493450 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.493866 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.494390 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.494853 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.495355 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.495837 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.496344 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.496812 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.497507 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.498238 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.499060 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.499881 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.500720 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.501554 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.502405 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.503239 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.504073 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.504893 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.505727 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.506534 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.507334 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.508145 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.508957 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.509758 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.510564 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.511352 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.512160 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.512960 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.513761 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.514554 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.515345 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.516151 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.516950 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.517740 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.518538 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.519328 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.520131 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.520917 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.521713 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.522507 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.523300 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.524110 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.524909 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.525711 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.526513 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.527314 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.528143 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.528966 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.529761 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.530570 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.531390 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.532217 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.533036 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.533857 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.534678 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.535502 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.536325 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.537149 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.537976 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.538783 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.539607 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.540427 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.541241 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.542036 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.542848 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.543666 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.544478 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.545289 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.546088 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.546891 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.547707 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.548514 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.549305 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.550115 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.550912 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.551719 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.552528 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:23:02.553321 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.conv0.quantizer                : TensorQuantizer(8bit per-tensor amax=2.1255 calibrator=HistogramCalibrator quant)\n",
      "features.conv0.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.1544 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer1.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.4862 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer1.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0649 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer1.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.1260 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer1.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0350 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer2.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.3875 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer2.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0556 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer2.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.1102 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer2.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0295 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer3.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.2227 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer3.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0524 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer3.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0947 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer3.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0293 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer4.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.2207 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer4.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0360 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer4.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.1142 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer4.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0259 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer5.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.1607 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer5.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0332 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer5.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0726 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer5.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0227 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer6.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.1492 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer6.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0406 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer6.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0814 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer6.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0247 calibrator=HistogramCalibrator quant)\n",
      "features.transition1.conv.quantizer     : TensorQuantizer(8bit per-tensor amax=0.2026 calibrator=HistogramCalibrator quant)\n",
      "features.transition1.conv.quantizer_w   : TensorQuantizer(8bit per-tensor amax=0.0519 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer1.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.1689 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer1.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0308 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer1.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0820 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer1.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0197 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer2.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.1395 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer2.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0267 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer2.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0717 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer2.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0230 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer3.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.1323 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer3.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0284 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer3.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0722 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer3.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0236 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer4.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.1071 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer4.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0266 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer4.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0752 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer4.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0222 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer5.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0984 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer5.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0265 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer5.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0689 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer5.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0243 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer6.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.1020 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer6.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0289 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer6.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0722 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer6.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0233 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer7.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0904 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer7.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0265 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer7.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0700 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer7.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0243 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer8.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0886 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer8.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0293 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer8.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0673 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer8.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0282 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer9.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0879 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer9.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0266 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer9.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0669 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer9.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0246 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer10.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0890 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer10.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0283 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer10.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0711 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer10.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0228 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer11.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0868 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer11.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0305 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer11.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0778 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer11.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0218 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer12.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0840 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer12.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0251 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer12.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0725 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer12.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0244 calibrator=HistogramCalibrator quant)\n",
      "features.transition2.conv.quantizer     : TensorQuantizer(8bit per-tensor amax=0.1379 calibrator=HistogramCalibrator quant)\n",
      "features.transition2.conv.quantizer_w   : TensorQuantizer(8bit per-tensor amax=0.0310 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer1.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0622 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer1.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0260 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer1.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0506 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer1.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0208 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer2.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0845 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer2.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0287 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer2.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0552 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer2.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0174 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer3.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0655 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer3.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0251 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer3.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0549 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer3.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0235 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer4.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0736 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer4.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0288 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer4.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0616 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer4.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0217 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer5.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0681 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer5.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0237 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer5.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0560 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer5.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0202 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer6.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0747 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer6.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0209 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer6.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0430 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer6.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0242 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer7.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0619 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer7.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0236 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer7.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0828 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer7.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0146 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer8.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0595 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer8.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0249 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer8.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0601 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer8.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0200 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer9.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0624 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer9.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0181 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer9.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0651 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer9.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0135 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer10.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0613 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer10.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0194 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer10.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0645 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer10.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0137 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer11.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0583 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer11.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0220 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer11.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0556 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer11.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0175 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer12.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0606 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer12.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0202 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer12.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0687 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer12.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0133 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer13.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0557 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer13.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0203 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer13.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0755 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer13.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0139 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer14.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0607 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer14.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0191 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer14.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0749 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer14.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0110 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer15.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0579 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer15.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0187 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer15.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0663 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer15.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0125 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer16.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0539 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer16.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0191 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer16.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0789 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer16.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0150 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer17.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0487 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer17.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0214 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer17.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0798 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer17.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0173 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer18.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0581 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer18.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0196 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer18.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0868 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer18.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0235 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer19.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0537 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer19.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0197 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer19.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0887 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer19.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0163 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer20.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0475 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer20.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0219 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer20.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0871 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer20.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0224 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer21.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0695 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer21.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0192 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer21.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0754 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer21.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0210 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer22.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0715 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer22.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0218 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer22.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0786 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer22.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0200 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer23.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0732 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer23.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0194 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer23.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.1397 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer23.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0256 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer24.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0673 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer24.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0184 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer24.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.1528 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer24.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0201 calibrator=HistogramCalibrator quant)\n",
      "features.transition3.conv.quantizer     : TensorQuantizer(8bit per-tensor amax=0.6439 calibrator=HistogramCalibrator quant)\n",
      "features.transition3.conv.quantizer_w   : TensorQuantizer(8bit per-tensor amax=0.0380 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer1.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0568 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer1.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0102 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer1.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.2451 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer1.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0348 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer2.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0550 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer2.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0118 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer2.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.2528 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer2.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0350 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer3.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0515 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer3.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0116 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer3.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.3123 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer3.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0352 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer4.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0606 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer4.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0117 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer4.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.2793 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer4.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0349 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer5.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0539 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer5.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0119 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer5.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.3152 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer5.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0382 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer6.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0473 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer6.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0104 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer6.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.2696 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer6.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0333 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer7.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0508 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer7.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0112 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer7.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.3306 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer7.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0326 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer8.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0522 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer8.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0100 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer8.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.2843 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer8.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0319 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer9.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0543 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer9.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0100 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer9.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.3476 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer9.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0332 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer10.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0512 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer10.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0108 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer10.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.4159 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer10.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0354 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer11.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0450 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer11.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0092 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer11.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.3182 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer11.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0376 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer12.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0436 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer12.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0095 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer12.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.2698 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer12.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0363 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer13.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0518 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer13.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0101 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer13.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.2911 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer13.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0405 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer14.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0534 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer14.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0102 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer14.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.3209 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer14.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0362 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer15.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0564 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer15.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0090 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer15.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.3142 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer15.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0319 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer16.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0606 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer16.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0108 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer16.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.3465 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer16.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0330 calibrator=HistogramCalibrator quant)\n"
     ]
    }
   ],
   "source": [
    "from pytorch_quantization import nn as quant_nn\n",
    "from pytorch_quantization import calib\n",
    "\n",
    "def collect_stats(model, data_loader, num_batches):\n",
    "     \"\"\"Feed data to the network and collect statistic\"\"\"\n",
    "\n",
    "     # Enable calibrators\n",
    "     for name, module in model.named_modules():\n",
    "         if isinstance(module, quant_nn.TensorQuantizer):\n",
    "             if module._calibrator is not None:\n",
    "                 module.disable_quant()\n",
    "                 module.enable_calib()\n",
    "             else:\n",
    "                 module.disable()\n",
    "\n",
    "     for i, (image, _) in tqdm(enumerate(data_loader), total=num_batches):\n",
    "         model(image.cpu())\n",
    "         if i >= num_batches:\n",
    "             break\n",
    "\n",
    "     # Disable calibrators\n",
    "     for name, module in model.named_modules():\n",
    "         if isinstance(module, quant_nn.TensorQuantizer):\n",
    "             if module._calibrator is not None:\n",
    "                 module.enable_quant()\n",
    "                 module.disable_calib()\n",
    "             else:\n",
    "                 module.enable()\n",
    "\n",
    "def compute_amax(model, **kwargs):\n",
    " # Load calib result\n",
    " for name, module in model.named_modules():\n",
    "     if isinstance(module, quant_nn.TensorQuantizer):\n",
    "         if module._calibrator is not None:\n",
    "             if isinstance(module._calibrator, calib.MaxCalibrator):\n",
    "                 module.load_calib_amax()\n",
    "             else:\n",
    "                 module.load_calib_amax(**kwargs)\n",
    "         print(F\"{name:40}: {module}\")\n",
    " model.cpu()\n",
    "\n",
    "# It is a bit slow since we collect histograms on CPU\n",
    "with torch.no_grad():\n",
    "    stats = collect_stats(model, data_t, num_batches=2)\n",
    "    amax = compute_amax(model, method=\"percentile\", percentile=99.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a0b99b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [01:52<00:00, 12.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112.20802609995008\n",
      "Accuracy of the network on the 10000 test images: 93.8043 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Inference without retraining\n",
    "import timeit\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "model.eval()\n",
    "start_time = timeit.default_timer()\n",
    "with torch.no_grad():\n",
    "    for iteraction, (images, labels) in tqdm(enumerate(data), total=len(data)):\n",
    "        images, labels = images.to(\"cpu\"), labels.to(\"cpu\")\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(timeit.default_timer() - start_time)\n",
    "print('Accuracy of the network on the 10000 test images: %.4f %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4939a153",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro1, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [00:06<00:00,  3.05s/it]\n",
      "W0222 02:25:04.695062 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.695796 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.696300 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.696727 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.697151 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.697560 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.697994 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.698407 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.698829 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.699223 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.699649 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.700058 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.700466 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.700875 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.701286 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.701679 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.702079 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.702488 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.702853 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.703245 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.703658 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.703985 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.704214 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.704431 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.704660 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.704889 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.705172 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.705434 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.705719 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.705986 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.706257 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.706519 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.706792 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.707054 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.707323 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.707592 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.707870 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.708131 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.708400 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.708667 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.708940 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.709207 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.709481 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.709745 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.710021 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.710282 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.710554 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.710813 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.711087 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.711346 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.711627 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.711909 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.712297 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.712675 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.713059 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.713424 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.713803 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.714171 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.714549 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.714924 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.715300 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.715681 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.716069 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.716437 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.716816 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.717182 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.717559 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.717928 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.718312 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.718677 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.719047 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.719413 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.719824 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.720187 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.720563 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.720927 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.721302 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.721667 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.722049 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.722419 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.722791 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.723151 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.723536 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.723904 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.724281 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.724641 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.725017 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.725384 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.725759 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.726123 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.726502 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.726874 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.727245 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.727637 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.728022 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.728387 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.728762 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.729029 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.729301 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.729560 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.729829 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.730088 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.730359 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.730616 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.730882 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.731141 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.731411 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.731678 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.731960 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.732221 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.732497 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.732765 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.733039 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.733303 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.733576 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.733836 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.734105 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.734364 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.734635 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.734899 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.735168 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.735428 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.735721 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.735981 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.736248 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.736506 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.736784 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.737047 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.737316 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.737575 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.737845 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.738216 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.738590 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.738951 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.739332 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.739707 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.740089 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.740451 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.740833 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.741201 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.741570 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.741938 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.742310 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.742672 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.743046 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.743413 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.743805 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.744179 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.744549 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.744918 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.745298 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.745658 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.746030 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.746395 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.746771 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.747133 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.747505 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.747890 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.748269 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.748633 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.749009 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.749375 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.749756 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.750117 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.750510 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.750893 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.751283 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.751657 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.752043 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.752418 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.752802 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.753175 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.753551 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.753909 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.754190 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.754448 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.754734 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.754996 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.755275 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.755538 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.755815 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.756076 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.756345 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.756618 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.756887 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.757149 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.757420 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.757681 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.757953 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.758211 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.758478 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.759023 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.759301 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.759562 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.759839 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.760103 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.760376 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.760635 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.760902 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.761164 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.761437 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.761697 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.761965 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.762224 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.762495 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.762755 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.763122 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.763481 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.763864 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.764232 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.764609 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.764980 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.765356 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.765728 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.766102 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.766463 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.766837 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.767205 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.767583 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.767956 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.768328 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.768695 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.769065 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.769431 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.769806 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.770171 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.770550 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.770920 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.771302 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.771674 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.772059 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.772422 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.772810 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.773176 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.773543 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.773909 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.774282 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.774650 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.775022 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.775393 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 02:25:04.791041 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.791861 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.792507 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.793104 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.793709 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.794294 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.794889 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.795459 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.796077 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.796656 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.797178 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.798118 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.798907 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.799696 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.800492 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.801261 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.801845 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.802788 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.803575 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.804350 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.805246 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.806129 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.806963 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.807771 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.808571 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.809351 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.810151 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.810927 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.811735 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.812522 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.813215 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.814101 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.814895 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.815679 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.816468 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.817237 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.818019 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.818781 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.819567 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.820339 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.821133 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.821908 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.822706 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.823491 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.824095 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.825102 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.825903 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.826676 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.827256 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.828221 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.829021 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.829793 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.830581 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.831346 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.832135 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.832902 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.833690 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.834246 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.835233 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.836028 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.836812 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.837579 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.838367 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.839142 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.840080 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.840862 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.841643 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.842417 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.843198 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.843983 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.844775 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.845564 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.846347 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.847125 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.847921 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.848707 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.849495 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.850263 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.851054 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.851834 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.852621 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.853393 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.853965 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.854912 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.855702 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.856476 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.857244 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.857992 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.858762 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.859542 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.860322 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.861084 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.861856 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.862610 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.863377 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.864144 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.864755 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.865325 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.865911 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.866476 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.867064 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.867632 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.868209 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.868781 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.869359 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.869926 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.870514 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.871076 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.871675 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.872428 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.873221 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.873988 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.874767 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.875537 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.876329 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.877095 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.877880 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.878652 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.879449 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.880253 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.881043 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.881802 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.882585 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.883353 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.884148 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.884912 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.885697 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.886466 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.887245 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.888030 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.888819 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.889593 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.890380 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.891142 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.891934 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.892711 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.893487 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.894258 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.895065 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.895863 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.896674 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.897468 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.898266 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.899057 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.899861 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.900653 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.901472 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.902241 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.903033 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.903819 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.904638 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.905441 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.906251 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.907054 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.907879 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.908681 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.909494 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.910283 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.911094 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.911895 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.912698 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.913478 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.914280 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.915057 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.915858 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.916656 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.917453 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.918237 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.919036 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.919824 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.920638 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.921433 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.922245 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.923042 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.923857 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.924651 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.925458 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.926246 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.927034 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.927826 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.928642 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.929416 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.930221 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.931021 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.931849 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.932647 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.933456 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.934244 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.935057 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.935858 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.936696 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.937484 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.938291 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.939082 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.939924 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.940719 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.941519 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.942295 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.943084 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.943886 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.944689 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.945462 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.946270 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.947057 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.947887 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.948685 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.949478 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.950268 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.951078 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.951888 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.952702 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.953498 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.954312 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.955096 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.955928 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.956724 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.957525 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.958321 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.959124 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.959932 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.960741 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.961529 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.962334 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.963120 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.963938 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.964721 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.965520 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.966312 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.967128 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.967922 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.968721 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.969508 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.970318 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.971091 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.971919 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.972715 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.973518 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.974293 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.975086 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 02:25:04.975873 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.conv0.quantizer                : TensorQuantizer(8bit per-tensor amax=2.1255 calibrator=HistogramCalibrator quant)\n",
      "features.conv0.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.1544 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer1.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.5013 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer1.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0649 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer1.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.1359 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer1.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0350 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer2.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.4326 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer2.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0556 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer2.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.1222 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer2.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0295 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer3.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.2484 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer3.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0524 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer3.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.1054 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer3.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0293 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer4.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.2378 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer4.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0360 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer4.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.1363 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer4.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0259 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer5.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.1728 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer5.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0332 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer5.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0792 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer5.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0227 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer6.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.1625 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer6.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0406 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer6.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0919 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer6.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0247 calibrator=HistogramCalibrator quant)\n",
      "features.transition1.conv.quantizer     : TensorQuantizer(8bit per-tensor amax=0.2214 calibrator=HistogramCalibrator quant)\n",
      "features.transition1.conv.quantizer_w   : TensorQuantizer(8bit per-tensor amax=0.0519 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer1.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.1819 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer1.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0308 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer1.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0900 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer1.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0197 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer2.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.1544 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer2.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0267 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer2.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0785 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer2.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0230 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer3.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.1493 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer3.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0284 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer3.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0800 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer3.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0236 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer4.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.1174 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer4.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0266 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer4.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0826 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer4.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0222 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer5.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.1079 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer5.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0265 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer5.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0767 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer5.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0243 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer6.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.1121 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer6.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0289 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer6.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0804 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer6.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0233 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer7.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.1001 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer7.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0265 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer7.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0791 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer7.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0243 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer8.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0979 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer8.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0293 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer8.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0770 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer8.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0282 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer9.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0975 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer9.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0266 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer9.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0748 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer9.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0246 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer10.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0988 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer10.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0283 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer10.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0816 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer10.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0228 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer11.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0976 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer11.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0305 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer11.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0907 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer11.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0218 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer12.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0959 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer12.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0251 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer12.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0853 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer12.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0244 calibrator=HistogramCalibrator quant)\n",
      "features.transition2.conv.quantizer     : TensorQuantizer(8bit per-tensor amax=0.1691 calibrator=HistogramCalibrator quant)\n",
      "features.transition2.conv.quantizer_w   : TensorQuantizer(8bit per-tensor amax=0.0310 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer1.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0697 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer1.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0260 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer1.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0567 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer1.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0208 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer2.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0948 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer2.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0287 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer2.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0639 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer2.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0174 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer3.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0764 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer3.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0251 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer3.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0619 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer3.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0235 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer4.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0819 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer4.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0288 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer4.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0748 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer4.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0217 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer5.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0827 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer5.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0237 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer5.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0661 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer5.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0202 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer6.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0818 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer6.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0209 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer6.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0443 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer6.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0242 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer7.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0718 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer7.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0236 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer7.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.1051 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer7.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0146 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer8.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0706 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer8.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0249 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer8.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0753 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer8.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0200 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer9.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0720 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer9.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0181 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer9.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0773 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer9.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0135 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer10.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0707 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer10.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0194 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer10.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0731 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer10.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0137 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer11.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0685 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer11.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0220 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer11.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0655 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer11.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0175 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer12.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0714 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer12.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0202 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer12.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0809 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer12.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0133 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer13.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0648 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer13.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0203 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer13.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0901 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer13.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0139 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer14.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0710 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer14.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0191 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer14.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0852 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer14.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0110 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer15.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0655 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer15.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0187 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer15.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0777 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer15.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0125 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer16.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0597 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer16.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0191 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer16.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0874 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer16.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0150 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer17.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0544 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer17.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0214 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer17.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0925 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer17.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0173 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer18.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0654 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer18.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0196 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer18.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0948 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer18.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0235 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer19.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0609 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer19.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0197 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer19.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.1000 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer19.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0163 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer20.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0556 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer20.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0219 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer20.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0926 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer20.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0224 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer21.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0781 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer21.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0192 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer21.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0951 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer21.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0210 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer22.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0790 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer22.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0218 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer22.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0830 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer22.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0200 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer23.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0829 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer23.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0194 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer23.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.1491 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer23.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0256 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer24.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0767 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer24.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0184 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer24.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.1683 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer24.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0201 calibrator=HistogramCalibrator quant)\n",
      "features.transition3.conv.quantizer     : TensorQuantizer(8bit per-tensor amax=0.8241 calibrator=HistogramCalibrator quant)\n",
      "features.transition3.conv.quantizer_w   : TensorQuantizer(8bit per-tensor amax=0.0380 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer1.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0586 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer1.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0102 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer1.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.2483 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer1.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0348 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer2.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0564 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer2.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0118 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer2.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.2634 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer2.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0350 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer3.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0533 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer3.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0116 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer3.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.3207 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer3.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0352 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer4.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0619 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer4.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0117 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer4.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.2851 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer4.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0349 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer5.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0551 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer5.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0119 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer5.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.3229 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer5.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0382 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer6.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0483 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer6.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0104 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer6.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.2771 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer6.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0333 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer7.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0517 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer7.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0112 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer7.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.3399 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer7.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0326 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer8.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0540 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer8.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0100 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer8.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.2910 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer8.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0319 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer9.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0558 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer9.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0100 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer9.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.3524 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer9.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0332 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer10.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0525 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer10.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0108 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer10.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.4275 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer10.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0354 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer11.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0465 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer11.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0092 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer11.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.3225 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer11.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0376 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer12.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0449 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer12.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0095 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer12.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.2729 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer12.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0363 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer13.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0529 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer13.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0101 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer13.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.2946 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer13.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0405 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer14.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0546 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer14.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0102 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer14.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.3379 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer14.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0362 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer15.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0575 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer15.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0090 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer15.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.3186 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer15.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0319 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer16.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0630 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer16.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0108 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer16.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.3635 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer16.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0330 calibrator=HistogramCalibrator quant)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [01:51<00:00, 12.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111.849509883672\n",
      "Accuracy of the network on the 10000 test images: 11.4041 %\n",
      "Epoch 1/15\n",
      "Epoch: [0]  [ 0/40]  eta: 0:07:46  lr: 0.0001  img/s: 10.991093558620445  loss: 2.1893 (2.1893)  acc1: 24.2188 (24.2188)  acc5: 75.0000 (75.0000)  time: 11.6640  data: 0.0182\n",
      "Epoch: [0]  [ 1/40]  eta: 0:07:37  lr: 0.0001  img/s: 10.853583712329716  loss: 2.1668 (2.1780)  acc1: 24.2188 (25.0000)  acc5: 75.0000 (75.3906)  time: 11.7375  data: 0.0179\n",
      "Epoch: [0]  [ 2/40]  eta: 0:07:26  lr: 0.0001  img/s: 10.892789282781235  loss: 2.1668 (2.1420)  acc1: 25.7812 (29.1667)  acc5: 75.7812 (79.1667)  time: 11.7479  data: 0.0178\n",
      "Epoch: [0]  [ 3/40]  eta: 0:07:12  lr: 0.0001  img/s: 11.146370781514209  loss: 2.0698 (2.0950)  acc1: 25.7812 (32.2266)  acc5: 75.7812 (81.0547)  time: 11.6862  data: 0.0178\n",
      "Epoch: [0]  [ 4/40]  eta: 0:07:00  lr: 0.0001  img/s: 11.018980067023822  loss: 2.0698 (2.0169)  acc1: 37.5000 (37.5000)  acc5: 86.7188 (83.4375)  time: 11.6757  data: 0.0177\n",
      "Epoch: [0]  [ 5/40]  eta: 0:06:47  lr: 0.0001  img/s: 11.10193862675454  loss: 1.9541 (1.9273)  acc1: 37.5000 (42.0573)  acc5: 86.7188 (85.5469)  time: 11.6543  data: 0.0177\n",
      "Epoch: [0]  [ 6/40]  eta: 0:06:36  lr: 0.0001  img/s: 11.03870739499408  loss: 1.9541 (1.8146)  acc1: 41.4062 (46.7634)  acc5: 86.7188 (87.5000)  time: 11.6484  data: 0.0177\n",
      "Epoch: [0]  [ 7/40]  eta: 0:06:25  lr: 0.0001  img/s: 10.851786959047313  loss: 1.7043 (1.7132)  acc1: 41.4062 (50.1953)  acc5: 86.7188 (88.8672)  time: 11.6690  data: 0.0177\n",
      "Epoch: [0]  [ 8/40]  eta: 0:06:13  lr: 0.0001  img/s: 11.006416377024443  loss: 1.7043 (1.6025)  acc1: 58.5938 (54.0799)  acc5: 92.9688 (89.9306)  time: 11.6666  data: 0.0177\n",
      "Epoch: [0]  [ 9/40]  eta: 0:06:02  lr: 0.0001  img/s: 10.872475918408957  loss: 1.4798 (1.5151)  acc1: 58.5938 (56.9531)  acc5: 92.9688 (90.8594)  time: 11.6790  data: 0.0177\n",
      "Epoch: [0]  [10/40]  eta: 0:05:50  lr: 0.0001  img/s: 11.073897965239938  loss: 1.4798 (1.4340)  acc1: 64.8438 (59.3750)  acc5: 96.0938 (91.5483)  time: 11.6696  data: 0.0177\n",
      "Epoch: [0]  [11/40]  eta: 0:05:38  lr: 0.0001  img/s: 11.073540045113353  loss: 1.1385 (1.3706)  acc1: 64.8438 (61.1979)  acc5: 96.0938 (91.9271)  time: 11.6619  data: 0.0177\n",
      "Epoch: [0]  [12/40]  eta: 0:05:26  lr: 0.0001  img/s: 10.920801429687454  loss: 1.1385 (1.3014)  acc1: 74.2188 (63.4014)  acc5: 96.0938 (92.5481)  time: 11.6678  data: 0.0177\n",
      "Epoch: [0]  [13/40]  eta: 0:05:15  lr: 0.0001  img/s: 10.991867665566069  loss: 1.0032 (1.2498)  acc1: 74.2188 (64.8996)  acc5: 96.0938 (93.0246)  time: 11.6675  data: 0.0177\n",
      "Epoch: [0]  [14/40]  eta: 0:05:03  lr: 0.0001  img/s: 11.106357899670414  loss: 1.0032 (1.2036)  acc1: 75.0000 (66.4062)  acc5: 97.6562 (93.3333)  time: 11.6591  data: 0.0177\n",
      "Epoch: [0]  [15/40]  eta: 0:04:51  lr: 0.0001  img/s: 10.935968847034864  loss: 0.7285 (1.1527)  acc1: 75.0000 (67.9199)  acc5: 97.6562 (93.7500)  time: 11.6631  data: 0.0177\n",
      "Epoch: [0]  [16/40]  eta: 0:04:39  lr: 0.0001  img/s: 11.044190056596467  loss: 0.7285 (1.1125)  acc1: 81.2500 (69.2096)  acc5: 98.4375 (94.0717)  time: 11.6598  data: 0.0177\n",
      "Epoch: [0]  [17/40]  eta: 0:04:28  lr: 0.0001  img/s: 11.114669921239322  loss: 0.7168 (1.0709)  acc1: 81.2500 (70.4861)  acc5: 98.4375 (94.4010)  time: 11.6529  data: 0.0177\n",
      "Epoch: [0]  [18/40]  eta: 0:04:16  lr: 0.0001  img/s: 11.143984910114813  loss: 0.7168 (1.0400)  acc1: 82.8125 (71.3816)  acc5: 98.4375 (94.5724)  time: 11.6450  data: 0.0177\n",
      "Epoch: [0]  [19/40]  eta: 0:04:04  lr: 0.0001  img/s: 11.003833144648397  loss: 0.6723 (1.0064)  acc1: 82.8125 (72.3047)  acc5: 98.4375 (94.8438)  time: 11.6453  data: 0.0178\n",
      "Epoch: [0]  [20/40]  eta: 0:03:52  lr: 0.0001  img/s: 10.968439460556446  loss: 0.6233 (0.9793)  acc1: 83.5938 (73.0655)  acc5: 98.4375 (95.0149)  time: 11.6464  data: 0.0177\n",
      "Epoch: [0]  [21/40]  eta: 0:03:41  lr: 0.0001  img/s: 11.030443969530491  loss: 0.5785 (0.9561)  acc1: 84.3750 (73.6151)  acc5: 98.4375 (95.2060)  time: 11.6370  data: 0.0177\n",
      "Epoch: [0]  [22/40]  eta: 0:03:29  lr: 0.0001  img/s: 10.927108917040615  loss: 0.5576 (0.9321)  acc1: 85.1562 (74.3207)  acc5: 98.4375 (95.3804)  time: 11.6351  data: 0.0177\n",
      "Epoch: [0]  [23/40]  eta: 0:03:18  lr: 0.0001  img/s: 10.948311848715784  loss: 0.4845 (0.9055)  acc1: 85.1562 (75.0977)  acc5: 99.2188 (95.5729)  time: 11.6455  data: 0.0177\n",
      "Epoch: [0]  [24/40]  eta: 0:03:06  lr: 0.0001  img/s: 11.011349113485043  loss: 0.4714 (0.8871)  acc1: 87.5000 (75.5938)  acc5: 99.2188 (95.7188)  time: 11.6459  data: 0.0177\n",
      "Epoch: [0]  [25/40]  eta: 0:02:54  lr: 0.0001  img/s: 10.947564401991304  loss: 0.4689 (0.8691)  acc1: 87.5000 (76.0517)  acc5: 99.2188 (95.8834)  time: 11.6541  data: 0.0177\n",
      "Epoch: [0]  [26/40]  eta: 0:02:43  lr: 0.0001  img/s: 11.003345554902506  loss: 0.4681 (0.8467)  acc1: 87.5000 (76.7072)  acc5: 99.2188 (96.0359)  time: 11.6559  data: 0.0177\n",
      "Epoch: [0]  [27/40]  eta: 0:02:31  lr: 0.0001  img/s: 10.877333820020274  loss: 0.4457 (0.8294)  acc1: 87.5000 (77.1763)  acc5: 99.2188 (96.1496)  time: 11.6545  data: 0.0177\n",
      "Epoch: [0]  [28/40]  eta: 0:02:19  lr: 0.0001  img/s: 11.077239821535564  loss: 0.4374 (0.8158)  acc1: 87.5000 (77.5323)  acc5: 99.2188 (96.2554)  time: 11.6508  data: 0.0177\n",
      "Epoch: [0]  [29/40]  eta: 0:02:08  lr: 0.0001  img/s: 10.994828280107617  loss: 0.4374 (0.8077)  acc1: 87.5000 (77.7344)  acc5: 99.2188 (96.3542)  time: 11.6443  data: 0.0177\n",
      "Epoch: [0]  [30/40]  eta: 0:01:56  lr: 0.0001  img/s: 11.15018307623697  loss: 0.4331 (0.7950)  acc1: 88.2812 (78.1502)  acc5: 99.2188 (96.4214)  time: 11.6403  data: 0.0177\n",
      "Epoch: [0]  [31/40]  eta: 0:01:44  lr: 0.0001  img/s: 11.041530703555754  loss: 0.4331 (0.7889)  acc1: 88.2812 (78.2715)  acc5: 99.2188 (96.5088)  time: 11.6420  data: 0.0177\n",
      "Epoch: [0]  [32/40]  eta: 0:01:33  lr: 0.0001  img/s: 11.163783447963606  loss: 0.4200 (0.7776)  acc1: 87.5000 (78.5511)  acc5: 99.2188 (96.6146)  time: 11.6292  data: 0.0177\n",
      "Epoch: [0]  [33/40]  eta: 0:01:21  lr: 0.0001  img/s: 11.100862934670973  loss: 0.4200 (0.7688)  acc1: 87.5000 (78.6994)  acc5: 99.2188 (96.6912)  time: 11.6235  data: 0.0177\n",
      "Epoch: [0]  [34/40]  eta: 0:01:09  lr: 0.0001  img/s: 11.032063020432224  loss: 0.4200 (0.7593)  acc1: 87.5000 (78.9286)  acc5: 99.2188 (96.7857)  time: 11.6274  data: 0.0177\n",
      "Epoch: [0]  [35/40]  eta: 0:00:58  lr: 0.0001  img/s: 10.903182153100953  loss: 0.4331 (0.7518)  acc1: 87.5000 (79.1233)  acc5: 99.2188 (96.8316)  time: 11.6291  data: 0.0177\n",
      "Epoch: [0]  [36/40]  eta: 0:00:46  lr: 0.0001  img/s: 10.836103938904072  loss: 0.4200 (0.7410)  acc1: 87.5000 (79.4552)  acc5: 99.2188 (96.9172)  time: 11.6402  data: 0.0177\n",
      "Epoch: [0]  [37/40]  eta: 0:00:34  lr: 0.0001  img/s: 10.850465770716044  loss: 0.4331 (0.7334)  acc1: 87.5000 (79.6875)  acc5: 99.2188 (96.9572)  time: 11.6543  data: 0.0177\n",
      "Epoch: [0]  [38/40]  eta: 0:00:23  lr: 0.0001  img/s: 11.041097669198566  loss: 0.4331 (0.7274)  acc1: 87.5000 (79.8077)  acc5: 99.2188 (96.9952)  time: 11.6596  data: 0.0177\n",
      "Epoch: [0]  [39/40]  eta: 0:00:11  lr: 0.0001  img/s: 11.400326233901078  loss: 0.4353 (0.7408)  acc1: 87.5000 (79.7800)  acc5: 99.2188 (97.0000)  time: 11.1123  data: 0.0169\n",
      "Epoch: [0] Total time: 0:07:35\n",
      "Epoch 2/15\n",
      "Epoch: [1]  [ 0/40]  eta: 0:07:48  lr: 1e-05  img/s: 10.948001962957287  loss: 1.1465 (1.1465)  acc1: 68.7500 (68.7500)  acc5: 98.4375 (98.4375)  time: 11.7097  data: 0.0180\n",
      "Epoch: [1]  [ 1/40]  eta: 0:07:36  lr: 1e-05  img/s: 10.948216068031158  loss: 0.8890 (1.0178)  acc1: 68.7500 (72.6562)  acc5: 98.4375 (98.8281)  time: 11.7095  data: 0.0179\n",
      "Epoch: [1]  [ 2/40]  eta: 0:07:24  lr: 1e-05  img/s: 10.981726738030144  loss: 0.9293 (0.9883)  acc1: 76.5625 (73.9583)  acc5: 98.4375 (98.4375)  time: 11.6974  data: 0.0178\n",
      "Epoch: [1]  [ 3/40]  eta: 0:07:14  lr: 1e-05  img/s: 10.742732669851776  loss: 0.9293 (0.9819)  acc1: 72.6562 (73.6328)  acc5: 97.6562 (97.8516)  time: 11.7563  data: 0.0178\n",
      "Epoch: [1]  [ 4/40]  eta: 0:07:03  lr: 1e-05  img/s: 10.91143533837739  loss: 0.9293 (0.9535)  acc1: 72.6562 (73.2812)  acc5: 97.6562 (97.8125)  time: 11.7548  data: 0.0179\n",
      "Epoch: [1]  [ 5/40]  eta: 0:06:51  lr: 1e-05  img/s: 10.873448120553599  loss: 0.8890 (0.9331)  acc1: 72.6562 (73.9583)  acc5: 97.6562 (97.7865)  time: 11.7606  data: 0.0179\n",
      "Epoch: [1]  [ 6/40]  eta: 0:06:39  lr: 1e-05  img/s: 10.980712169570854  loss: 0.8890 (0.8946)  acc1: 76.5625 (74.8884)  acc5: 97.6562 (97.6562)  time: 11.7483  data: 0.0178\n",
      "Epoch: [1]  [ 7/40]  eta: 0:06:27  lr: 1e-05  img/s: 10.893738595601905  loss: 0.8398 (0.8779)  acc1: 76.5625 (75.0977)  acc5: 97.6562 (97.4609)  time: 11.7507  data: 0.0178\n",
      "Epoch: [1]  [ 8/40]  eta: 0:06:15  lr: 1e-05  img/s: 11.040405838435214  loss: 0.8398 (0.8534)  acc1: 76.5625 (75.6076)  acc5: 97.6562 (97.6562)  time: 11.7353  data: 0.0178\n",
      "Epoch: [1]  [ 9/40]  eta: 0:06:03  lr: 1e-05  img/s: 10.904685870854184  loss: 0.8315 (0.8321)  acc1: 76.5625 (76.4062)  acc5: 97.6562 (97.7344)  time: 11.7373  data: 0.0178\n",
      "Epoch: [1]  [10/40]  eta: 0:05:52  lr: 1e-05  img/s: 10.921230633268578  loss: 0.8315 (0.8283)  acc1: 76.5625 (76.2074)  acc5: 97.6562 (97.9403)  time: 11.7374  data: 0.0178\n",
      "Epoch: [1]  [11/40]  eta: 0:05:40  lr: 1e-05  img/s: 10.98786868660655  loss: 0.7907 (0.8089)  acc1: 76.5625 (76.5625)  acc5: 97.6562 (97.9818)  time: 11.7315  data: 0.0178\n",
      "Epoch: [1]  [12/40]  eta: 0:05:28  lr: 1e-05  img/s: 11.050212630865358  loss: 0.7907 (0.7891)  acc1: 76.5625 (76.9832)  acc5: 98.4375 (98.1370)  time: 11.7215  data: 0.0178\n",
      "Epoch: [1]  [13/40]  eta: 0:05:16  lr: 1e-05  img/s: 10.985194576567885  loss: 0.7610 (0.7704)  acc1: 76.5625 (77.5670)  acc5: 98.4375 (98.2143)  time: 11.7178  data: 0.0178\n",
      "Epoch: [1]  [14/40]  eta: 0:05:04  lr: 1e-05  img/s: 11.048320179515605  loss: 0.7610 (0.7607)  acc1: 77.3438 (77.8646)  acc5: 98.4375 (98.2292)  time: 11.7101  data: 0.0178\n",
      "Epoch: [1]  [15/40]  eta: 0:04:52  lr: 1e-05  img/s: 11.080527665607757  loss: 0.6632 (0.7422)  acc1: 77.3438 (78.3203)  acc5: 98.4375 (98.2910)  time: 11.7014  data: 0.0178\n",
      "Epoch: [1]  [16/40]  eta: 0:04:40  lr: 1e-05  img/s: 10.998592804481932  loss: 0.6632 (0.7256)  acc1: 79.6875 (78.8143)  acc5: 98.4375 (98.3456)  time: 11.6987  data: 0.0178\n",
      "Epoch: [1]  [17/40]  eta: 0:04:29  lr: 1e-05  img/s: 10.80934143604717  loss: 0.6575 (0.7128)  acc1: 79.6875 (79.1667)  acc5: 98.4375 (98.3941)  time: 11.7076  data: 0.0178\n",
      "Epoch: [1]  [18/40]  eta: 0:04:17  lr: 1e-05  img/s: 11.049682715528133  loss: 0.6575 (0.7094)  acc1: 80.4688 (79.2352)  acc5: 98.4375 (98.3964)  time: 11.7020  data: 0.0178\n",
      "Epoch: [1]  [19/40]  eta: 0:04:05  lr: 1e-05  img/s: 10.957437895065205  loss: 0.6487 (0.6940)  acc1: 80.4688 (79.6484)  acc5: 98.4375 (98.4766)  time: 11.7019  data: 0.0178\n",
      "Epoch: [1]  [20/40]  eta: 0:03:53  lr: 1e-05  img/s: 11.169471724226717  loss: 0.6401 (0.6874)  acc1: 80.4688 (79.8735)  acc5: 98.4375 (98.4747)  time: 11.6903  data: 0.0178\n",
      "Epoch: [1]  [21/40]  eta: 0:03:42  lr: 1e-05  img/s: 11.052281379114532  loss: 0.6242 (0.6791)  acc1: 80.4688 (79.9716)  acc5: 98.4375 (98.5085)  time: 11.6848  data: 0.0178\n",
      "Epoch: [1]  [22/40]  eta: 0:03:30  lr: 1e-05  img/s: 11.076853803356851  loss: 0.5957 (0.6728)  acc1: 82.0312 (80.2989)  acc5: 98.4375 (98.5734)  time: 11.6798  data: 0.0178\n",
      "Epoch: [1]  [23/40]  eta: 0:03:18  lr: 1e-05  img/s: 11.12485811100284  loss: 0.5568 (0.6642)  acc1: 82.0312 (80.6315)  acc5: 99.2188 (98.6003)  time: 11.6593  data: 0.0178\n",
      "Epoch: [1]  [24/40]  eta: 0:03:06  lr: 1e-05  img/s: 10.946022948013605  loss: 0.5516 (0.6542)  acc1: 82.0312 (81.0625)  acc5: 99.2188 (98.6250)  time: 11.6575  data: 0.0177\n",
      "Epoch: [1]  [25/40]  eta: 0:02:55  lr: 1e-05  img/s: 11.021044376179196  loss: 0.5341 (0.6480)  acc1: 83.5938 (81.2800)  acc5: 99.2188 (98.6178)  time: 11.6496  data: 0.0177\n",
      "Epoch: [1]  [26/40]  eta: 0:02:43  lr: 1e-05  img/s: 10.876297905481046  loss: 0.5271 (0.6377)  acc1: 84.3750 (81.6840)  acc5: 99.2188 (98.6690)  time: 11.6552  data: 0.0178\n",
      "Epoch: [1]  [27/40]  eta: 0:02:31  lr: 1e-05  img/s: 10.882811734143214  loss: 0.5037 (0.6310)  acc1: 85.1562 (81.9196)  acc5: 99.2188 (98.6886)  time: 11.6558  data: 0.0178\n",
      "Epoch: [1]  [28/40]  eta: 0:02:20  lr: 1e-05  img/s: 11.075178855572709  loss: 0.4952 (0.6260)  acc1: 85.1562 (82.0582)  acc5: 99.2188 (98.6800)  time: 11.6539  data: 0.0178\n",
      "Epoch: [1]  [29/40]  eta: 0:02:08  lr: 1e-05  img/s: 11.013441516667323  loss: 0.4952 (0.6231)  acc1: 85.1562 (82.1094)  acc5: 99.2188 (98.6719)  time: 11.6481  data: 0.0177\n",
      "Epoch: [1]  [30/40]  eta: 0:01:56  lr: 1e-05  img/s: 11.061626549803629  loss: 0.4942 (0.6155)  acc1: 85.1562 (82.3841)  acc5: 99.2188 (98.6391)  time: 11.6407  data: 0.0178\n",
      "Epoch: [1]  [31/40]  eta: 0:01:45  lr: 1e-05  img/s: 11.048949332872994  loss: 0.4853 (0.6102)  acc1: 85.9375 (82.5195)  acc5: 99.2188 (98.6572)  time: 11.6375  data: 0.0178\n",
      "Epoch: [1]  [32/40]  eta: 0:01:33  lr: 1e-05  img/s: 11.06546934920488  loss: 0.4674 (0.6017)  acc1: 86.7188 (82.7652)  acc5: 99.2188 (98.6979)  time: 11.6367  data: 0.0178\n",
      "Epoch: [1]  [33/40]  eta: 0:01:21  lr: 1e-05  img/s: 10.93559818060407  loss: 0.4648 (0.5960)  acc1: 86.7188 (82.9044)  acc5: 99.2188 (98.6673)  time: 11.6393  data: 0.0178\n",
      "Epoch: [1]  [34/40]  eta: 0:01:10  lr: 1e-05  img/s: 11.043707289326065  loss: 0.4609 (0.5910)  acc1: 86.7188 (83.0357)  acc5: 99.2188 (98.6830)  time: 11.6396  data: 0.0178\n",
      "Epoch: [1]  [35/40]  eta: 0:00:58  lr: 1e-05  img/s: 10.901019431505706  loss: 0.4609 (0.5885)  acc1: 87.5000 (83.1597)  acc5: 99.2188 (98.6979)  time: 11.6491  data: 0.0178\n",
      "Epoch: [1]  [36/40]  eta: 0:00:46  lr: 1e-05  img/s: 11.184623557235074  loss: 0.4516 (0.5826)  acc1: 87.5000 (83.4037)  acc5: 99.2188 (98.7331)  time: 11.6394  data: 0.0178\n",
      "Epoch: [1]  [37/40]  eta: 0:00:34  lr: 1e-05  img/s: 11.099384714684462  loss: 0.4455 (0.5767)  acc1: 87.5000 (83.6143)  acc5: 99.2188 (98.7253)  time: 11.6239  data: 0.0178\n",
      "Epoch: [1]  [38/40]  eta: 0:00:23  lr: 1e-05  img/s: 11.07222117099053  loss: 0.4193 (0.5697)  acc1: 87.5000 (83.8341)  acc5: 99.2188 (98.7580)  time: 11.6228  data: 0.0178\n",
      "Epoch: [1]  [39/40]  eta: 0:00:11  lr: 1e-05  img/s: 11.279596879107432  loss: 0.4455 (0.5885)  acc1: 87.5000 (83.8000)  acc5: 99.2188 (98.7600)  time: 11.0734  data: 0.0170\n",
      "Epoch: [1] Total time: 0:07:35\n",
      "Epoch 3/15\n",
      "Epoch: [2]  [ 0/40]  eta: 0:07:47  lr: 1.0000000000000002e-06  img/s: 10.958510797922624  loss: 0.4405 (0.4405)  acc1: 89.8438 (89.8438)  acc5: 99.2188 (99.2188)  time: 11.6986  data: 0.0181\n",
      "Epoch: [2]  [ 1/40]  eta: 0:07:37  lr: 1.0000000000000002e-06  img/s: 10.892709278410923  loss: 0.3790 (0.4098)  acc1: 89.0625 (89.4531)  acc5: 99.2188 (99.6094)  time: 11.7337  data: 0.0180\n",
      "Epoch: [2]  [ 2/40]  eta: 0:07:24  lr: 1.0000000000000002e-06  img/s: 11.063778690180824  loss: 0.3931 (0.4042)  acc1: 89.8438 (90.6250)  acc5: 99.2188 (99.4792)  time: 11.6848  data: 0.0179\n",
      "Epoch: [2]  [ 3/40]  eta: 0:07:11  lr: 1.0000000000000002e-06  img/s: 11.07321366588215  loss: 0.3931 (0.4362)  acc1: 89.0625 (88.6719)  acc5: 99.2188 (99.6094)  time: 11.6579  data: 0.0178\n",
      "Epoch: [2]  [ 4/40]  eta: 0:06:59  lr: 1.0000000000000002e-06  img/s: 11.05408938122139  loss: 0.4405 (0.4438)  acc1: 89.0625 (87.9688)  acc5: 99.2188 (99.5312)  time: 11.6457  data: 0.0178\n",
      "Epoch: [2]  [ 5/40]  eta: 0:06:47  lr: 1.0000000000000002e-06  img/s: 10.985619640217323  loss: 0.4405 (0.4862)  acc1: 85.1562 (86.0677)  acc5: 99.2188 (99.4792)  time: 11.6497  data: 0.0179\n",
      "Epoch: [2]  [ 6/40]  eta: 0:06:37  lr: 1.0000000000000002e-06  img/s: 10.752647429607501  loss: 0.4405 (0.4652)  acc1: 89.0625 (86.8304)  acc5: 99.2188 (99.5536)  time: 11.6885  data: 0.0178\n",
      "Epoch: [2]  [ 7/40]  eta: 0:06:25  lr: 1.0000000000000002e-06  img/s: 11.014908005455023  loss: 0.4114 (0.4585)  acc1: 89.0625 (87.1094)  acc5: 99.2188 (99.4141)  time: 11.6823  data: 0.0178\n",
      "Epoch: [2]  [ 8/40]  eta: 0:06:14  lr: 1.0000000000000002e-06  img/s: 10.885266722137429  loss: 0.4114 (0.4531)  acc1: 89.0625 (87.2396)  acc5: 99.2188 (99.3924)  time: 11.6928  data: 0.0178\n",
      "Epoch: [2]  [ 9/40]  eta: 0:06:02  lr: 1.0000000000000002e-06  img/s: 11.061064318289285  loss: 0.4114 (0.4569)  acc1: 88.2812 (87.1875)  acc5: 99.2188 (99.4531)  time: 11.6825  data: 0.0178\n",
      "Epoch: [2]  [10/40]  eta: 0:05:50  lr: 1.0000000000000002e-06  img/s: 11.099556820653365  loss: 0.4405 (0.4558)  acc1: 89.0625 (87.3580)  acc5: 99.2188 (99.4318)  time: 11.6704  data: 0.0177\n",
      "Epoch: [2]  [11/40]  eta: 0:05:38  lr: 1.0000000000000002e-06  img/s: 10.939035382351411  loss: 0.4114 (0.4468)  acc1: 89.0625 (87.7604)  acc5: 99.2188 (99.4141)  time: 11.6744  data: 0.0178\n",
      "Epoch: [2]  [12/40]  eta: 0:05:26  lr: 1.0000000000000002e-06  img/s: 10.988897398746735  loss: 0.4114 (0.4420)  acc1: 89.0625 (87.9207)  acc5: 99.2188 (99.4591)  time: 11.6738  data: 0.0177\n",
      "Epoch: [2]  [13/40]  eta: 0:05:15  lr: 1.0000000000000002e-06  img/s: 10.93338717844778  loss: 0.4099 (0.4396)  acc1: 89.0625 (88.0022)  acc5: 99.2188 (99.4420)  time: 11.6774  data: 0.0178\n",
      "Epoch: [2]  [14/40]  eta: 0:05:03  lr: 1.0000000000000002e-06  img/s: 10.8037991588679  loss: 0.4114 (0.4400)  acc1: 89.0625 (88.1250)  acc5: 99.2188 (99.3750)  time: 11.6900  data: 0.0177\n",
      "Epoch: [2]  [15/40]  eta: 0:04:52  lr: 1.0000000000000002e-06  img/s: 11.03168150448973  loss: 0.4099 (0.4331)  acc1: 89.0625 (88.3301)  acc5: 99.2188 (99.4141)  time: 11.6856  data: 0.0177\n",
      "Epoch: [2]  [16/40]  eta: 0:04:40  lr: 1.0000000000000002e-06  img/s: 11.088777922581595  loss: 0.4099 (0.4296)  acc1: 89.0625 (88.3732)  acc5: 99.2188 (99.4026)  time: 11.6783  data: 0.0177\n",
      "Epoch: [2]  [17/40]  eta: 0:04:28  lr: 1.0000000000000002e-06  img/s: 11.129501219170347  loss: 0.4090 (0.4230)  acc1: 89.0625 (88.4983)  acc5: 99.2188 (99.4358)  time: 11.6694  data: 0.0178\n",
      "Epoch: [2]  [18/40]  eta: 0:04:16  lr: 1.0000000000000002e-06  img/s: 11.062949512054013  loss: 0.4099 (0.4250)  acc1: 89.0625 (88.5280)  acc5: 99.2188 (99.4243)  time: 11.6651  data: 0.0177\n",
      "Epoch: [2]  [19/40]  eta: 0:04:04  lr: 1.0000000000000002e-06  img/s: 11.205879120147435  loss: 0.4090 (0.4221)  acc1: 89.0625 (88.6719)  acc5: 99.2188 (99.4531)  time: 11.6540  data: 0.0178\n",
      "Epoch: [2]  [20/40]  eta: 0:03:53  lr: 1.0000000000000002e-06  img/s: 10.771851108379  loss: 0.4090 (0.4256)  acc1: 89.0625 (88.6905)  acc5: 99.2188 (99.3676)  time: 11.6641  data: 0.0178\n",
      "Epoch: [2]  [21/40]  eta: 0:03:41  lr: 1.0000000000000002e-06  img/s: 11.015480019341842  loss: 0.4090 (0.4213)  acc1: 89.0625 (88.9560)  acc5: 99.2188 (99.3963)  time: 11.6575  data: 0.0178\n",
      "Epoch: [2]  [22/40]  eta: 0:03:29  lr: 1.0000000000000002e-06  img/s: 11.115198033913273  loss: 0.4099 (0.4231)  acc1: 89.0625 (88.9266)  acc5: 99.2188 (99.3207)  time: 11.6548  data: 0.0178\n",
      "Epoch: [2]  [23/40]  eta: 0:03:18  lr: 1.0000000000000002e-06  img/s: 10.868949484321485  loss: 0.4090 (0.4205)  acc1: 89.0625 (89.0299)  acc5: 99.2188 (99.3490)  time: 11.6657  data: 0.0178\n",
      "Epoch: [2]  [24/40]  eta: 0:03:06  lr: 1.0000000000000002e-06  img/s: 10.760558606071184  loss: 0.3836 (0.4180)  acc1: 89.0625 (89.1562)  acc5: 99.2188 (99.3750)  time: 11.6815  data: 0.0178\n",
      "Epoch: [2]  [25/40]  eta: 0:02:55  lr: 1.0000000000000002e-06  img/s: 11.114001280632516  loss: 0.3836 (0.4197)  acc1: 89.0625 (89.0024)  acc5: 99.2188 (99.3990)  time: 11.6748  data: 0.0178\n",
      "Epoch: [2]  [26/40]  eta: 0:02:43  lr: 1.0000000000000002e-06  img/s: 10.854882615441785  loss: 0.3836 (0.4154)  acc1: 89.0625 (89.1493)  acc5: 99.2188 (99.3924)  time: 11.6692  data: 0.0178\n",
      "Epoch: [2]  [27/40]  eta: 0:02:31  lr: 1.0000000000000002e-06  img/s: 10.964505875508598  loss: 0.3737 (0.4128)  acc1: 89.8438 (89.2299)  acc5: 99.2188 (99.4141)  time: 11.6718  data: 0.0178\n",
      "Epoch: [2]  [28/40]  eta: 0:02:20  lr: 1.0000000000000002e-06  img/s: 11.019405713767775  loss: 0.3686 (0.4100)  acc1: 89.8438 (89.3050)  acc5: 99.2188 (99.4073)  time: 11.6647  data: 0.0178\n",
      "Epoch: [2]  [29/40]  eta: 0:02:08  lr: 1.0000000000000002e-06  img/s: 10.872242087342189  loss: 0.3686 (0.4118)  acc1: 89.8438 (89.2188)  acc5: 99.2188 (99.4010)  time: 11.6747  data: 0.0178\n",
      "Epoch: [2]  [30/40]  eta: 0:01:56  lr: 1.0000000000000002e-06  img/s: 11.002887323079142  loss: 0.3597 (0.4101)  acc1: 90.6250 (89.3145)  acc5: 99.2188 (99.3448)  time: 11.6798  data: 0.0178\n",
      "Epoch: [2]  [31/40]  eta: 0:01:45  lr: 1.0000000000000002e-06  img/s: 10.840587679320587  loss: 0.3686 (0.4091)  acc1: 90.6250 (89.3799)  acc5: 99.2188 (99.3408)  time: 11.6851  data: 0.0178\n",
      "Epoch: [2]  [32/40]  eta: 0:01:33  lr: 1.0000000000000002e-06  img/s: 11.040276200767062  loss: 0.3686 (0.4094)  acc1: 90.6250 (89.3703)  acc5: 99.2188 (99.3608)  time: 11.6824  data: 0.0178\n",
      "Epoch: [2]  [33/40]  eta: 0:01:21  lr: 1.0000000000000002e-06  img/s: 10.80290045321317  loss: 0.3686 (0.4101)  acc1: 90.6250 (89.3842)  acc5: 99.2188 (99.3566)  time: 11.6895  data: 0.0178\n",
      "Epoch: [2]  [34/40]  eta: 0:01:10  lr: 1.0000000000000002e-06  img/s: 10.853756398710862  loss: 0.3597 (0.4073)  acc1: 91.4062 (89.4643)  acc5: 99.2188 (99.3750)  time: 11.6868  data: 0.0178\n",
      "Epoch: [2]  [35/40]  eta: 0:00:58  lr: 1.0000000000000002e-06  img/s: 11.001292153225464  loss: 0.3686 (0.4068)  acc1: 90.6250 (89.4531)  acc5: 99.2188 (99.3707)  time: 11.6884  data: 0.0178\n",
      "Epoch: [2]  [36/40]  eta: 0:00:46  lr: 1.0000000000000002e-06  img/s: 10.88604696323563  loss: 0.3597 (0.4046)  acc1: 91.4062 (89.5059)  acc5: 99.2188 (99.3877)  time: 11.6991  data: 0.0178\n",
      "Epoch: [2]  [37/40]  eta: 0:00:35  lr: 1.0000000000000002e-06  img/s: 10.958637851406007  loss: 0.3686 (0.4046)  acc1: 91.4062 (89.5559)  acc5: 99.2188 (99.3627)  time: 11.7081  data: 0.0178\n",
      "Epoch: [2]  [38/40]  eta: 0:00:23  lr: 1.0000000000000002e-06  img/s: 10.981514465007018  loss: 0.3597 (0.4027)  acc1: 91.4062 (89.6234)  acc5: 99.2188 (99.3590)  time: 11.7124  data: 0.0178\n",
      "Epoch: [2]  [39/40]  eta: 0:00:11  lr: 1.0000000000000002e-06  img/s: 10.141644494509576  loss: 0.3597 (0.4212)  acc1: 91.4062 (89.5800)  acc5: 99.2188 (99.3400)  time: 11.1798  data: 0.0169\n",
      "Epoch: [2] Total time: 0:07:36\n",
      "Epoch 4/15\n",
      "Epoch: [3]  [ 0/40]  eta: 0:07:54  lr: 1.0000000000000002e-07  img/s: 10.814604233334153  loss: 0.4603 (0.4603)  acc1: 86.7188 (86.7188)  acc5: 100.0000 (100.0000)  time: 11.8543  data: 0.0184\n",
      "Epoch: [3]  [ 1/40]  eta: 0:07:44  lr: 1.0000000000000002e-07  img/s: 10.725596487551421  loss: 0.3536 (0.4069)  acc1: 86.7188 (89.0625)  acc5: 100.0000 (100.0000)  time: 11.9032  data: 0.0182\n",
      "Epoch: [3]  [ 2/40]  eta: 0:07:31  lr: 1.0000000000000002e-07  img/s: 10.814893323900156  loss: 0.4068 (0.4069)  acc1: 89.0625 (89.0625)  acc5: 100.0000 (99.7396)  time: 11.8866  data: 0.0181\n",
      "Epoch: [3]  [ 3/40]  eta: 0:07:20  lr: 1.0000000000000002e-07  img/s: 10.735675839811199  loss: 0.4068 (0.4409)  acc1: 86.7188 (88.2812)  acc5: 99.2188 (99.4141)  time: 11.9001  data: 0.0181\n",
      "Epoch: [3]  [ 4/40]  eta: 0:07:06  lr: 1.0000000000000002e-07  img/s: 10.97987922705561  loss: 0.4111 (0.4349)  acc1: 89.0625 (88.5938)  acc5: 100.0000 (99.5312)  time: 11.8552  data: 0.0180\n",
      "Epoch: [3]  [ 5/40]  eta: 0:06:53  lr: 1.0000000000000002e-07  img/s: 10.985825327986294  loss: 0.4111 (0.4500)  acc1: 86.7188 (87.7604)  acc5: 100.0000 (99.6094)  time: 11.8242  data: 0.0179\n",
      "Epoch: [3]  [ 6/40]  eta: 0:06:41  lr: 1.0000000000000002e-07  img/s: 10.866553976818638  loss: 0.4111 (0.4359)  acc1: 89.0625 (88.1696)  acc5: 100.0000 (99.6652)  time: 11.8203  data: 0.0179\n",
      "Epoch: [3]  [ 7/40]  eta: 0:06:30  lr: 1.0000000000000002e-07  img/s: 10.854001065443933  loss: 0.4111 (0.4431)  acc1: 86.7188 (87.8906)  acc5: 100.0000 (99.6094)  time: 11.8191  data: 0.0179\n",
      "Epoch: [3]  [ 8/40]  eta: 0:06:18  lr: 1.0000000000000002e-07  img/s: 10.809319454964141  loss: 0.4111 (0.4377)  acc1: 88.2812 (87.9340)  acc5: 100.0000 (99.6528)  time: 11.8243  data: 0.0186\n",
      "Epoch: [3]  [ 9/40]  eta: 0:06:06  lr: 1.0000000000000002e-07  img/s: 10.90458642221954  loss: 0.4111 (0.4424)  acc1: 86.7188 (87.5781)  acc5: 100.0000 (99.6875)  time: 11.8175  data: 0.0185\n",
      "Epoch: [3]  [10/40]  eta: 0:05:54  lr: 1.0000000000000002e-07  img/s: 10.942832495618095  loss: 0.4603 (0.4442)  acc1: 86.7188 (87.5000)  acc5: 100.0000 (99.7159)  time: 11.8082  data: 0.0185\n",
      "Epoch: [3]  [11/40]  eta: 0:05:42  lr: 1.0000000000000002e-07  img/s: 10.850542743480707  loss: 0.4153 (0.4418)  acc1: 86.7188 (87.5000)  acc5: 100.0000 (99.7396)  time: 11.8087  data: 0.0184\n",
      "Epoch: [3]  [12/40]  eta: 0:05:30  lr: 1.0000000000000002e-07  img/s: 11.009001274049764  loss: 0.4153 (0.4335)  acc1: 87.5000 (87.8606)  acc5: 100.0000 (99.7596)  time: 11.7961  data: 0.0184\n",
      "Epoch: [3]  [13/40]  eta: 0:05:18  lr: 1.0000000000000002e-07  img/s: 10.870933963850616  loss: 0.4111 (0.4285)  acc1: 87.5000 (88.1138)  acc5: 100.0000 (99.6652)  time: 11.7958  data: 0.0183\n",
      "Epoch: [3]  [14/40]  eta: 0:05:06  lr: 1.0000000000000002e-07  img/s: 10.952073849496154  loss: 0.4153 (0.4307)  acc1: 87.5000 (87.9167)  acc5: 100.0000 (99.6354)  time: 11.7898  data: 0.0183\n",
      "Epoch: [3]  [15/40]  eta: 0:04:54  lr: 1.0000000000000002e-07  img/s: 10.871445111627482  loss: 0.4111 (0.4274)  acc1: 87.5000 (87.8906)  acc5: 100.0000 (99.6094)  time: 11.7899  data: 0.0183\n",
      "Epoch: [3]  [16/40]  eta: 0:04:42  lr: 1.0000000000000002e-07  img/s: 10.89134562775709  loss: 0.4111 (0.4244)  acc1: 87.5000 (87.9596)  acc5: 100.0000 (99.6324)  time: 11.7887  data: 0.0182\n",
      "Epoch: [3]  [17/40]  eta: 0:04:31  lr: 1.0000000000000002e-07  img/s: 10.925474944060126  loss: 0.4068 (0.4172)  acc1: 87.5000 (88.2378)  acc5: 100.0000 (99.6528)  time: 11.7857  data: 0.0182\n",
      "Epoch: [3]  [18/40]  eta: 0:04:19  lr: 1.0000000000000002e-07  img/s: 10.880840337422013  loss: 0.4111 (0.4199)  acc1: 88.2812 (88.2401)  acc5: 100.0000 (99.5477)  time: 11.7855  data: 0.0182\n",
      "Epoch: [3]  [19/40]  eta: 0:04:07  lr: 1.0000000000000002e-07  img/s: 11.007872644417777  loss: 0.4068 (0.4145)  acc1: 88.2812 (88.4375)  acc5: 100.0000 (99.5703)  time: 11.7785  data: 0.0182\n",
      "Epoch: [3]  [20/40]  eta: 0:03:55  lr: 1.0000000000000002e-07  img/s: 11.065115850661359  loss: 0.4068 (0.4149)  acc1: 88.2812 (88.5045)  acc5: 100.0000 (99.5164)  time: 11.7651  data: 0.0182\n",
      "Epoch: [3]  [21/40]  eta: 0:03:43  lr: 1.0000000000000002e-07  img/s: 11.068173105545727  loss: 0.4068 (0.4107)  acc1: 88.2812 (88.7074)  acc5: 100.0000 (99.5384)  time: 11.7466  data: 0.0182\n",
      "Epoch: [3]  [22/40]  eta: 0:03:31  lr: 1.0000000000000002e-07  img/s: 11.049446885431774  loss: 0.4111 (0.4129)  acc1: 88.2812 (88.7568)  acc5: 100.0000 (99.5584)  time: 11.7340  data: 0.0182\n",
      "Epoch: [3]  [23/40]  eta: 0:03:19  lr: 1.0000000000000002e-07  img/s: 10.848882260071719  loss: 0.3940 (0.4108)  acc1: 89.0625 (88.8021)  acc5: 100.0000 (99.5768)  time: 11.7278  data: 0.0182\n",
      "Epoch: [3]  [24/40]  eta: 0:03:08  lr: 1.0000000000000002e-07  img/s: 10.97170988843114  loss: 0.3783 (0.4091)  acc1: 89.0625 (88.8750)  acc5: 100.0000 (99.5938)  time: 11.7282  data: 0.0182\n",
      "Epoch: [3]  [25/40]  eta: 0:02:56  lr: 1.0000000000000002e-07  img/s: 10.781164323606037  loss: 0.3783 (0.4116)  acc1: 89.0625 (88.8221)  acc5: 100.0000 (99.6094)  time: 11.7393  data: 0.0182\n",
      "Epoch: [3]  [26/40]  eta: 0:02:44  lr: 1.0000000000000002e-07  img/s: 10.905523171815659  loss: 0.3783 (0.4102)  acc1: 89.0625 (88.9178)  acc5: 100.0000 (99.5949)  time: 11.7372  data: 0.0182\n",
      "Epoch: [3]  [27/40]  eta: 0:02:32  lr: 1.0000000000000002e-07  img/s: 10.69992015581554  loss: 0.3765 (0.4066)  acc1: 89.8438 (89.0067)  acc5: 100.0000 (99.6094)  time: 11.7457  data: 0.0182\n",
      "Epoch: [3]  [28/40]  eta: 0:02:21  lr: 1.0000000000000002e-07  img/s: 10.96718717222671  loss: 0.3752 (0.4052)  acc1: 89.8438 (88.9278)  acc5: 100.0000 (99.5690)  time: 11.7369  data: 0.0179\n",
      "Epoch: [3]  [29/40]  eta: 0:02:09  lr: 1.0000000000000002e-07  img/s: 10.718383890783713  loss: 0.3752 (0.4084)  acc1: 89.8438 (88.8802)  acc5: 100.0000 (99.5312)  time: 11.7470  data: 0.0178\n",
      "Epoch: [3]  [30/40]  eta: 0:01:57  lr: 1.0000000000000002e-07  img/s: 10.840831314895679  loss: 0.3752 (0.4092)  acc1: 89.8438 (88.9869)  acc5: 100.0000 (99.4708)  time: 11.7525  data: 0.0178\n",
      "Epoch: [3]  [31/40]  eta: 0:01:46  lr: 1.0000000000000002e-07  img/s: 10.720786868914516  loss: 0.3698 (0.4080)  acc1: 89.8438 (88.9648)  acc5: 100.0000 (99.4873)  time: 11.7597  data: 0.0178\n",
      "Epoch: [3]  [32/40]  eta: 0:01:34  lr: 1.0000000000000002e-07  img/s: 10.78384159970256  loss: 0.3698 (0.4045)  acc1: 89.8438 (89.1335)  acc5: 100.0000 (99.5028)  time: 11.7718  data: 0.0178\n",
      "Epoch: [3]  [33/40]  eta: 0:01:22  lr: 1.0000000000000002e-07  img/s: 10.96651129287078  loss: 0.3698 (0.4028)  acc1: 89.8438 (89.2923)  acc5: 100.0000 (99.5175)  time: 11.7667  data: 0.0178\n",
      "Epoch: [3]  [34/40]  eta: 0:01:10  lr: 1.0000000000000002e-07  img/s: 10.872837914021835  loss: 0.3698 (0.4022)  acc1: 89.8438 (89.3527)  acc5: 100.0000 (99.5312)  time: 11.7709  data: 0.0178\n",
      "Epoch: [3]  [35/40]  eta: 0:00:58  lr: 1.0000000000000002e-07  img/s: 10.865471516077287  loss: 0.3698 (0.4024)  acc1: 89.8438 (89.3446)  acc5: 100.0000 (99.5443)  time: 11.7712  data: 0.0178\n",
      "Epoch: [3]  [36/40]  eta: 0:00:47  lr: 1.0000000000000002e-07  img/s: 11.071326342518997  loss: 0.3698 (0.4023)  acc1: 89.8438 (89.3159)  acc5: 100.0000 (99.5566)  time: 11.7617  data: 0.0178\n",
      "Epoch: [3]  [37/40]  eta: 0:00:35  lr: 1.0000000000000002e-07  img/s: 10.988438120422941  loss: 0.3752 (0.4021)  acc1: 89.8438 (89.3914)  acc5: 100.0000 (99.5271)  time: 11.7583  data: 0.0178\n",
      "Epoch: [3]  [38/40]  eta: 0:00:23  lr: 1.0000000000000002e-07  img/s: 11.071859934228888  loss: 0.3698 (0.4002)  acc1: 89.8438 (89.4030)  acc5: 100.0000 (99.4992)  time: 11.7482  data: 0.0178\n",
      "Epoch: [3]  [39/40]  eta: 0:00:11  lr: 1.0000000000000002e-07  img/s: 10.80926439383177  loss: 0.3752 (0.4153)  acc1: 89.8438 (89.4000)  acc5: 100.0000 (99.5000)  time: 11.2030  data: 0.0170\n",
      "Epoch: [3] Total time: 0:07:39\n",
      "Epoch 5/15\n",
      "Epoch: [4]  [ 0/40]  eta: 0:07:44  lr: 1.0000000000000004e-08  img/s: 11.036341976983385  loss: 0.4543 (0.4543)  acc1: 88.2812 (88.2812)  acc5: 99.2188 (99.2188)  time: 11.6155  data: 0.0175\n",
      "Epoch: [4]  [ 1/40]  eta: 0:07:33  lr: 1.0000000000000004e-08  img/s: 10.993719213512298  loss: 0.3946 (0.4245)  acc1: 88.2812 (88.2812)  acc5: 99.2188 (99.6094)  time: 11.6384  data: 0.0179\n",
      "Epoch: [4]  [ 2/40]  eta: 0:07:21  lr: 1.0000000000000004e-08  img/s: 11.106213842340066  loss: 0.3946 (0.4117)  acc1: 88.2812 (89.5833)  acc5: 100.0000 (99.7396)  time: 11.6066  data: 0.0179\n",
      "Epoch: [4]  [ 3/40]  eta: 0:07:08  lr: 1.0000000000000004e-08  img/s: 11.139205136370949  loss: 0.3946 (0.4285)  acc1: 88.2812 (89.0625)  acc5: 99.2188 (99.6094)  time: 11.5822  data: 0.0179\n",
      "Epoch: [4]  [ 4/40]  eta: 0:06:57  lr: 1.0000000000000004e-08  img/s: 11.05046623494169  loss: 0.3946 (0.4185)  acc1: 88.2812 (88.7500)  acc5: 100.0000 (99.6875)  time: 11.5859  data: 0.0179\n",
      "Epoch: [4]  [ 5/40]  eta: 0:06:45  lr: 1.0000000000000004e-08  img/s: 11.108668382116768  loss: 0.3946 (0.4513)  acc1: 87.5000 (87.8906)  acc5: 100.0000 (99.7396)  time: 11.5783  data: 0.0178\n",
      "Epoch: [4]  [ 6/40]  eta: 0:06:33  lr: 1.0000000000000004e-08  img/s: 11.177021912376103  loss: 0.3946 (0.4376)  acc1: 88.2812 (88.6161)  acc5: 100.0000 (99.7768)  time: 11.5629  data: 0.0179\n",
      "Epoch: [4]  [ 7/40]  eta: 0:06:21  lr: 1.0000000000000004e-08  img/s: 11.150704610471118  loss: 0.3946 (0.4359)  acc1: 88.2812 (88.5742)  acc5: 100.0000 (99.7070)  time: 11.5546  data: 0.0179\n",
      "Epoch: [4]  [ 8/40]  eta: 0:06:09  lr: 1.0000000000000004e-08  img/s: 11.044923943688861  loss: 0.3946 (0.4229)  acc1: 88.2812 (89.0625)  acc5: 100.0000 (99.6528)  time: 11.5604  data: 0.0179\n",
      "Epoch: [4]  [ 9/40]  eta: 0:05:57  lr: 1.0000000000000004e-08  img/s: 11.220164469897608  loss: 0.3946 (0.4214)  acc1: 88.2812 (88.5938)  acc5: 99.2188 (99.6094)  time: 11.5470  data: 0.0178\n",
      "Epoch: [4]  [10/40]  eta: 0:05:46  lr: 1.0000000000000004e-08  img/s: 11.167692223110421  loss: 0.4077 (0.4221)  acc1: 88.2812 (88.2812)  acc5: 99.2188 (99.5739)  time: 11.5408  data: 0.0179\n",
      "Epoch: [4]  [11/40]  eta: 0:05:34  lr: 1.0000000000000004e-08  img/s: 11.077850100540703  loss: 0.3946 (0.4135)  acc1: 88.2812 (88.5417)  acc5: 99.2188 (99.6094)  time: 11.5435  data: 0.0178\n",
      "Epoch: [4]  [12/40]  eta: 0:05:23  lr: 1.0000000000000004e-08  img/s: 11.178781344370325  loss: 0.3946 (0.4074)  acc1: 88.2812 (88.7620)  acc5: 100.0000 (99.6394)  time: 11.5377  data: 0.0178\n",
      "Epoch: [4]  [13/40]  eta: 0:05:11  lr: 1.0000000000000004e-08  img/s: 11.110830819981315  loss: 0.3946 (0.4117)  acc1: 88.2812 (88.6161)  acc5: 99.2188 (99.4978)  time: 11.5377  data: 0.0178\n",
      "Epoch: [4]  [14/40]  eta: 0:04:59  lr: 1.0000000000000004e-08  img/s: 11.107038259189302  loss: 0.4041 (0.4111)  acc1: 88.2812 (88.6979)  acc5: 99.2188 (99.4792)  time: 11.5380  data: 0.0178\n",
      "Epoch: [4]  [15/40]  eta: 0:04:48  lr: 1.0000000000000004e-08  img/s: 10.968844851860778  loss: 0.3946 (0.4046)  acc1: 88.2812 (88.9160)  acc5: 99.2188 (99.5117)  time: 11.5473  data: 0.0178\n",
      "Epoch: [4]  [16/40]  eta: 0:04:37  lr: 1.0000000000000004e-08  img/s: 11.093335026041926  loss: 0.3946 (0.3983)  acc1: 88.2812 (89.2463)  acc5: 100.0000 (99.5404)  time: 11.5482  data: 0.0182\n",
      "Epoch: [4]  [17/40]  eta: 0:04:25  lr: 1.0000000000000004e-08  img/s: 10.980275134094851  loss: 0.3863 (0.3922)  acc1: 88.2812 (89.3229)  acc5: 100.0000 (99.5660)  time: 11.5553  data: 0.0182\n",
      "Epoch: [4]  [18/40]  eta: 0:04:14  lr: 1.0000000000000004e-08  img/s: 11.088608211946529  loss: 0.3946 (0.3950)  acc1: 89.0625 (89.3092)  acc5: 100.0000 (99.5066)  time: 11.5556  data: 0.0182\n",
      "Epoch: [4]  [19/40]  eta: 0:04:02  lr: 1.0000000000000004e-08  img/s: 11.199456288984575  loss: 0.3863 (0.3874)  acc1: 89.0625 (89.6094)  acc5: 100.0000 (99.5312)  time: 11.5502  data: 0.0182\n",
      "Epoch: [4]  [20/40]  eta: 0:03:50  lr: 1.0000000000000004e-08  img/s: 11.173207755962423  loss: 0.3798 (0.3870)  acc1: 89.8438 (89.6577)  acc5: 100.0000 (99.5164)  time: 11.5431  data: 0.0182\n",
      "Epoch: [4]  [21/40]  eta: 0:03:39  lr: 1.0000000000000004e-08  img/s: 11.027944814398507  loss: 0.3788 (0.3824)  acc1: 90.6250 (89.8082)  acc5: 100.0000 (99.5384)  time: 11.5412  data: 0.0182\n",
      "Epoch: [4]  [22/40]  eta: 0:03:27  lr: 1.0000000000000004e-08  img/s: 11.05824445131946  loss: 0.3788 (0.3876)  acc1: 89.8438 (89.7079)  acc5: 99.2188 (99.5245)  time: 11.5437  data: 0.0182\n",
      "Epoch: [4]  [23/40]  eta: 0:03:16  lr: 1.0000000000000004e-08  img/s: 10.89358342303255  loss: 0.3549 (0.3851)  acc1: 90.6250 (89.8763)  acc5: 100.0000 (99.5443)  time: 11.5567  data: 0.0182\n",
      "Epoch: [4]  [24/40]  eta: 0:03:05  lr: 1.0000000000000004e-08  img/s: 10.860102427905515  loss: 0.3549 (0.3845)  acc1: 90.6250 (89.8750)  acc5: 100.0000 (99.5625)  time: 11.5668  data: 0.0182\n",
      "Epoch: [4]  [25/40]  eta: 0:02:53  lr: 1.0000000000000004e-08  img/s: 10.92336871178712  loss: 0.3549 (0.3858)  acc1: 90.6250 (89.8438)  acc5: 100.0000 (99.5793)  time: 11.5766  data: 0.0182\n",
      "Epoch: [4]  [26/40]  eta: 0:02:42  lr: 1.0000000000000004e-08  img/s: 11.113939160503728  loss: 0.3347 (0.3822)  acc1: 90.6250 (89.8727)  acc5: 99.2188 (99.5660)  time: 11.5799  data: 0.0182\n",
      "Epoch: [4]  [27/40]  eta: 0:02:30  lr: 1.0000000000000004e-08  img/s: 10.889462799922429  loss: 0.3347 (0.3824)  acc1: 90.6250 (89.8438)  acc5: 100.0000 (99.5815)  time: 11.5936  data: 0.0181\n",
      "Epoch: [4]  [28/40]  eta: 0:02:19  lr: 1.0000000000000004e-08  img/s: 11.030116499873257  loss: 0.3704 (0.3828)  acc1: 90.6250 (89.8976)  acc5: 100.0000 (99.5959)  time: 11.5944  data: 0.0181\n",
      "Epoch: [4]  [29/40]  eta: 0:02:07  lr: 1.0000000000000004e-08  img/s: 11.030616437145913  loss: 0.3704 (0.3885)  acc1: 90.6250 (89.7135)  acc5: 100.0000 (99.5312)  time: 11.6042  data: 0.0182\n",
      "Epoch: [4]  [30/40]  eta: 0:01:55  lr: 1.0000000000000004e-08  img/s: 11.037105907046497  loss: 0.3546 (0.3874)  acc1: 90.6250 (89.7681)  acc5: 100.0000 (99.5464)  time: 11.6110  data: 0.0181\n",
      "Epoch: [4]  [31/40]  eta: 0:01:44  lr: 1.0000000000000004e-08  img/s: 11.11994893121331  loss: 0.3704 (0.3870)  acc1: 90.6250 (89.6973)  acc5: 100.0000 (99.5605)  time: 11.6088  data: 0.0181\n",
      "Epoch: [4]  [32/40]  eta: 0:01:32  lr: 1.0000000000000004e-08  img/s: 11.158360006736531  loss: 0.3740 (0.3876)  acc1: 90.6250 (89.7491)  acc5: 100.0000 (99.5739)  time: 11.6098  data: 0.0181\n",
      "Epoch: [4]  [33/40]  eta: 0:01:21  lr: 1.0000000000000004e-08  img/s: 11.14704725732068  loss: 0.3740 (0.3910)  acc1: 90.6250 (89.6140)  acc5: 100.0000 (99.5864)  time: 11.6080  data: 0.0181\n",
      "Epoch: [4]  [34/40]  eta: 0:01:09  lr: 1.0000000000000004e-08  img/s: 11.138799072398356  loss: 0.3740 (0.3906)  acc1: 90.6250 (89.6429)  acc5: 100.0000 (99.5982)  time: 11.6063  data: 0.0182\n",
      "Epoch: [4]  [35/40]  eta: 0:00:57  lr: 1.0000000000000004e-08  img/s: 10.941587162289608  loss: 0.3740 (0.3901)  acc1: 90.6250 (89.6701)  acc5: 100.0000 (99.6094)  time: 11.6078  data: 0.0182\n",
      "Epoch: [4]  [36/40]  eta: 0:00:46  lr: 1.0000000000000004e-08  img/s: 11.033747627893181  loss: 0.3740 (0.3889)  acc1: 90.6250 (89.7171)  acc5: 100.0000 (99.5988)  time: 11.6106  data: 0.0178\n",
      "Epoch: [4]  [37/40]  eta: 0:00:34  lr: 1.0000000000000004e-08  img/s: 11.100325166795091  loss: 0.3782 (0.3902)  acc1: 90.6250 (89.7410)  acc5: 100.0000 (99.5683)  time: 11.6043  data: 0.0178\n",
      "Epoch: [4]  [38/40]  eta: 0:00:23  lr: 1.0000000000000004e-08  img/s: 10.93483041967827  loss: 0.3782 (0.3903)  acc1: 90.6250 (89.7236)  acc5: 100.0000 (99.5593)  time: 11.6124  data: 0.0178\n",
      "Epoch: [4]  [39/40]  eta: 0:00:11  lr: 1.0000000000000004e-08  img/s: 10.000244981091454  loss: 0.3798 (0.4163)  acc1: 90.6250 (89.7000)  acc5: 100.0000 (99.5400)  time: 11.0802  data: 0.0171\n",
      "Epoch: [4] Total time: 0:07:32\n",
      "Epoch 6/15\n",
      "Epoch: [5]  [ 0/40]  eta: 0:07:47  lr: 1.0000000000000005e-09  img/s: 10.9669729965994  loss: 0.4286 (0.4286)  acc1: 88.2812 (88.2812)  acc5: 99.2188 (99.2188)  time: 11.6894  data: 0.0180\n",
      "Epoch: [5]  [ 1/40]  eta: 0:07:38  lr: 1.0000000000000005e-09  img/s: 10.820883610808265  loss: 0.3679 (0.3983)  acc1: 88.2812 (90.2344)  acc5: 99.2188 (99.6094)  time: 11.7681  data: 0.0179\n",
      "Epoch: [5]  [ 2/40]  eta: 0:07:25  lr: 1.0000000000000005e-09  img/s: 11.001000901479337  loss: 0.4286 (0.4120)  acc1: 88.2812 (89.5833)  acc5: 99.2188 (99.4792)  time: 11.7297  data: 0.0178\n",
      "Epoch: [5]  [ 3/40]  eta: 0:07:14  lr: 1.0000000000000005e-09  img/s: 10.833074285944893  loss: 0.4283 (0.4161)  acc1: 88.2812 (90.2344)  acc5: 99.2188 (99.4141)  time: 11.7557  data: 0.0178\n",
      "Epoch: [5]  [ 4/40]  eta: 0:07:02  lr: 1.0000000000000005e-09  img/s: 10.962681838201366  loss: 0.4283 (0.4070)  acc1: 88.2812 (89.8438)  acc5: 99.2188 (99.5312)  time: 11.7434  data: 0.0179\n",
      "Epoch: [5]  [ 5/40]  eta: 0:06:51  lr: 1.0000000000000005e-09  img/s: 10.904026531468098  loss: 0.4283 (0.4238)  acc1: 88.2812 (89.1927)  acc5: 99.2188 (99.4792)  time: 11.7456  data: 0.0179\n",
      "Epoch: [5]  [ 6/40]  eta: 0:06:38  lr: 1.0000000000000005e-09  img/s: 11.097440517472604  loss: 0.4283 (0.4188)  acc1: 88.2812 (89.1741)  acc5: 99.2188 (99.5536)  time: 11.7179  data: 0.0179\n",
      "Epoch: [5]  [ 7/40]  eta: 0:06:26  lr: 1.0000000000000005e-09  img/s: 10.880253335366424  loss: 0.4283 (0.4253)  acc1: 88.2812 (88.7695)  acc5: 99.2188 (99.6094)  time: 11.7260  data: 0.0179\n",
      "Epoch: [5]  [ 8/40]  eta: 0:06:14  lr: 1.0000000000000005e-09  img/s: 11.042880889731302  loss: 0.4286 (0.4276)  acc1: 88.2812 (88.8889)  acc5: 99.2188 (99.5660)  time: 11.7130  data: 0.0179\n",
      "Epoch: [5]  [ 9/40]  eta: 0:06:02  lr: 1.0000000000000005e-09  img/s: 11.08945567176969  loss: 0.4286 (0.4356)  acc1: 88.2812 (88.5156)  acc5: 99.2188 (99.6094)  time: 11.6977  data: 0.0179\n",
      "Epoch: [5]  [10/40]  eta: 0:05:50  lr: 1.0000000000000005e-09  img/s: 11.040500514452683  loss: 0.4395 (0.4360)  acc1: 88.2812 (88.7074)  acc5: 99.2188 (99.4318)  time: 11.6899  data: 0.0179\n",
      "Epoch: [5]  [11/40]  eta: 0:05:38  lr: 1.0000000000000005e-09  img/s: 11.04966156545573  loss: 0.4286 (0.4297)  acc1: 88.2812 (88.8021)  acc5: 99.2188 (99.4141)  time: 11.6826  data: 0.0179\n",
      "Epoch: [5]  [12/40]  eta: 0:05:26  lr: 1.0000000000000005e-09  img/s: 11.078081658448756  loss: 0.4286 (0.4236)  acc1: 89.0625 (89.0024)  acc5: 99.2188 (99.4591)  time: 11.6741  data: 0.0178\n",
      "Epoch: [5]  [13/40]  eta: 0:05:15  lr: 1.0000000000000005e-09  img/s: 10.784309062863876  loss: 0.4283 (0.4232)  acc1: 89.0625 (89.1741)  acc5: 99.2188 (99.4420)  time: 11.6893  data: 0.0178\n",
      "Epoch: [5]  [14/40]  eta: 0:05:03  lr: 1.0000000000000005e-09  img/s: 11.012305199939858  loss: 0.4283 (0.4227)  acc1: 89.8438 (89.2708)  acc5: 99.2188 (99.4271)  time: 11.6860  data: 0.0178\n",
      "Epoch: [5]  [15/40]  eta: 0:04:52  lr: 1.0000000000000005e-09  img/s: 10.794851600248023  loss: 0.4176 (0.4165)  acc1: 89.8438 (89.3066)  acc5: 99.2188 (99.4629)  time: 11.6979  data: 0.0178\n",
      "Epoch: [5]  [16/40]  eta: 0:04:40  lr: 1.0000000000000005e-09  img/s: 11.00271121165084  loss: 0.4176 (0.4090)  acc1: 89.8438 (89.6599)  acc5: 99.2188 (99.4945)  time: 11.6951  data: 0.0178\n",
      "Epoch: [5]  [17/40]  eta: 0:04:29  lr: 1.0000000000000005e-09  img/s: 10.791865142178013  loss: 0.4154 (0.4027)  acc1: 89.8438 (89.9306)  acc5: 99.2188 (99.5226)  time: 11.7054  data: 0.0179\n",
      "Epoch: [5]  [18/40]  eta: 0:04:17  lr: 1.0000000000000005e-09  img/s: 10.916150556288219  loss: 0.4176 (0.4043)  acc1: 89.8438 (89.7615)  acc5: 100.0000 (99.5477)  time: 11.7074  data: 0.0179\n",
      "Epoch: [5]  [19/40]  eta: 0:04:05  lr: 1.0000000000000005e-09  img/s: 11.049847143046057  loss: 0.4154 (0.3995)  acc1: 89.8438 (89.8438)  acc5: 100.0000 (99.5703)  time: 11.7021  data: 0.0179\n",
      "Epoch: [5]  [20/40]  eta: 0:03:53  lr: 1.0000000000000005e-09  img/s: 11.074068596443656  loss: 0.3891 (0.3987)  acc1: 89.8438 (89.8438)  acc5: 100.0000 (99.5164)  time: 11.6964  data: 0.0178\n",
      "Epoch: [5]  [21/40]  eta: 0:03:42  lr: 1.0000000000000005e-09  img/s: 10.968427807960351  loss: 0.3891 (0.3947)  acc1: 89.8438 (89.9858)  acc5: 99.2188 (99.5028)  time: 11.6885  data: 0.0178\n",
      "Epoch: [5]  [22/40]  eta: 0:03:30  lr: 1.0000000000000005e-09  img/s: 10.979863732692484  loss: 0.3877 (0.3944)  acc1: 89.8438 (90.0476)  acc5: 99.2188 (99.4905)  time: 11.6896  data: 0.0178\n",
      "Epoch: [5]  [23/40]  eta: 0:03:18  lr: 1.0000000000000005e-09  img/s: 10.866214392573736  loss: 0.3848 (0.3940)  acc1: 89.8438 (90.0716)  acc5: 99.2188 (99.4792)  time: 11.6878  data: 0.0178\n",
      "Epoch: [5]  [24/40]  eta: 0:03:07  lr: 1.0000000000000005e-09  img/s: 11.086093851088782  loss: 0.3848 (0.3915)  acc1: 90.6250 (90.1562)  acc5: 99.2188 (99.4688)  time: 11.6813  data: 0.0178\n",
      "Epoch: [5]  [25/40]  eta: 0:02:55  lr: 1.0000000000000005e-09  img/s: 10.898415740430453  loss: 0.3848 (0.3918)  acc1: 90.6250 (90.1142)  acc5: 99.2188 (99.4892)  time: 11.6816  data: 0.0178\n",
      "Epoch: [5]  [26/40]  eta: 0:02:43  lr: 1.0000000000000005e-09  img/s: 10.919899145840256  loss: 0.3831 (0.3881)  acc1: 90.6250 (90.2488)  acc5: 99.2188 (99.5081)  time: 11.6910  data: 0.0178\n",
      "Epoch: [5]  [27/40]  eta: 0:02:32  lr: 1.0000000000000005e-09  img/s: 11.053124205430015  loss: 0.3606 (0.3857)  acc1: 90.6250 (90.3181)  acc5: 99.2188 (99.5257)  time: 11.6818  data: 0.0178\n",
      "Epoch: [5]  [28/40]  eta: 0:02:20  lr: 1.0000000000000005e-09  img/s: 10.946820628613995  loss: 0.3507 (0.3834)  acc1: 91.4062 (90.3556)  acc5: 99.2188 (99.5151)  time: 11.6869  data: 0.0178\n",
      "Epoch: [5]  [29/40]  eta: 0:02:08  lr: 1.0000000000000005e-09  img/s: 11.085598257461802  loss: 0.3507 (0.3871)  acc1: 91.4062 (90.2344)  acc5: 99.2188 (99.5052)  time: 11.6871  data: 0.0178\n",
      "Epoch: [5]  [30/40]  eta: 0:01:56  lr: 1.0000000000000005e-09  img/s: 10.975162652382414  loss: 0.3507 (0.3864)  acc1: 91.4062 (90.2470)  acc5: 99.2188 (99.4456)  time: 11.6905  data: 0.0178\n",
      "Epoch: [5]  [31/40]  eta: 0:01:45  lr: 1.0000000000000005e-09  img/s: 11.117035428571477  loss: 0.3507 (0.3854)  acc1: 91.4062 (90.2832)  acc5: 99.2188 (99.4629)  time: 11.6870  data: 0.0179\n",
      "Epoch: [5]  [32/40]  eta: 0:01:33  lr: 1.0000000000000005e-09  img/s: 11.313875588083663  loss: 0.3442 (0.3842)  acc1: 91.4062 (90.3172)  acc5: 99.2188 (99.4792)  time: 11.6750  data: 0.0179\n",
      "Epoch: [5]  [33/40]  eta: 0:01:21  lr: 1.0000000000000005e-09  img/s: 11.369381682626665  loss: 0.3442 (0.3839)  acc1: 91.4062 (90.3722)  acc5: 99.2188 (99.4715)  time: 11.6445  data: 0.0179\n",
      "Epoch: [5]  [34/40]  eta: 0:01:09  lr: 1.0000000000000005e-09  img/s: 11.0725214584264  loss: 0.3442 (0.3842)  acc1: 91.4062 (90.3795)  acc5: 100.0000 (99.4866)  time: 11.6413  data: 0.0179\n",
      "Epoch: [5]  [35/40]  eta: 0:00:58  lr: 1.0000000000000005e-09  img/s: 11.22782587105692  loss: 0.3442 (0.3819)  acc1: 91.4062 (90.4514)  acc5: 100.0000 (99.5009)  time: 11.6184  data: 0.0179\n",
      "Epoch: [5]  [36/40]  eta: 0:00:46  lr: 1.0000000000000005e-09  img/s: 10.70371779858551  loss: 0.3442 (0.3808)  acc1: 91.4062 (90.4350)  acc5: 100.0000 (99.5144)  time: 11.6347  data: 0.0179\n",
      "Epoch: [5]  [37/40]  eta: 0:00:34  lr: 1.0000000000000005e-09  img/s: 10.88613260895771  loss: 0.3566 (0.3828)  acc1: 91.4062 (90.3988)  acc5: 99.2188 (99.5066)  time: 11.6295  data: 0.0178\n",
      "Epoch: [5]  [38/40]  eta: 0:00:23  lr: 1.0000000000000005e-09  img/s: 10.829176451072284  loss: 0.3442 (0.3801)  acc1: 91.4062 (90.4848)  acc5: 99.2188 (99.5192)  time: 11.6342  data: 0.0179\n",
      "Epoch: [5]  [39/40]  eta: 0:00:11  lr: 1.0000000000000005e-09  img/s: 10.288112498758231  loss: 0.3566 (0.4088)  acc1: 91.4062 (90.4400)  acc5: 99.2188 (99.5000)  time: 11.0931  data: 0.0171\n",
      "Epoch: [5] Total time: 0:07:35\n",
      "Epoch 7/15\n",
      "Epoch: [6]  [ 0/40]  eta: 0:07:47  lr: 1.0000000000000006e-10  img/s: 10.957740040116605  loss: 0.4668 (0.4668)  acc1: 85.9375 (85.9375)  acc5: 99.2188 (99.2188)  time: 11.6992  data: 0.0180\n",
      "Epoch: [6]  [ 1/40]  eta: 0:07:39  lr: 1.0000000000000006e-10  img/s: 10.806630167437769  loss: 0.3486 (0.4077)  acc1: 85.9375 (89.0625)  acc5: 99.2188 (99.6094)  time: 11.7808  data: 0.0179\n",
      "Epoch: [6]  [ 2/40]  eta: 0:07:27  lr: 1.0000000000000006e-10  img/s: 10.900367174875178  loss: 0.3677 (0.3944)  acc1: 90.6250 (89.5833)  acc5: 99.2188 (99.4792)  time: 11.7740  data: 0.0178\n",
      "Epoch: [6]  [ 3/40]  eta: 0:07:16  lr: 1.0000000000000006e-10  img/s: 10.830464930309272  loss: 0.3677 (0.4109)  acc1: 88.2812 (89.2578)  acc5: 99.2188 (99.4141)  time: 11.7896  data: 0.0178\n",
      "Epoch: [6]  [ 4/40]  eta: 0:07:02  lr: 1.0000000000000006e-10  img/s: 11.08342457852525  loss: 0.4231 (0.4133)  acc1: 88.2812 (88.7500)  acc5: 99.2188 (99.3750)  time: 11.7450  data: 0.0178\n",
      "Epoch: [6]  [ 5/40]  eta: 0:06:50  lr: 1.0000000000000006e-10  img/s: 11.058392961397464  loss: 0.4231 (0.4377)  acc1: 86.7188 (88.0208)  acc5: 99.2188 (99.4792)  time: 11.7196  data: 0.0178\n",
      "Epoch: [6]  [ 6/40]  eta: 0:06:37  lr: 1.0000000000000006e-10  img/s: 11.147446284628307  loss: 0.4231 (0.4322)  acc1: 86.7188 (87.7232)  acc5: 99.2188 (99.5536)  time: 11.6883  data: 0.0178\n",
      "Epoch: [6]  [ 7/40]  eta: 0:06:25  lr: 1.0000000000000006e-10  img/s: 11.06150439006251  loss: 0.4231 (0.4372)  acc1: 85.9375 (87.5000)  acc5: 99.2188 (99.6094)  time: 11.6760  data: 0.0178\n",
      "Epoch: [6]  [ 8/40]  eta: 0:06:13  lr: 1.0000000000000006e-10  img/s: 10.903543760450727  loss: 0.4231 (0.4288)  acc1: 86.7188 (87.7604)  acc5: 99.2188 (99.5660)  time: 11.6850  data: 0.0179\n",
      "Epoch: [6]  [ 9/40]  eta: 0:06:02  lr: 1.0000000000000006e-10  img/s: 10.957275758957977  loss: 0.3995 (0.4235)  acc1: 86.7188 (87.9688)  acc5: 99.2188 (99.5312)  time: 11.6865  data: 0.0179\n",
      "Epoch: [6]  [10/40]  eta: 0:05:50  lr: 1.0000000000000006e-10  img/s: 10.98067062050859  loss: 0.4231 (0.4303)  acc1: 86.7188 (87.7131)  acc5: 99.2188 (99.5028)  time: 11.6854  data: 0.0179\n",
      "Epoch: [6]  [11/40]  eta: 0:05:38  lr: 1.0000000000000006e-10  img/s: 10.94490340054087  loss: 0.3995 (0.4226)  acc1: 86.7188 (88.1510)  acc5: 99.2188 (99.4792)  time: 11.6877  data: 0.0179\n",
      "Epoch: [6]  [12/40]  eta: 0:05:27  lr: 1.0000000000000006e-10  img/s: 10.982914270549017  loss: 0.3995 (0.4184)  acc1: 88.2812 (88.2812)  acc5: 99.2188 (99.5192)  time: 11.6865  data: 0.0179\n",
      "Epoch: [6]  [13/40]  eta: 0:05:15  lr: 1.0000000000000006e-10  img/s: 10.810501772392115  loss: 0.3960 (0.4168)  acc1: 88.2812 (88.3929)  acc5: 99.2188 (99.5536)  time: 11.6988  data: 0.0179\n",
      "Epoch: [6]  [14/40]  eta: 0:05:04  lr: 1.0000000000000006e-10  img/s: 10.918516244354054  loss: 0.3960 (0.4147)  acc1: 89.8438 (88.4896)  acc5: 99.2188 (99.5312)  time: 11.7016  data: 0.0179\n",
      "Epoch: [6]  [15/40]  eta: 0:04:52  lr: 1.0000000000000006e-10  img/s: 10.919627068327863  loss: 0.3857 (0.4090)  acc1: 89.8438 (88.6719)  acc5: 99.2188 (99.5117)  time: 11.7040  data: 0.0179\n",
      "Epoch: [6]  [16/40]  eta: 0:04:41  lr: 1.0000000000000006e-10  img/s: 10.862614433645831  loss: 0.3857 (0.4037)  acc1: 89.8438 (88.8787)  acc5: 99.2188 (99.4945)  time: 11.7097  data: 0.0179\n",
      "Epoch: [6]  [17/40]  eta: 0:04:29  lr: 1.0000000000000006e-10  img/s: 11.135991639319021  loss: 0.3754 (0.3973)  acc1: 89.8438 (89.1493)  acc5: 99.2188 (99.5226)  time: 11.6987  data: 0.0179\n",
      "Epoch: [6]  [18/40]  eta: 0:04:17  lr: 1.0000000000000006e-10  img/s: 11.123830522979103  loss: 0.3754 (0.3960)  acc1: 89.8438 (89.2681)  acc5: 99.2188 (99.5066)  time: 11.6896  data: 0.0179\n",
      "Epoch: [6]  [19/40]  eta: 0:04:05  lr: 1.0000000000000006e-10  img/s: 10.916846214357637  loss: 0.3719 (0.3894)  acc1: 89.8438 (89.4922)  acc5: 99.2188 (99.5312)  time: 11.6922  data: 0.0179\n",
      "Epoch: [6]  [20/40]  eta: 0:03:53  lr: 1.0000000000000006e-10  img/s: 10.927390041683463  loss: 0.3719 (0.3903)  acc1: 89.8438 (89.3601)  acc5: 99.2188 (99.5164)  time: 11.6939  data: 0.0179\n",
      "Epoch: [6]  [21/40]  eta: 0:03:42  lr: 1.0000000000000006e-10  img/s: 10.967854620432016  loss: 0.3719 (0.3875)  acc1: 89.8438 (89.5241)  acc5: 99.2188 (99.5384)  time: 11.6851  data: 0.0179\n",
      "Epoch: [6]  [22/40]  eta: 0:03:30  lr: 1.0000000000000006e-10  img/s: 11.004687771340002  loss: 0.3754 (0.3909)  acc1: 89.8438 (89.3342)  acc5: 99.2188 (99.5245)  time: 11.6796  data: 0.0179\n",
      "Epoch: [6]  [23/40]  eta: 0:03:18  lr: 1.0000000000000006e-10  img/s: 10.917959584929113  loss: 0.3719 (0.3884)  acc1: 89.8438 (89.4857)  acc5: 99.2188 (99.5117)  time: 11.6749  data: 0.0179\n",
      "Epoch: [6]  [24/40]  eta: 0:03:07  lr: 1.0000000000000006e-10  img/s: 11.058410955987673  loss: 0.3719 (0.3890)  acc1: 89.8438 (89.5312)  acc5: 99.2188 (99.5000)  time: 11.6761  data: 0.0178\n",
      "Epoch: [6]  [25/40]  eta: 0:02:55  lr: 1.0000000000000006e-10  img/s: 11.122565776221368  loss: 0.3719 (0.3931)  acc1: 89.8438 (89.5132)  acc5: 99.2188 (99.5192)  time: 11.6728  data: 0.0178\n",
      "Epoch: [6]  [26/40]  eta: 0:02:43  lr: 1.0000000000000006e-10  img/s: 10.984373988828885  loss: 0.3677 (0.3905)  acc1: 89.8438 (89.6123)  acc5: 99.2188 (99.4792)  time: 11.6813  data: 0.0178\n",
      "Epoch: [6]  [27/40]  eta: 0:02:31  lr: 1.0000000000000006e-10  img/s: 11.133971098966933  loss: 0.3677 (0.3900)  acc1: 89.8438 (89.5926)  acc5: 99.2188 (99.4978)  time: 11.6775  data: 0.0178\n",
      "Epoch: [6]  [28/40]  eta: 0:02:20  lr: 1.0000000000000006e-10  img/s: 11.14408530343061  loss: 0.3719 (0.3906)  acc1: 89.8438 (89.5744)  acc5: 99.2188 (99.4881)  time: 11.6649  data: 0.0178\n",
      "Epoch: [6]  [29/40]  eta: 0:02:08  lr: 1.0000000000000006e-10  img/s: 10.88129772251767  loss: 0.3719 (0.3922)  acc1: 89.8438 (89.4792)  acc5: 99.2188 (99.4792)  time: 11.6689  data: 0.0178\n",
      "Epoch: [6]  [30/40]  eta: 0:01:56  lr: 1.0000000000000006e-10  img/s: 11.043206846213007  loss: 0.3719 (0.3916)  acc1: 90.6250 (89.5413)  acc5: 99.2188 (99.4708)  time: 11.6656  data: 0.0178\n",
      "Epoch: [6]  [31/40]  eta: 0:01:45  lr: 1.0000000000000006e-10  img/s: 10.991165788987459  loss: 0.3744 (0.3915)  acc1: 89.8438 (89.5020)  acc5: 99.2188 (99.4629)  time: 11.6631  data: 0.0178\n",
      "Epoch: [6]  [32/40]  eta: 0:01:33  lr: 1.0000000000000006e-10  img/s: 11.06610228204915  loss: 0.3744 (0.3887)  acc1: 90.6250 (89.5833)  acc5: 99.2188 (99.4792)  time: 11.6587  data: 0.0178\n",
      "Epoch: [6]  [33/40]  eta: 0:01:21  lr: 1.0000000000000006e-10  img/s: 10.881364988274992  loss: 0.3744 (0.3901)  acc1: 90.6250 (89.5680)  acc5: 99.2188 (99.4715)  time: 11.6552  data: 0.0181\n",
      "Epoch: [6]  [34/40]  eta: 0:01:10  lr: 1.0000000000000006e-10  img/s: 11.075352039514145  loss: 0.3744 (0.3910)  acc1: 90.6250 (89.5759)  acc5: 99.2188 (99.4643)  time: 11.6469  data: 0.0181\n",
      "Epoch: [6]  [35/40]  eta: 0:00:58  lr: 1.0000000000000006e-10  img/s: 10.921413921474421  loss: 0.3744 (0.3897)  acc1: 89.8438 (89.5833)  acc5: 99.2188 (99.4575)  time: 11.6468  data: 0.0181\n",
      "Epoch: [6]  [36/40]  eta: 0:00:46  lr: 1.0000000000000006e-10  img/s: 10.890090999561066  loss: 0.3744 (0.3881)  acc1: 89.8438 (89.6326)  acc5: 99.2188 (99.4721)  time: 11.6453  data: 0.0181\n",
      "Epoch: [6]  [37/40]  eta: 0:00:35  lr: 1.0000000000000006e-10  img/s: 11.086087899124143  loss: 0.3774 (0.3885)  acc1: 89.8438 (89.6382)  acc5: 99.2188 (99.4243)  time: 11.6479  data: 0.0181\n",
      "Epoch: [6]  [38/40]  eta: 0:00:23  lr: 1.0000000000000006e-10  img/s: 11.070855125856811  loss: 0.3774 (0.3867)  acc1: 89.8438 (89.7035)  acc5: 99.2188 (99.4391)  time: 11.6506  data: 0.0180\n",
      "Epoch: [6]  [39/40]  eta: 0:00:11  lr: 1.0000000000000006e-10  img/s: 10.323932360253034  loss: 0.3887 (0.4096)  acc1: 89.8438 (89.6800)  acc5: 99.2188 (99.4200)  time: 11.1023  data: 0.0173\n",
      "Epoch: [6] Total time: 0:07:35\n",
      "Epoch 8/15\n",
      "Epoch: [7]  [ 0/40]  eta: 0:07:47  lr: 1.0000000000000006e-11  img/s: 10.971293971876122  loss: 0.4161 (0.4161)  acc1: 89.8438 (89.8438)  acc5: 98.4375 (98.4375)  time: 11.6946  data: 0.0278\n",
      "Epoch: [7]  [ 1/40]  eta: 0:07:37  lr: 1.0000000000000006e-11  img/s: 10.896001257339623  loss: 0.3493 (0.3827)  acc1: 89.8438 (89.8438)  acc5: 98.4375 (99.2188)  time: 11.7300  data: 0.0229\n",
      "Epoch: [7]  [ 2/40]  eta: 0:07:25  lr: 1.0000000000000006e-11  img/s: 10.922613555813806  loss: 0.4161 (0.4021)  acc1: 89.8438 (89.3229)  acc5: 100.0000 (99.4792)  time: 11.7323  data: 0.0212\n",
      "Epoch: [7]  [ 3/40]  eta: 0:07:12  lr: 1.0000000000000006e-11  img/s: 11.100918022670713  loss: 0.4161 (0.4075)  acc1: 88.2812 (89.0625)  acc5: 100.0000 (99.6094)  time: 11.6863  data: 0.0204\n",
      "Epoch: [7]  [ 4/40]  eta: 0:07:00  lr: 1.0000000000000006e-11  img/s: 11.002925206940084  loss: 0.4161 (0.4004)  acc1: 88.2812 (88.9062)  acc5: 100.0000 (99.5312)  time: 11.6792  data: 0.0198\n",
      "Epoch: [7]  [ 5/40]  eta: 0:06:49  lr: 1.0000000000000006e-11  img/s: 10.877691510878535  loss: 0.4161 (0.4389)  acc1: 88.2812 (87.6302)  acc5: 99.2188 (99.4792)  time: 11.6968  data: 0.0195\n",
      "Epoch: [7]  [ 6/40]  eta: 0:06:37  lr: 1.0000000000000006e-11  img/s: 10.921868058394331  loss: 0.4161 (0.4206)  acc1: 88.2812 (88.6161)  acc5: 99.2188 (99.4420)  time: 11.7026  data: 0.0192\n",
      "Epoch: [7]  [ 7/40]  eta: 0:06:25  lr: 1.0000000000000006e-11  img/s: 11.070468182866165  loss: 0.4161 (0.4336)  acc1: 88.2812 (88.2812)  acc5: 99.2188 (99.5117)  time: 11.6873  data: 0.0191\n",
      "Epoch: [7]  [ 8/40]  eta: 0:06:13  lr: 1.0000000000000006e-11  img/s: 11.006140196835464  loss: 0.4161 (0.4269)  acc1: 88.2812 (88.7153)  acc5: 99.2188 (99.4792)  time: 11.6829  data: 0.0189\n",
      "Epoch: [7]  [ 9/40]  eta: 0:06:01  lr: 1.0000000000000006e-11  img/s: 11.058764481839152  loss: 0.4161 (0.4295)  acc1: 88.2812 (88.5156)  acc5: 99.2188 (99.5312)  time: 11.6739  data: 0.0188\n",
      "Epoch: [7]  [10/40]  eta: 0:05:50  lr: 1.0000000000000006e-11  img/s: 10.959766922590784  loss: 0.4235 (0.4343)  acc1: 88.2812 (88.3523)  acc5: 99.2188 (99.5028)  time: 11.6760  data: 0.0187\n",
      "Epoch: [7]  [11/40]  eta: 0:05:38  lr: 1.0000000000000006e-11  img/s: 10.990642421580363  loss: 0.4161 (0.4246)  acc1: 88.2812 (88.8021)  acc5: 99.2188 (99.4792)  time: 11.6750  data: 0.0186\n",
      "Epoch: [7]  [12/40]  eta: 0:05:26  lr: 1.0000000000000006e-11  img/s: 10.974752980065466  loss: 0.4161 (0.4166)  acc1: 88.2812 (88.9423)  acc5: 99.2188 (99.5192)  time: 11.6754  data: 0.0186\n",
      "Epoch: [7]  [13/40]  eta: 0:05:15  lr: 1.0000000000000006e-11  img/s: 11.025783041414584  loss: 0.4161 (0.4205)  acc1: 88.2812 (88.9509)  acc5: 99.2188 (99.4978)  time: 11.6720  data: 0.0185\n",
      "Epoch: [7]  [14/40]  eta: 0:05:03  lr: 1.0000000000000006e-11  img/s: 10.929889884707132  loss: 0.4235 (0.4213)  acc1: 89.0625 (89.1146)  acc5: 99.2188 (99.4271)  time: 11.6758  data: 0.0185\n",
      "Epoch: [7]  [15/40]  eta: 0:04:51  lr: 1.0000000000000006e-11  img/s: 10.933027596546369  loss: 0.4161 (0.4143)  acc1: 89.0625 (89.3555)  acc5: 99.2188 (99.4629)  time: 11.6789  data: 0.0184\n",
      "Epoch: [7]  [16/40]  eta: 0:04:40  lr: 1.0000000000000006e-11  img/s: 10.966660261441453  loss: 0.4161 (0.4057)  acc1: 89.8438 (89.5680)  acc5: 99.2188 (99.4945)  time: 11.6795  data: 0.0184\n",
      "Epoch: [7]  [17/40]  eta: 0:04:28  lr: 1.0000000000000006e-11  img/s: 10.905894017609189  loss: 0.3734 (0.4000)  acc1: 89.8438 (89.7135)  acc5: 99.2188 (99.5226)  time: 11.6837  data: 0.0184\n",
      "Epoch: [7]  [18/40]  eta: 0:04:16  lr: 1.0000000000000006e-11  img/s: 11.040576574300012  loss: 0.4161 (0.4035)  acc1: 89.8438 (89.6382)  acc5: 99.2188 (99.5066)  time: 11.6799  data: 0.0183\n",
      "Epoch: [7]  [19/40]  eta: 0:04:05  lr: 1.0000000000000006e-11  img/s: 11.044895540671375  loss: 0.3734 (0.3984)  acc1: 89.8438 (89.8047)  acc5: 99.2188 (99.5312)  time: 11.6762  data: 0.0183\n",
      "Epoch: [7]  [20/40]  eta: 0:03:53  lr: 1.0000000000000006e-11  img/s: 10.912941333633684  loss: 0.3734 (0.3988)  acc1: 89.0625 (89.7693)  acc5: 99.2188 (99.5164)  time: 11.6788  data: 0.0178\n",
      "Epoch: [7]  [21/40]  eta: 0:03:41  lr: 1.0000000000000006e-11  img/s: 11.042416633721706  loss: 0.3734 (0.3952)  acc1: 89.0625 (89.8793)  acc5: 99.2188 (99.5384)  time: 11.6710  data: 0.0178\n",
      "Epoch: [7]  [22/40]  eta: 0:03:30  lr: 1.0000000000000006e-11  img/s: 10.963686808350742  loss: 0.3722 (0.3930)  acc1: 90.6250 (89.9457)  acc5: 99.2188 (99.5584)  time: 11.6688  data: 0.0178\n",
      "Epoch: [7]  [23/40]  eta: 0:03:18  lr: 1.0000000000000006e-11  img/s: 10.943648226608943  loss: 0.3452 (0.3897)  acc1: 91.4062 (90.0391)  acc5: 99.2188 (99.5443)  time: 11.6771  data: 0.0178\n",
      "Epoch: [7]  [24/40]  eta: 0:03:06  lr: 1.0000000000000006e-11  img/s: 10.918190057693833  loss: 0.3452 (0.3891)  acc1: 91.4062 (90.1250)  acc5: 99.2188 (99.4688)  time: 11.6816  data: 0.0178\n",
      "Epoch: [7]  [25/40]  eta: 0:02:55  lr: 1.0000000000000006e-11  img/s: 10.976821621891286  loss: 0.3452 (0.3922)  acc1: 91.4062 (90.0841)  acc5: 99.2188 (99.4892)  time: 11.6763  data: 0.0178\n",
      "Epoch: [7]  [26/40]  eta: 0:02:43  lr: 1.0000000000000006e-11  img/s: 11.103366547758315  loss: 0.3734 (0.3916)  acc1: 91.4062 (90.1910)  acc5: 100.0000 (99.5081)  time: 11.6667  data: 0.0178\n",
      "Epoch: [7]  [27/40]  eta: 0:02:31  lr: 1.0000000000000006e-11  img/s: 11.043280217467936  loss: 0.3452 (0.3887)  acc1: 92.1875 (90.3460)  acc5: 100.0000 (99.5257)  time: 11.6681  data: 0.0178\n",
      "Epoch: [7]  [28/40]  eta: 0:02:20  lr: 1.0000000000000006e-11  img/s: 10.901388421554048  loss: 0.3291 (0.3867)  acc1: 91.4062 (90.3287)  acc5: 100.0000 (99.5151)  time: 11.6737  data: 0.0178\n",
      "Epoch: [7]  [29/40]  eta: 0:02:08  lr: 1.0000000000000006e-11  img/s: 11.118661809701363  loss: 0.3291 (0.3895)  acc1: 91.4062 (90.2344)  acc5: 99.2188 (99.4531)  time: 11.6706  data: 0.0178\n",
      "Epoch: [7]  [30/40]  eta: 0:01:56  lr: 1.0000000000000006e-11  img/s: 10.878285951228273  loss: 0.3291 (0.3887)  acc1: 91.4062 (90.2218)  acc5: 99.2188 (99.4456)  time: 11.6750  data: 0.0178\n",
      "Epoch: [7]  [31/40]  eta: 0:01:45  lr: 1.0000000000000006e-11  img/s: 10.993482164356118  loss: 0.3452 (0.3899)  acc1: 91.4062 (90.1367)  acc5: 99.2188 (99.4141)  time: 11.6748  data: 0.0178\n",
      "Epoch: [7]  [32/40]  eta: 0:01:33  lr: 1.0000000000000006e-11  img/s: 11.001106174131106  loss: 0.3452 (0.3879)  acc1: 91.4062 (90.2225)  acc5: 99.2188 (99.4318)  time: 11.6734  data: 0.0178\n",
      "Epoch: [7]  [33/40]  eta: 0:01:21  lr: 1.0000000000000006e-11  img/s: 11.074059002581846  loss: 0.3452 (0.3885)  acc1: 91.4062 (90.1654)  acc5: 99.2188 (99.4256)  time: 11.6709  data: 0.0178\n",
      "Epoch: [7]  [34/40]  eta: 0:01:10  lr: 1.0000000000000006e-11  img/s: 11.05658787773968  loss: 0.3452 (0.3888)  acc1: 91.4062 (90.1562)  acc5: 99.2188 (99.4196)  time: 11.6642  data: 0.0178\n",
      "Epoch: [7]  [35/40]  eta: 0:00:58  lr: 1.0000000000000006e-11  img/s: 10.976167668174416  loss: 0.3659 (0.3889)  acc1: 89.8438 (90.0825)  acc5: 99.2188 (99.4358)  time: 11.6619  data: 0.0178\n",
      "Epoch: [7]  [36/40]  eta: 0:00:46  lr: 1.0000000000000006e-11  img/s: 11.111746304818194  loss: 0.3659 (0.3875)  acc1: 89.8438 (90.2027)  acc5: 99.2188 (99.4510)  time: 11.6543  data: 0.0178\n",
      "Epoch: [7]  [37/40]  eta: 0:00:34  lr: 1.0000000000000006e-11  img/s: 11.094624773264957  loss: 0.3752 (0.3886)  acc1: 89.8438 (90.1933)  acc5: 99.2188 (99.4038)  time: 11.6443  data: 0.0178\n",
      "Epoch: [7]  [38/40]  eta: 0:00:23  lr: 1.0000000000000006e-11  img/s: 11.097608663392078  loss: 0.3659 (0.3871)  acc1: 89.8438 (90.2043)  acc5: 99.2188 (99.4191)  time: 11.6413  data: 0.0178\n",
      "Epoch: [7]  [39/40]  eta: 0:00:11  lr: 1.0000000000000006e-11  img/s: 10.245155719537816  loss: 0.3752 (0.4085)  acc1: 89.8438 (90.1800)  acc5: 99.2188 (99.4200)  time: 11.1001  data: 0.0170\n",
      "Epoch: [7] Total time: 0:07:35\n",
      "Epoch 9/15\n",
      "Epoch: [8]  [ 0/40]  eta: 0:07:45  lr: 1.0000000000000006e-12  img/s: 11.009147335635845  loss: 0.3865 (0.3865)  acc1: 90.6250 (90.6250)  acc5: 100.0000 (100.0000)  time: 11.6447  data: 0.0180\n",
      "Epoch: [8]  [ 1/40]  eta: 0:07:37  lr: 1.0000000000000006e-12  img/s: 10.832919525267636  loss: 0.3771 (0.3818)  acc1: 90.6250 (91.0156)  acc5: 100.0000 (100.0000)  time: 11.7392  data: 0.0179\n",
      "Epoch: [8]  [ 2/40]  eta: 0:07:25  lr: 1.0000000000000006e-12  img/s: 10.927884491396659  loss: 0.3865 (0.4043)  acc1: 90.6250 (90.6250)  acc5: 100.0000 (99.7396)  time: 11.7365  data: 0.0179\n",
      "Epoch: [8]  [ 3/40]  eta: 0:07:14  lr: 1.0000000000000006e-12  img/s: 10.863275148767046  loss: 0.3865 (0.4375)  acc1: 89.8438 (88.4766)  acc5: 100.0000 (99.8047)  time: 11.7525  data: 0.0179\n",
      "Epoch: [8]  [ 4/40]  eta: 0:07:01  lr: 1.0000000000000006e-12  img/s: 11.097669683498768  loss: 0.4153 (0.4331)  acc1: 89.8438 (88.7500)  acc5: 100.0000 (99.8438)  time: 11.7124  data: 0.0179\n",
      "Epoch: [8]  [ 5/40]  eta: 0:06:49  lr: 1.0000000000000006e-12  img/s: 11.022080441403116  loss: 0.4153 (0.4456)  acc1: 89.8438 (88.2812)  acc5: 100.0000 (99.7396)  time: 11.6988  data: 0.0179\n",
      "Epoch: [8]  [ 6/40]  eta: 0:06:37  lr: 1.0000000000000006e-12  img/s: 10.915969886150167  loss: 0.4153 (0.4391)  acc1: 89.8438 (88.1696)  acc5: 100.0000 (99.7768)  time: 11.7052  data: 0.0178\n",
      "Epoch: [8]  [ 7/40]  eta: 0:06:25  lr: 1.0000000000000006e-12  img/s: 11.060200229518285  loss: 0.4000 (0.4326)  acc1: 89.0625 (88.2812)  acc5: 100.0000 (99.7070)  time: 11.6909  data: 0.0178\n",
      "Epoch: [8]  [ 8/40]  eta: 0:06:14  lr: 1.0000000000000006e-12  img/s: 10.868989091996877  loss: 0.4000 (0.4287)  acc1: 89.0625 (88.3681)  acc5: 100.0000 (99.6528)  time: 11.7024  data: 0.0178\n",
      "Epoch: [8]  [ 9/40]  eta: 0:06:02  lr: 1.0000000000000006e-12  img/s: 10.957368790951405  loss: 0.4000 (0.4289)  acc1: 89.0625 (88.2812)  acc5: 99.2188 (99.5312)  time: 11.7021  data: 0.0178\n",
      "Epoch: [8]  [10/40]  eta: 0:05:50  lr: 1.0000000000000006e-12  img/s: 11.108875715279959  loss: 0.4153 (0.4338)  acc1: 89.0625 (87.8551)  acc5: 99.2188 (99.5028)  time: 11.6874  data: 0.0178\n",
      "Epoch: [8]  [11/40]  eta: 0:05:38  lr: 1.0000000000000006e-12  img/s: 11.121229439499647  loss: 0.4000 (0.4283)  acc1: 89.0625 (88.0859)  acc5: 99.2188 (99.4141)  time: 11.6740  data: 0.0178\n",
      "Epoch: [8]  [12/40]  eta: 0:05:26  lr: 1.0000000000000006e-12  img/s: 10.949918713507174  loss: 0.4000 (0.4241)  acc1: 89.0625 (88.4615)  acc5: 99.2188 (99.4591)  time: 11.6766  data: 0.0178\n",
      "Epoch: [8]  [13/40]  eta: 0:05:15  lr: 1.0000000000000006e-12  img/s: 10.875125385200127  loss: 0.4000 (0.4306)  acc1: 89.0625 (88.2254)  acc5: 99.2188 (99.3862)  time: 11.6846  data: 0.0178\n",
      "Epoch: [8]  [14/40]  eta: 0:05:03  lr: 1.0000000000000006e-12  img/s: 11.133714109428087  loss: 0.4153 (0.4321)  acc1: 89.0625 (88.1771)  acc5: 99.2188 (99.3229)  time: 11.6732  data: 0.0178\n",
      "Epoch: [8]  [15/40]  eta: 0:04:51  lr: 1.0000000000000006e-12  img/s: 11.06125802759852  loss: 0.4000 (0.4239)  acc1: 89.0625 (88.4766)  acc5: 99.2188 (99.3164)  time: 11.6680  data: 0.0178\n",
      "Epoch: [8]  [16/40]  eta: 0:04:39  lr: 1.0000000000000006e-12  img/s: 11.116577116326654  loss: 0.4000 (0.4171)  acc1: 89.0625 (88.7408)  acc5: 99.2188 (99.3107)  time: 11.6600  data: 0.0178\n",
      "Epoch: [8]  [17/40]  eta: 0:04:28  lr: 1.0000000000000006e-12  img/s: 10.816717761251285  loss: 0.3972 (0.4091)  acc1: 89.0625 (89.0625)  acc5: 99.2188 (99.3490)  time: 11.6706  data: 0.0178\n",
      "Epoch: [8]  [18/40]  eta: 0:04:16  lr: 1.0000000000000006e-12  img/s: 10.93852409942025  loss: 0.4000 (0.4096)  acc1: 89.0625 (89.0214)  acc5: 99.2188 (99.3010)  time: 11.6732  data: 0.0178\n",
      "Epoch: [8]  [19/40]  eta: 0:04:05  lr: 1.0000000000000006e-12  img/s: 10.987182161039396  loss: 0.3972 (0.4036)  acc1: 89.0625 (89.2969)  acc5: 99.2188 (99.3359)  time: 11.6729  data: 0.0178\n",
      "Epoch: [8]  [20/40]  eta: 0:03:53  lr: 1.0000000000000006e-12  img/s: 11.035857853902897  loss: 0.4000 (0.4045)  acc1: 89.0625 (89.2485)  acc5: 99.2188 (99.2932)  time: 11.6715  data: 0.0178\n",
      "Epoch: [8]  [21/40]  eta: 0:03:41  lr: 1.0000000000000006e-12  img/s: 10.960803582108737  loss: 0.4000 (0.4003)  acc1: 89.0625 (89.4176)  acc5: 99.2188 (99.3253)  time: 11.6646  data: 0.0178\n",
      "Epoch: [8]  [22/40]  eta: 0:03:29  lr: 1.0000000000000006e-12  img/s: 11.15410874995515  loss: 0.3972 (0.3982)  acc1: 89.0625 (89.5380)  acc5: 99.2188 (99.3207)  time: 11.6528  data: 0.0178\n",
      "Epoch: [8]  [23/40]  eta: 0:03:18  lr: 1.0000000000000006e-12  img/s: 11.057450491182127  loss: 0.3871 (0.3943)  acc1: 89.0625 (89.6810)  acc5: 99.2188 (99.3490)  time: 11.6424  data: 0.0178\n",
      "Epoch: [8]  [24/40]  eta: 0:03:06  lr: 1.0000000000000006e-12  img/s: 11.060867881189253  loss: 0.3735 (0.3918)  acc1: 89.0625 (89.7812)  acc5: 99.2188 (99.3750)  time: 11.6443  data: 0.0178\n",
      "Epoch: [8]  [25/40]  eta: 0:02:54  lr: 1.0000000000000006e-12  img/s: 11.01927769965277  loss: 0.3735 (0.3922)  acc1: 90.6250 (89.8137)  acc5: 99.2188 (99.3990)  time: 11.6445  data: 0.0178\n",
      "Epoch: [8]  [26/40]  eta: 0:02:43  lr: 1.0000000000000006e-12  img/s: 11.03745920657647  loss: 0.3680 (0.3907)  acc1: 90.6250 (89.8727)  acc5: 99.2188 (99.3634)  time: 11.6380  data: 0.0178\n",
      "Epoch: [8]  [27/40]  eta: 0:02:31  lr: 1.0000000000000006e-12  img/s: 11.047000259018763  loss: 0.3529 (0.3887)  acc1: 91.4062 (89.9275)  acc5: 99.2188 (99.3304)  time: 11.6387  data: 0.0178\n",
      "Epoch: [8]  [28/40]  eta: 0:02:19  lr: 1.0000000000000006e-12  img/s: 10.967252143474465  loss: 0.3518 (0.3861)  acc1: 91.4062 (90.0593)  acc5: 99.2188 (99.3265)  time: 11.6334  data: 0.0178\n",
      "Epoch: [8]  [29/40]  eta: 0:02:08  lr: 1.0000000000000006e-12  img/s: 11.062825499395458  loss: 0.3518 (0.3882)  acc1: 91.4062 (90.0000)  acc5: 99.2188 (99.2969)  time: 11.6279  data: 0.0178\n",
      "Epoch: [8]  [30/40]  eta: 0:01:56  lr: 1.0000000000000006e-12  img/s: 11.047148694549913  loss: 0.3518 (0.3887)  acc1: 91.4062 (89.9950)  acc5: 99.2188 (99.2944)  time: 11.6311  data: 0.0178\n",
      "Epoch: [8]  [31/40]  eta: 0:01:44  lr: 1.0000000000000006e-12  img/s: 10.94343809231332  loss: 0.3518 (0.3898)  acc1: 91.4062 (89.8682)  acc5: 99.2188 (99.3164)  time: 11.6405  data: 0.0178\n",
      "Epoch: [8]  [32/40]  eta: 0:01:33  lr: 1.0000000000000006e-12  img/s: 11.074613189566106  loss: 0.3356 (0.3876)  acc1: 91.4062 (90.0331)  acc5: 99.2188 (99.3371)  time: 11.6339  data: 0.0178\n",
      "Epoch: [8]  [33/40]  eta: 0:01:21  lr: 1.0000000000000006e-12  img/s: 11.051092448045319  loss: 0.3356 (0.3874)  acc1: 92.1875 (90.0965)  acc5: 99.2188 (99.2877)  time: 11.6245  data: 0.0178\n",
      "Epoch: [8]  [34/40]  eta: 0:01:09  lr: 1.0000000000000006e-12  img/s: 11.095528416848914  loss: 0.3356 (0.3887)  acc1: 92.1875 (90.0223)  acc5: 99.2188 (99.3080)  time: 11.6265  data: 0.0178\n",
      "Epoch: [8]  [35/40]  eta: 0:00:58  lr: 1.0000000000000006e-12  img/s: 11.009652598648469  loss: 0.3518 (0.3888)  acc1: 91.4062 (90.0608)  acc5: 99.2188 (99.2839)  time: 11.6292  data: 0.0178\n",
      "Epoch: [8]  [36/40]  eta: 0:00:46  lr: 1.0000000000000006e-12  img/s: 11.07871260571984  loss: 0.3518 (0.3863)  acc1: 91.4062 (90.1394)  acc5: 99.2188 (99.3032)  time: 11.6312  data: 0.0179\n",
      "Epoch: [8]  [37/40]  eta: 0:00:34  lr: 1.0000000000000006e-12  img/s: 11.048819267106044  loss: 0.3529 (0.3891)  acc1: 91.4062 (90.0493)  acc5: 99.2188 (99.2599)  time: 11.6188  data: 0.0179\n",
      "Epoch: [8]  [38/40]  eta: 0:00:23  lr: 1.0000000000000006e-12  img/s: 10.883877353281987  loss: 0.3518 (0.3875)  acc1: 91.4062 (90.0641)  acc5: 99.2188 (99.2788)  time: 11.6217  data: 0.0179\n",
      "Epoch: [8]  [39/40]  eta: 0:00:11  lr: 1.0000000000000006e-12  img/s: 11.464349727522764  loss: 0.3529 (0.4111)  acc1: 91.4062 (90.0200)  acc5: 99.2188 (99.2800)  time: 11.0733  data: 0.0171\n",
      "Epoch: [8] Total time: 0:07:34\n",
      "Epoch 10/15\n",
      "Epoch: [9]  [ 0/40]  eta: 0:07:45  lr: 1.0000000000000007e-13  img/s: 11.019792940644823  loss: 0.4176 (0.4176)  acc1: 89.8438 (89.8438)  acc5: 99.2188 (99.2188)  time: 11.6335  data: 0.0180\n",
      "Epoch: [9]  [ 1/40]  eta: 0:07:35  lr: 1.0000000000000007e-13  img/s: 10.954011023649745  loss: 0.3474 (0.3825)  acc1: 89.8438 (90.2344)  acc5: 99.2188 (99.6094)  time: 11.6683  data: 0.0179\n",
      "Epoch: [9]  [ 2/40]  eta: 0:07:24  lr: 1.0000000000000007e-13  img/s: 10.93193051696559  loss: 0.4176 (0.4018)  acc1: 89.8438 (88.8021)  acc5: 99.2188 (99.2188)  time: 11.6877  data: 0.0179\n",
      "Epoch: [9]  [ 3/40]  eta: 0:07:10  lr: 1.0000000000000007e-13  img/s: 11.1286727724083  loss: 0.4176 (0.4544)  acc1: 85.9375 (87.1094)  acc5: 99.2188 (99.4141)  time: 11.6457  data: 0.0179\n",
      "Epoch: [9]  [ 4/40]  eta: 0:06:59  lr: 1.0000000000000007e-13  img/s: 10.955314851157281  loss: 0.4176 (0.4369)  acc1: 89.8438 (87.6562)  acc5: 100.0000 (99.5312)  time: 11.6569  data: 0.0179\n",
      "Epoch: [9]  [ 5/40]  eta: 0:06:47  lr: 1.0000000000000007e-13  img/s: 11.089924349769674  loss: 0.4176 (0.4547)  acc1: 85.9375 (86.4583)  acc5: 100.0000 (99.6094)  time: 11.6407  data: 0.0179\n",
      "Epoch: [9]  [ 6/40]  eta: 0:06:36  lr: 1.0000000000000007e-13  img/s: 10.947501672903146  loss: 0.4176 (0.4307)  acc1: 89.8438 (87.7232)  acc5: 100.0000 (99.5536)  time: 11.6506  data: 0.0178\n",
      "Epoch: [9]  [ 7/40]  eta: 0:06:23  lr: 1.0000000000000007e-13  img/s: 11.146262710365706  loss: 0.4176 (0.4330)  acc1: 89.0625 (87.8906)  acc5: 99.2188 (99.3164)  time: 11.6320  data: 0.0178\n",
      "Epoch: [9]  [ 8/40]  eta: 0:06:12  lr: 1.0000000000000007e-13  img/s: 11.063709606256088  loss: 0.4176 (0.4246)  acc1: 89.8438 (88.2812)  acc5: 99.2188 (99.3056)  time: 11.6270  data: 0.0179\n",
      "Epoch: [9]  [ 9/40]  eta: 0:06:00  lr: 1.0000000000000007e-13  img/s: 11.074902412219268  loss: 0.3911 (0.4213)  acc1: 89.8438 (88.5156)  acc5: 99.2188 (99.2969)  time: 11.6219  data: 0.0179\n",
      "Epoch: [9]  [10/40]  eta: 0:05:48  lr: 1.0000000000000007e-13  img/s: 11.106978744509904  loss: 0.4176 (0.4310)  acc1: 89.8438 (88.1392)  acc5: 99.2188 (99.2898)  time: 11.6146  data: 0.0179\n",
      "Epoch: [9]  [11/40]  eta: 0:05:36  lr: 1.0000000000000007e-13  img/s: 11.025458565533741  loss: 0.4046 (0.4288)  acc1: 89.0625 (88.1510)  acc5: 99.2188 (99.2839)  time: 11.6157  data: 0.0179\n",
      "Epoch: [9]  [12/40]  eta: 0:05:24  lr: 1.0000000000000007e-13  img/s: 11.16044926599388  loss: 0.4046 (0.4245)  acc1: 89.8438 (88.3413)  acc5: 99.2188 (99.3389)  time: 11.6058  data: 0.0179\n",
      "Epoch: [9]  [13/40]  eta: 0:05:13  lr: 1.0000000000000007e-13  img/s: 11.17181900489512  loss: 0.4046 (0.4247)  acc1: 89.8438 (88.4487)  acc5: 99.2188 (99.2746)  time: 11.5965  data: 0.0178\n",
      "Epoch: [9]  [14/40]  eta: 0:05:01  lr: 1.0000000000000007e-13  img/s: 11.20250338706813  loss: 0.4176 (0.4243)  acc1: 89.8438 (88.5938)  acc5: 99.2188 (99.2708)  time: 11.5863  data: 0.0178\n",
      "Epoch: [9]  [15/40]  eta: 0:04:49  lr: 1.0000000000000007e-13  img/s: 11.208517381460792  loss: 0.4046 (0.4163)  acc1: 89.8438 (88.9648)  acc5: 99.2188 (99.2676)  time: 11.5770  data: 0.0178\n",
      "Epoch: [9]  [16/40]  eta: 0:04:37  lr: 1.0000000000000007e-13  img/s: 11.010157230651666  loss: 0.4046 (0.4082)  acc1: 89.8438 (89.2004)  acc5: 99.2188 (99.3107)  time: 11.5809  data: 0.0178\n",
      "Epoch: [9]  [17/40]  eta: 0:04:26  lr: 1.0000000000000007e-13  img/s: 11.132268216064876  loss: 0.3911 (0.4031)  acc1: 89.8438 (89.4097)  acc5: 99.2188 (99.3490)  time: 11.5773  data: 0.0178\n",
      "Epoch: [9]  [18/40]  eta: 0:04:14  lr: 1.0000000000000007e-13  img/s: 11.070768146934066  loss: 0.4046 (0.4052)  acc1: 90.6250 (89.5559)  acc5: 99.2188 (99.3010)  time: 11.5774  data: 0.0178\n",
      "Epoch: [9]  [19/40]  eta: 0:04:03  lr: 1.0000000000000007e-13  img/s: 11.060462951086453  loss: 0.3911 (0.4014)  acc1: 90.6250 (89.7656)  acc5: 99.2188 (99.3359)  time: 11.5781  data: 0.0178\n",
      "Epoch: [9]  [20/40]  eta: 0:03:51  lr: 1.0000000000000007e-13  img/s: 10.964772132276424  loss: 0.3911 (0.4027)  acc1: 90.6250 (89.6577)  acc5: 99.2188 (99.3304)  time: 11.5810  data: 0.0178\n",
      "Epoch: [9]  [21/40]  eta: 0:03:40  lr: 1.0000000000000007e-13  img/s: 11.06591022774708  loss: 0.3911 (0.3976)  acc1: 90.6250 (89.8438)  acc5: 99.2188 (99.3608)  time: 11.5751  data: 0.0178\n",
      "Epoch: [9]  [22/40]  eta: 0:03:28  lr: 1.0000000000000007e-13  img/s: 10.915859134317135  loss: 0.3911 (0.3996)  acc1: 90.6250 (89.7758)  acc5: 99.2188 (99.3546)  time: 11.5759  data: 0.0178\n",
      "Epoch: [9]  [23/40]  eta: 0:03:17  lr: 1.0000000000000007e-13  img/s: 11.1051269847715  loss: 0.3728 (0.3954)  acc1: 90.6250 (89.9414)  acc5: 99.2188 (99.3490)  time: 11.5772  data: 0.0178\n",
      "Epoch: [9]  [24/40]  eta: 0:03:05  lr: 1.0000000000000007e-13  img/s: 11.089211498439576  loss: 0.3728 (0.3943)  acc1: 90.6250 (90.0312)  acc5: 99.2188 (99.3750)  time: 11.5701  data: 0.0178\n",
      "Epoch: [9]  [25/40]  eta: 0:02:53  lr: 1.0000000000000007e-13  img/s: 11.100701116833632  loss: 0.3728 (0.3944)  acc1: 90.6250 (89.9639)  acc5: 99.2188 (99.3690)  time: 11.5695  data: 0.0178\n",
      "Epoch: [9]  [26/40]  eta: 0:02:42  lr: 1.0000000000000007e-13  img/s: 11.107217266644406  loss: 0.3728 (0.3921)  acc1: 90.6250 (90.0463)  acc5: 99.2188 (99.3634)  time: 11.5611  data: 0.0178\n",
      "Epoch: [9]  [27/40]  eta: 0:02:30  lr: 1.0000000000000007e-13  img/s: 10.913373025416568  loss: 0.3673 (0.3897)  acc1: 91.4062 (90.0949)  acc5: 99.2188 (99.3862)  time: 11.5734  data: 0.0178\n",
      "Epoch: [9]  [28/40]  eta: 0:02:19  lr: 1.0000000000000007e-13  img/s: 11.00530722746962  loss: 0.3728 (0.3894)  acc1: 90.6250 (90.0593)  acc5: 99.2188 (99.3804)  time: 11.5764  data: 0.0178\n",
      "Epoch: [9]  [29/40]  eta: 0:02:07  lr: 1.0000000000000007e-13  img/s: 11.0307694188479  loss: 0.3728 (0.3939)  acc1: 90.6250 (89.9740)  acc5: 99.2188 (99.3490)  time: 11.5787  data: 0.0178\n",
      "Epoch: [9]  [30/40]  eta: 0:01:55  lr: 1.0000000000000007e-13  img/s: 11.031263068743122  loss: 0.3728 (0.3933)  acc1: 90.6250 (89.9446)  acc5: 99.2188 (99.3196)  time: 11.5827  data: 0.0178\n",
      "Epoch: [9]  [31/40]  eta: 0:01:44  lr: 1.0000000000000007e-13  img/s: 11.065262036675014  loss: 0.3728 (0.3941)  acc1: 90.6250 (89.8682)  acc5: 99.2188 (99.3408)  time: 11.5806  data: 0.0178\n",
      "Epoch: [9]  [32/40]  eta: 0:01:32  lr: 1.0000000000000007e-13  img/s: 10.991116510222946  loss: 0.3673 (0.3919)  acc1: 91.4062 (90.0095)  acc5: 99.2188 (99.3608)  time: 11.5894  data: 0.0178\n",
      "Epoch: [9]  [33/40]  eta: 0:01:21  lr: 1.0000000000000007e-13  img/s: 10.897478884204146  loss: 0.3673 (0.3916)  acc1: 91.4062 (90.0276)  acc5: 99.2188 (99.3796)  time: 11.6039  data: 0.0178\n",
      "Epoch: [9]  [34/40]  eta: 0:01:09  lr: 1.0000000000000007e-13  img/s: 10.88339879166848  loss: 0.3673 (0.3920)  acc1: 91.4062 (90.0446)  acc5: 99.2188 (99.3973)  time: 11.6206  data: 0.0178\n",
      "Epoch: [9]  [35/40]  eta: 0:00:58  lr: 1.0000000000000007e-13  img/s: 10.790054282802366  loss: 0.3673 (0.3891)  acc1: 91.4062 (90.1476)  acc5: 100.0000 (99.4141)  time: 11.6428  data: 0.0178\n",
      "Epoch: [9]  [36/40]  eta: 0:00:46  lr: 1.0000000000000007e-13  img/s: 11.008819549048251  loss: 0.3751 (0.3889)  acc1: 90.6250 (90.1394)  acc5: 100.0000 (99.4299)  time: 11.6429  data: 0.0178\n",
      "Epoch: [9]  [37/40]  eta: 0:00:34  lr: 1.0000000000000007e-13  img/s: 10.878436720621828  loss: 0.3752 (0.3886)  acc1: 90.6250 (90.1727)  acc5: 99.2188 (99.4038)  time: 11.6563  data: 0.0178\n",
      "Epoch: [9]  [38/40]  eta: 0:00:23  lr: 1.0000000000000007e-13  img/s: 10.994652201406728  loss: 0.3751 (0.3864)  acc1: 90.6250 (90.1843)  acc5: 100.0000 (99.4191)  time: 11.6603  data: 0.0178\n",
      "Epoch: [9]  [39/40]  eta: 0:00:11  lr: 1.0000000000000007e-13  img/s: 10.327284590679588  loss: 0.3752 (0.4026)  acc1: 90.6250 (90.1600)  acc5: 100.0000 (99.4200)  time: 11.1196  data: 0.0170\n",
      "Epoch: [9] Total time: 0:07:33\n",
      "Epoch 11/15\n",
      "Epoch: [10]  [ 0/40]  eta: 0:07:51  lr: 1.0000000000000008e-14  img/s: 10.882652681359078  loss: 0.4453 (0.4453)  acc1: 86.7188 (86.7188)  acc5: 100.0000 (100.0000)  time: 11.7798  data: 0.0180\n",
      "Epoch: [10]  [ 1/40]  eta: 0:07:41  lr: 1.0000000000000008e-14  img/s: 10.794551859975515  loss: 0.2888 (0.3670)  acc1: 86.7188 (89.4531)  acc5: 100.0000 (100.0000)  time: 11.8278  data: 0.0179\n",
      "Epoch: [10]  [ 2/40]  eta: 0:07:28  lr: 1.0000000000000008e-14  img/s: 10.923739883217287  loss: 0.4365 (0.3902)  acc1: 88.2812 (89.0625)  acc5: 100.0000 (99.7396)  time: 11.7975  data: 0.0184\n",
      "Epoch: [10]  [ 3/40]  eta: 0:07:14  lr: 1.0000000000000008e-14  img/s: 11.059470917886529  loss: 0.4365 (0.4261)  acc1: 86.7188 (88.0859)  acc5: 99.2188 (99.6094)  time: 11.7460  data: 0.0182\n",
      "Epoch: [10]  [ 4/40]  eta: 0:07:02  lr: 1.0000000000000008e-14  img/s: 10.935565882135304  loss: 0.4453 (0.4324)  acc1: 86.7188 (87.8125)  acc5: 100.0000 (99.6875)  time: 11.7413  data: 0.0181\n",
      "Epoch: [10]  [ 5/40]  eta: 0:06:50  lr: 1.0000000000000008e-14  img/s: 11.006536194799635  loss: 0.4453 (0.4506)  acc1: 86.7188 (87.1094)  acc5: 99.2188 (99.6094)  time: 11.7256  data: 0.0180\n",
      "Epoch: [10]  [ 6/40]  eta: 0:06:37  lr: 1.0000000000000008e-14  img/s: 11.080069385754456  loss: 0.4453 (0.4326)  acc1: 86.7188 (87.9464)  acc5: 99.2188 (99.5536)  time: 11.7034  data: 0.0180\n",
      "Epoch: [10]  [ 7/40]  eta: 0:06:26  lr: 1.0000000000000008e-14  img/s: 10.918755401075302  loss: 0.4453 (0.4344)  acc1: 86.7188 (87.9883)  acc5: 99.2188 (99.4141)  time: 11.7081  data: 0.0180\n",
      "Epoch: [10]  [ 8/40]  eta: 0:06:14  lr: 1.0000000000000008e-14  img/s: 11.0336351535495  loss: 0.4453 (0.4298)  acc1: 88.2812 (88.1944)  acc5: 99.2188 (99.4792)  time: 11.6982  data: 0.0180\n",
      "Epoch: [10]  [ 9/40]  eta: 0:06:02  lr: 1.0000000000000008e-14  img/s: 11.051443686169257  loss: 0.4453 (0.4364)  acc1: 86.7188 (87.8125)  acc5: 99.2188 (99.5312)  time: 11.6883  data: 0.0180\n",
      "Epoch: [10]  [10/40]  eta: 0:05:50  lr: 1.0000000000000008e-14  img/s: 11.140572613295584  loss: 0.4466 (0.4384)  acc1: 87.5000 (87.7841)  acc5: 99.2188 (99.5028)  time: 11.6719  data: 0.0179\n",
      "Epoch: [10]  [11/40]  eta: 0:05:38  lr: 1.0000000000000008e-14  img/s: 11.070301543185495  loss: 0.4453 (0.4384)  acc1: 87.5000 (87.8906)  acc5: 99.2188 (99.4792)  time: 11.6642  data: 0.0179\n",
      "Epoch: [10]  [12/40]  eta: 0:05:26  lr: 1.0000000000000008e-14  img/s: 10.963745245118945  loss: 0.4453 (0.4303)  acc1: 88.2812 (88.0409)  acc5: 99.2188 (99.5192)  time: 11.6664  data: 0.0179\n",
      "Epoch: [10]  [13/40]  eta: 0:05:14  lr: 1.0000000000000008e-14  img/s: 11.094584191863508  loss: 0.4385 (0.4281)  acc1: 88.2812 (88.1696)  acc5: 99.2188 (99.5536)  time: 11.6585  data: 0.0179\n",
      "Epoch: [10]  [14/40]  eta: 0:05:02  lr: 1.0000000000000008e-14  img/s: 11.093851714217973  loss: 0.4385 (0.4245)  acc1: 88.2812 (88.4375)  acc5: 99.2188 (99.4792)  time: 11.6516  data: 0.0179\n",
      "Epoch: [10]  [15/40]  eta: 0:04:51  lr: 1.0000000000000008e-14  img/s: 11.091390885159335  loss: 0.4365 (0.4150)  acc1: 88.2812 (88.8184)  acc5: 99.2188 (99.4629)  time: 11.6458  data: 0.0179\n",
      "Epoch: [10]  [16/40]  eta: 0:04:39  lr: 1.0000000000000008e-14  img/s: 11.106447047067263  loss: 0.4365 (0.4112)  acc1: 89.0625 (88.9246)  acc5: 99.2188 (99.4945)  time: 11.6397  data: 0.0179\n",
      "Epoch: [10]  [17/40]  eta: 0:04:27  lr: 1.0000000000000008e-14  img/s: 11.134582333788481  loss: 0.3993 (0.4060)  acc1: 89.0625 (89.1059)  acc5: 99.2188 (99.5226)  time: 11.6327  data: 0.0179\n",
      "Epoch: [10]  [18/40]  eta: 0:04:15  lr: 1.0000000000000008e-14  img/s: 11.051362244253319  loss: 0.4340 (0.4075)  acc1: 89.0625 (89.1036)  acc5: 99.2188 (99.4243)  time: 11.6310  data: 0.0179\n",
      "Epoch: [10]  [19/40]  eta: 0:04:04  lr: 1.0000000000000008e-14  img/s: 11.103165390495224  loss: 0.3993 (0.4002)  acc1: 89.0625 (89.3750)  acc5: 99.2188 (99.4531)  time: 11.6267  data: 0.0179\n",
      "Epoch: [10]  [20/40]  eta: 0:03:52  lr: 1.0000000000000008e-14  img/s: 11.08588874091531  loss: 0.3993 (0.4014)  acc1: 89.0625 (89.3229)  acc5: 99.2188 (99.4420)  time: 11.6159  data: 0.0179\n",
      "Epoch: [10]  [21/40]  eta: 0:03:40  lr: 1.0000000000000008e-14  img/s: 11.08529794699648  loss: 0.3993 (0.3971)  acc1: 89.0625 (89.4531)  acc5: 99.2188 (99.4673)  time: 11.6004  data: 0.0179\n",
      "Epoch: [10]  [22/40]  eta: 0:03:29  lr: 1.0000000000000008e-14  img/s: 10.878053192651622  loss: 0.3939 (0.3970)  acc1: 89.0625 (89.3682)  acc5: 99.2188 (99.4905)  time: 11.6028  data: 0.0178\n",
      "Epoch: [10]  [23/40]  eta: 0:03:17  lr: 1.0000000000000008e-14  img/s: 11.063147846900817  loss: 0.3935 (0.3942)  acc1: 89.8438 (89.5182)  acc5: 100.0000 (99.5117)  time: 11.6026  data: 0.0178\n",
      "Epoch: [10]  [24/40]  eta: 0:03:05  lr: 1.0000000000000008e-14  img/s: 11.123420047946215  loss: 0.3786 (0.3936)  acc1: 89.8438 (89.5312)  acc5: 99.2188 (99.4688)  time: 11.5927  data: 0.0178\n",
      "Epoch: [10]  [25/40]  eta: 0:02:54  lr: 1.0000000000000008e-14  img/s: 11.054054330624892  loss: 0.3786 (0.3944)  acc1: 89.8438 (89.5733)  acc5: 99.2188 (99.4291)  time: 11.5902  data: 0.0178\n",
      "Epoch: [10]  [26/40]  eta: 0:02:42  lr: 1.0000000000000008e-14  img/s: 11.107210372797335  loss: 0.3786 (0.3923)  acc1: 89.8438 (89.7280)  acc5: 100.0000 (99.4502)  time: 11.5888  data: 0.0178\n",
      "Epoch: [10]  [27/40]  eta: 0:02:31  lr: 1.0000000000000008e-14  img/s: 11.072157005217086  loss: 0.3738 (0.3909)  acc1: 89.8438 (89.7321)  acc5: 100.0000 (99.4699)  time: 11.5807  data: 0.0178\n",
      "Epoch: [10]  [28/40]  eta: 0:02:19  lr: 1.0000000000000008e-14  img/s: 11.05193304541306  loss: 0.3551 (0.3897)  acc1: 89.8438 (89.6821)  acc5: 100.0000 (99.4881)  time: 11.5797  data: 0.0178\n",
      "Epoch: [10]  [29/40]  eta: 0:02:07  lr: 1.0000000000000008e-14  img/s: 10.959567802593138  loss: 0.3551 (0.3932)  acc1: 89.8438 (89.6354)  acc5: 100.0000 (99.4792)  time: 11.5846  data: 0.0178\n",
      "Epoch: [10]  [30/40]  eta: 0:01:56  lr: 1.0000000000000008e-14  img/s: 11.052624273771434  loss: 0.3551 (0.3934)  acc1: 89.8438 (89.7177)  acc5: 100.0000 (99.4708)  time: 11.5892  data: 0.0178\n",
      "Epoch: [10]  [31/40]  eta: 0:01:44  lr: 1.0000000000000008e-14  img/s: 10.868325701509079  loss: 0.3551 (0.3946)  acc1: 89.8438 (89.5996)  acc5: 100.0000 (99.4873)  time: 11.5999  data: 0.0178\n",
      "Epoch: [10]  [32/40]  eta: 0:01:32  lr: 1.0000000000000008e-14  img/s: 11.008096997558793  loss: 0.3738 (0.3942)  acc1: 90.6250 (89.6307)  acc5: 100.0000 (99.5028)  time: 11.5975  data: 0.0178\n",
      "Epoch: [10]  [33/40]  eta: 0:01:21  lr: 1.0000000000000008e-14  img/s: 10.917777522765023  loss: 0.3738 (0.3938)  acc1: 90.6250 (89.6140)  acc5: 100.0000 (99.5175)  time: 11.6069  data: 0.0178\n",
      "Epoch: [10]  [34/40]  eta: 0:01:09  lr: 1.0000000000000008e-14  img/s: 11.22585566920117  loss: 0.3776 (0.3966)  acc1: 89.8438 (89.4866)  acc5: 100.0000 (99.5312)  time: 11.6001  data: 0.0178\n",
      "Epoch: [10]  [35/40]  eta: 0:00:58  lr: 1.0000000000000008e-14  img/s: 11.15934968042497  loss: 0.3786 (0.3971)  acc1: 89.8438 (89.5182)  acc5: 100.0000 (99.5443)  time: 11.5966  data: 0.0178\n",
      "Epoch: [10]  [36/40]  eta: 0:00:46  lr: 1.0000000000000008e-14  img/s: 11.163308738622488  loss: 0.3786 (0.3957)  acc1: 89.8438 (89.5693)  acc5: 100.0000 (99.5566)  time: 11.5936  data: 0.0178\n",
      "Epoch: [10]  [37/40]  eta: 0:00:34  lr: 1.0000000000000008e-14  img/s: 11.158851922429951  loss: 0.3826 (0.3962)  acc1: 89.8438 (89.6176)  acc5: 100.0000 (99.5271)  time: 11.5924  data: 0.0178\n",
      "Epoch: [10]  [38/40]  eta: 0:00:23  lr: 1.0000000000000008e-14  img/s: 11.085460460548303  loss: 0.3786 (0.3946)  acc1: 90.6250 (89.6635)  acc5: 100.0000 (99.5393)  time: 11.5906  data: 0.0178\n",
      "Epoch: [10]  [39/40]  eta: 0:00:11  lr: 1.0000000000000008e-14  img/s: 10.474143520202126  loss: 0.3826 (0.4084)  acc1: 89.8438 (89.6400)  acc5: 100.0000 (99.5400)  time: 11.0516  data: 0.0170\n",
      "Epoch: [10] Total time: 0:07:33\n",
      "Epoch 12/15\n",
      "Epoch: [11]  [ 0/40]  eta: 0:07:44  lr: 1.0000000000000009e-15  img/s: 11.046687943684148  loss: 0.4254 (0.4254)  acc1: 88.2812 (88.2812)  acc5: 98.4375 (98.4375)  time: 11.6049  data: 0.0177\n",
      "Epoch: [11]  [ 1/40]  eta: 0:07:34  lr: 1.0000000000000009e-15  img/s: 10.97573750157827  loss: 0.3726 (0.3990)  acc1: 88.2812 (89.4531)  acc5: 98.4375 (99.2188)  time: 11.6424  data: 0.0177\n",
      "Epoch: [11]  [ 2/40]  eta: 0:07:21  lr: 1.0000000000000009e-15  img/s: 11.08504892139046  loss: 0.4254 (0.4208)  acc1: 89.8438 (89.5833)  acc5: 99.2188 (99.2188)  time: 11.6166  data: 0.0178\n",
      "Epoch: [11]  [ 3/40]  eta: 0:07:09  lr: 1.0000000000000009e-15  img/s: 11.068668511558448  loss: 0.4254 (0.4515)  acc1: 88.2812 (88.0859)  acc5: 99.2188 (99.2188)  time: 11.6079  data: 0.0177\n",
      "Epoch: [11]  [ 4/40]  eta: 0:06:58  lr: 1.0000000000000009e-15  img/s: 10.999866248803452  loss: 0.4254 (0.4251)  acc1: 89.8438 (88.7500)  acc5: 99.2188 (99.3750)  time: 11.6172  data: 0.0178\n",
      "Epoch: [11]  [ 5/40]  eta: 0:06:46  lr: 1.0000000000000009e-15  img/s: 11.004085977620383  loss: 0.4254 (0.4520)  acc1: 88.2812 (87.5000)  acc5: 99.2188 (99.3490)  time: 11.6226  data: 0.0178\n",
      "Epoch: [11]  [ 6/40]  eta: 0:06:35  lr: 1.0000000000000009e-15  img/s: 10.956155026503842  loss: 0.4254 (0.4412)  acc1: 88.2812 (87.6116)  acc5: 99.2188 (99.3304)  time: 11.6338  data: 0.0178\n",
      "Epoch: [11]  [ 7/40]  eta: 0:06:23  lr: 1.0000000000000009e-15  img/s: 11.09820054085832  loss: 0.4254 (0.4484)  acc1: 88.2812 (87.2070)  acc5: 99.2188 (99.3164)  time: 11.6235  data: 0.0178\n",
      "Epoch: [11]  [ 8/40]  eta: 0:06:12  lr: 1.0000000000000009e-15  img/s: 10.910695800717345  loss: 0.4254 (0.4398)  acc1: 88.2812 (87.4132)  acc5: 99.2188 (99.3056)  time: 11.6375  data: 0.0178\n",
      "Epoch: [11]  [ 9/40]  eta: 0:06:00  lr: 1.0000000000000009e-15  img/s: 11.020271583434237  loss: 0.4254 (0.4441)  acc1: 88.2812 (87.0312)  acc5: 99.2188 (99.2188)  time: 11.6370  data: 0.0178\n",
      "Epoch: [11]  [10/40]  eta: 0:05:49  lr: 1.0000000000000009e-15  img/s: 10.96671940196622  loss: 0.4643 (0.4461)  acc1: 88.2812 (87.0028)  acc5: 99.2188 (99.2188)  time: 11.6418  data: 0.0178\n",
      "Epoch: [11]  [11/40]  eta: 0:05:37  lr: 1.0000000000000009e-15  img/s: 10.885928430013617  loss: 0.4254 (0.4372)  acc1: 88.2812 (87.3698)  acc5: 99.2188 (99.2188)  time: 11.6530  data: 0.0178\n",
      "Epoch: [11]  [12/40]  eta: 0:05:26  lr: 1.0000000000000009e-15  img/s: 10.878135626841699  loss: 0.4254 (0.4328)  acc1: 88.2812 (87.5601)  acc5: 99.2188 (99.2788)  time: 11.6631  data: 0.0178\n",
      "Epoch: [11]  [13/40]  eta: 0:05:14  lr: 1.0000000000000009e-15  img/s: 11.100494777243345  loss: 0.4254 (0.4365)  acc1: 88.2812 (87.6116)  acc5: 99.2188 (99.2746)  time: 11.6549  data: 0.0178\n",
      "Epoch: [11]  [14/40]  eta: 0:05:02  lr: 1.0000000000000009e-15  img/s: 11.02638222850966  loss: 0.4643 (0.4396)  acc1: 88.2812 (87.3958)  acc5: 99.2188 (99.2708)  time: 11.6530  data: 0.0178\n",
      "Epoch: [11]  [15/40]  eta: 0:04:51  lr: 1.0000000000000009e-15  img/s: 10.982693638765179  loss: 0.4254 (0.4365)  acc1: 88.2812 (87.4023)  acc5: 99.2188 (99.3164)  time: 11.6542  data: 0.0178\n",
      "Epoch: [11]  [16/40]  eta: 0:04:39  lr: 1.0000000000000009e-15  img/s: 10.952653432720838  loss: 0.4254 (0.4284)  acc1: 88.2812 (87.7757)  acc5: 99.2188 (99.3566)  time: 11.6572  data: 0.0178\n",
      "Epoch: [11]  [17/40]  eta: 0:04:28  lr: 1.0000000000000009e-15  img/s: 11.072234643613942  loss: 0.3901 (0.4218)  acc1: 88.2812 (88.0642)  acc5: 99.2188 (99.3924)  time: 11.6528  data: 0.0178\n",
      "Epoch: [11]  [18/40]  eta: 0:04:16  lr: 1.0000000000000009e-15  img/s: 11.016538322547698  loss: 0.4254 (0.4251)  acc1: 88.2812 (88.1168)  acc5: 99.2188 (99.3832)  time: 11.6519  data: 0.0178\n",
      "Epoch: [11]  [19/40]  eta: 0:04:04  lr: 1.0000000000000009e-15  img/s: 10.964846032578569  loss: 0.3901 (0.4185)  acc1: 88.2812 (88.3203)  acc5: 99.2188 (99.4141)  time: 11.6539  data: 0.0178\n",
      "Epoch: [11]  [20/40]  eta: 0:03:52  lr: 1.0000000000000009e-15  img/s: 11.125592613930406  loss: 0.3827 (0.4168)  acc1: 89.0625 (88.4301)  acc5: 99.2188 (99.4420)  time: 11.6498  data: 0.0178\n",
      "Epoch: [11]  [21/40]  eta: 0:03:41  lr: 1.0000000000000009e-15  img/s: 10.993212485534153  loss: 0.3827 (0.4129)  acc1: 89.0625 (88.5653)  acc5: 99.2188 (99.4673)  time: 11.6489  data: 0.0178\n",
      "Epoch: [11]  [22/40]  eta: 0:03:29  lr: 1.0000000000000009e-15  img/s: 11.06060081084839  loss: 0.3808 (0.4107)  acc1: 89.0625 (88.6889)  acc5: 99.2188 (99.4565)  time: 11.6501  data: 0.0178\n",
      "Epoch: [11]  [23/40]  eta: 0:03:17  lr: 1.0000000000000009e-15  img/s: 11.03939016001259  loss: 0.3765 (0.4059)  acc1: 89.0625 (88.8346)  acc5: 99.2188 (99.4792)  time: 11.6517  data: 0.0178\n",
      "Epoch: [11]  [24/40]  eta: 0:03:06  lr: 1.0000000000000009e-15  img/s: 11.106654756863106  loss: 0.3765 (0.4018)  acc1: 89.0625 (89.0625)  acc5: 99.2188 (99.4688)  time: 11.6461  data: 0.0178\n",
      "Epoch: [11]  [25/40]  eta: 0:02:54  lr: 1.0000000000000009e-15  img/s: 11.07379517789606  loss: 0.3765 (0.4039)  acc1: 89.8438 (89.0925)  acc5: 99.2188 (99.4591)  time: 11.6424  data: 0.0178\n",
      "Epoch: [11]  [26/40]  eta: 0:02:42  lr: 1.0000000000000009e-15  img/s: 10.916675510215493  loss: 0.3711 (0.4024)  acc1: 89.8438 (89.1204)  acc5: 99.2188 (99.4213)  time: 11.6445  data: 0.0178\n",
      "Epoch: [11]  [27/40]  eta: 0:02:31  lr: 1.0000000000000009e-15  img/s: 11.048914314865266  loss: 0.3711 (0.4013)  acc1: 89.8438 (89.1462)  acc5: 99.2188 (99.4420)  time: 11.6471  data: 0.0178\n",
      "Epoch: [11]  [28/40]  eta: 0:02:19  lr: 1.0000000000000009e-15  img/s: 11.050889085917836  loss: 0.3638 (0.3994)  acc1: 89.8438 (89.2511)  acc5: 99.2188 (99.4612)  time: 11.6397  data: 0.0178\n",
      "Epoch: [11]  [29/40]  eta: 0:02:08  lr: 1.0000000000000009e-15  img/s: 10.947251879857545  loss: 0.3638 (0.4018)  acc1: 89.8438 (89.2188)  acc5: 99.2188 (99.4531)  time: 11.6435  data: 0.0178\n",
      "Epoch: [11]  [30/40]  eta: 0:01:56  lr: 1.0000000000000009e-15  img/s: 11.031723667164995  loss: 0.3638 (0.4015)  acc1: 89.8438 (89.2389)  acc5: 100.0000 (99.4708)  time: 11.6401  data: 0.0178\n",
      "Epoch: [11]  [31/40]  eta: 0:01:44  lr: 1.0000000000000009e-15  img/s: 11.095067978177925  loss: 0.3732 (0.4027)  acc1: 89.8438 (89.2334)  acc5: 100.0000 (99.4873)  time: 11.6290  data: 0.0177\n",
      "Epoch: [11]  [32/40]  eta: 0:01:33  lr: 1.0000000000000009e-15  img/s: 10.909788978333639  loss: 0.3638 (0.3994)  acc1: 89.8438 (89.2992)  acc5: 100.0000 (99.5028)  time: 11.6273  data: 0.0177\n",
      "Epoch: [11]  [33/40]  eta: 0:01:21  lr: 1.0000000000000009e-15  img/s: 10.95460019990729  loss: 0.3638 (0.3992)  acc1: 90.6250 (89.3382)  acc5: 100.0000 (99.4945)  time: 11.6350  data: 0.0177\n",
      "Epoch: [11]  [34/40]  eta: 0:01:09  lr: 1.0000000000000009e-15  img/s: 11.080666483403002  loss: 0.3638 (0.3994)  acc1: 90.6250 (89.3080)  acc5: 100.0000 (99.5089)  time: 11.6321  data: 0.0177\n",
      "Epoch: [11]  [35/40]  eta: 0:00:58  lr: 1.0000000000000009e-15  img/s: 10.981091740425734  loss: 0.3618 (0.3975)  acc1: 90.6250 (89.3229)  acc5: 100.0000 (99.5226)  time: 11.6322  data: 0.0177\n",
      "Epoch: [11]  [36/40]  eta: 0:00:46  lr: 1.0000000000000009e-15  img/s: 11.154784079446811  loss: 0.3618 (0.3954)  acc1: 90.6250 (89.4426)  acc5: 100.0000 (99.5355)  time: 11.6216  data: 0.0177\n",
      "Epoch: [11]  [37/40]  eta: 0:00:34  lr: 1.0000000000000009e-15  img/s: 11.153318574588903  loss: 0.3638 (0.3957)  acc1: 89.8438 (89.4326)  acc5: 100.0000 (99.5066)  time: 11.6174  data: 0.0177\n",
      "Epoch: [11]  [38/40]  eta: 0:00:23  lr: 1.0000000000000009e-15  img/s: 10.872164366060355  loss: 0.3618 (0.3947)  acc1: 90.6250 (89.4631)  acc5: 100.0000 (99.5192)  time: 11.6251  data: 0.0177\n",
      "Epoch: [11]  [39/40]  eta: 0:00:11  lr: 1.0000000000000009e-15  img/s: 10.43665878188709  loss: 0.3638 (0.4081)  acc1: 89.8438 (89.4400)  acc5: 100.0000 (99.5200)  time: 11.0790  data: 0.0169\n",
      "Epoch: [11] Total time: 0:07:34\n",
      "Epoch 13/15\n",
      "Epoch: [12]  [ 0/40]  eta: 0:07:39  lr: 1.000000000000001e-16  img/s: 11.16776052095298  loss: 0.4364 (0.4364)  acc1: 88.2812 (88.2812)  acc5: 99.2188 (99.2188)  time: 11.4797  data: 0.0181\n",
      "Epoch: [12]  [ 1/40]  eta: 0:07:32  lr: 1.000000000000001e-16  img/s: 10.940649558060622  loss: 0.3385 (0.3874)  acc1: 88.2812 (89.0625)  acc5: 99.2188 (99.6094)  time: 11.5985  data: 0.0180\n",
      "Epoch: [12]  [ 2/40]  eta: 0:07:20  lr: 1.000000000000001e-16  img/s: 11.04631587051897  loss: 0.3908 (0.3886)  acc1: 88.2812 (88.8021)  acc5: 99.2188 (99.2188)  time: 11.6008  data: 0.0179\n",
      "Epoch: [12]  [ 3/40]  eta: 0:07:10  lr: 1.000000000000001e-16  img/s: 10.960832896938292  loss: 0.3908 (0.3989)  acc1: 88.2812 (89.0625)  acc5: 99.2188 (99.2188)  time: 11.6245  data: 0.0179\n",
      "Epoch: [12]  [ 4/40]  eta: 0:06:57  lr: 1.000000000000001e-16  img/s: 11.098281068601802  loss: 0.3908 (0.3965)  acc1: 89.0625 (89.0625)  acc5: 99.2188 (99.3750)  time: 11.6098  data: 0.0178\n",
      "Epoch: [12]  [ 5/40]  eta: 0:06:46  lr: 1.000000000000001e-16  img/s: 11.089748648145473  loss: 0.3908 (0.4271)  acc1: 88.2812 (87.8906)  acc5: 99.2188 (99.3490)  time: 11.6014  data: 0.0178\n",
      "Epoch: [12]  [ 6/40]  eta: 0:06:34  lr: 1.000000000000001e-16  img/s: 11.006723937124727  loss: 0.3908 (0.4127)  acc1: 89.0625 (88.1696)  acc5: 99.2188 (99.3304)  time: 11.6080  data: 0.0178\n",
      "Epoch: [12]  [ 7/40]  eta: 0:06:23  lr: 1.000000000000001e-16  img/s: 11.056881169426259  loss: 0.3908 (0.4126)  acc1: 88.2812 (88.1836)  acc5: 99.2188 (99.1211)  time: 11.6062  data: 0.0177\n",
      "Epoch: [12]  [ 8/40]  eta: 0:06:12  lr: 1.000000000000001e-16  img/s: 10.871254471220283  loss: 0.3908 (0.4042)  acc1: 89.0625 (88.8889)  acc5: 99.2188 (99.1319)  time: 11.6269  data: 0.0177\n",
      "Epoch: [12]  [ 9/40]  eta: 0:06:00  lr: 1.000000000000001e-16  img/s: 11.097648349325024  loss: 0.3908 (0.4109)  acc1: 88.2812 (88.5938)  acc5: 99.2188 (98.9844)  time: 11.6194  data: 0.0177\n",
      "Epoch: [12]  [10/40]  eta: 0:05:48  lr: 1.000000000000001e-16  img/s: 11.013011812274613  loss: 0.4115 (0.4187)  acc1: 88.2812 (88.0682)  acc5: 99.2188 (99.0057)  time: 11.6213  data: 0.0178\n",
      "Epoch: [12]  [11/40]  eta: 0:05:37  lr: 1.000000000000001e-16  img/s: 10.938011972892554  loss: 0.3908 (0.4124)  acc1: 88.2812 (88.4115)  acc5: 99.2188 (99.0885)  time: 11.6295  data: 0.0178\n",
      "Epoch: [12]  [12/40]  eta: 0:05:25  lr: 1.000000000000001e-16  img/s: 10.925371558563155  loss: 0.4115 (0.4148)  acc1: 89.0625 (88.5216)  acc5: 99.2188 (99.1587)  time: 11.6375  data: 0.0178\n",
      "Epoch: [12]  [13/40]  eta: 0:05:14  lr: 1.000000000000001e-16  img/s: 11.037460341165023  loss: 0.4115 (0.4152)  acc1: 88.2812 (88.5045)  acc5: 99.2188 (99.1629)  time: 11.6359  data: 0.0178\n",
      "Epoch: [12]  [14/40]  eta: 0:05:02  lr: 1.000000000000001e-16  img/s: 11.049668842890862  loss: 0.4205 (0.4176)  acc1: 88.2812 (88.4896)  acc5: 99.2188 (99.0625)  time: 11.6336  data: 0.0178\n",
      "Epoch: [12]  [15/40]  eta: 0:04:50  lr: 1.000000000000001e-16  img/s: 11.04401716426647  loss: 0.4115 (0.4169)  acc1: 88.2812 (88.4277)  acc5: 99.2188 (99.0723)  time: 11.6321  data: 0.0178\n",
      "Epoch: [12]  [16/40]  eta: 0:04:39  lr: 1.000000000000001e-16  img/s: 10.942319742571497  loss: 0.4115 (0.4107)  acc1: 88.2812 (88.6489)  acc5: 99.2188 (99.0809)  time: 11.6370  data: 0.0178\n",
      "Epoch: [12]  [17/40]  eta: 0:04:27  lr: 1.000000000000001e-16  img/s: 10.99566619477704  loss: 0.4065 (0.4060)  acc1: 88.2812 (88.8021)  acc5: 99.2188 (99.1319)  time: 11.6382  data: 0.0178\n",
      "Epoch: [12]  [18/40]  eta: 0:04:16  lr: 1.000000000000001e-16  img/s: 10.933285424389435  loss: 0.4065 (0.4044)  acc1: 89.0625 (88.8980)  acc5: 99.2188 (99.1365)  time: 11.6427  data: 0.0178\n",
      "Epoch: [12]  [19/40]  eta: 0:04:04  lr: 1.000000000000001e-16  img/s: 11.106488174828119  loss: 0.3908 (0.3997)  acc1: 89.0625 (89.1406)  acc5: 99.2188 (99.1797)  time: 11.6377  data: 0.0178\n",
      "Epoch: [12]  [20/40]  eta: 0:03:52  lr: 1.000000000000001e-16  img/s: 11.072864925013524  loss: 0.3870 (0.3982)  acc1: 89.8438 (89.1741)  acc5: 99.2188 (99.1815)  time: 11.6426  data: 0.0178\n",
      "Epoch: [12]  [21/40]  eta: 0:03:41  lr: 1.000000000000001e-16  img/s: 10.951053136760423  loss: 0.3870 (0.3973)  acc1: 89.8438 (89.3111)  acc5: 99.2188 (99.1832)  time: 11.6421  data: 0.0178\n",
      "Epoch: [12]  [22/40]  eta: 0:03:29  lr: 1.000000000000001e-16  img/s: 11.112232509062787  loss: 0.3870 (0.3986)  acc1: 89.8438 (89.3342)  acc5: 99.2188 (99.2188)  time: 11.6386  data: 0.0178\n",
      "Epoch: [12]  [23/40]  eta: 0:03:17  lr: 1.000000000000001e-16  img/s: 10.990136877094258  loss: 0.3786 (0.3944)  acc1: 89.8438 (89.4531)  acc5: 99.2188 (99.2513)  time: 11.6371  data: 0.0178\n",
      "Epoch: [12]  [24/40]  eta: 0:03:06  lr: 1.000000000000001e-16  img/s: 10.877273876595718  loss: 0.3745 (0.3918)  acc1: 89.8438 (89.4688)  acc5: 99.2188 (99.2500)  time: 11.6488  data: 0.0178\n",
      "Epoch: [12]  [25/40]  eta: 0:02:54  lr: 1.000000000000001e-16  img/s: 11.090034767459178  loss: 0.3682 (0.3900)  acc1: 89.8438 (89.5132)  acc5: 99.2188 (99.2788)  time: 11.6488  data: 0.0178\n",
      "Epoch: [12]  [26/40]  eta: 0:02:42  lr: 1.000000000000001e-16  img/s: 10.986751355527309  loss: 0.3682 (0.3880)  acc1: 89.8438 (89.6123)  acc5: 99.2188 (99.3056)  time: 11.6498  data: 0.0178\n",
      "Epoch: [12]  [27/40]  eta: 0:02:31  lr: 1.000000000000001e-16  img/s: 10.816343366933575  loss: 0.3682 (0.3891)  acc1: 89.8438 (89.5368)  acc5: 99.2188 (99.3304)  time: 11.6627  data: 0.0178\n",
      "Epoch: [12]  [28/40]  eta: 0:02:19  lr: 1.000000000000001e-16  img/s: 11.04063083853312  loss: 0.3745 (0.3906)  acc1: 89.8438 (89.4666)  acc5: 99.2188 (99.3265)  time: 11.6537  data: 0.0178\n",
      "Epoch: [12]  [29/40]  eta: 0:02:08  lr: 1.000000000000001e-16  img/s: 11.06355297362233  loss: 0.3745 (0.3920)  acc1: 89.8438 (89.3750)  acc5: 99.2188 (99.2969)  time: 11.6555  data: 0.0178\n",
      "Epoch: [12]  [30/40]  eta: 0:01:56  lr: 1.000000000000001e-16  img/s: 11.04337017241118  loss: 0.3682 (0.3903)  acc1: 89.8438 (89.4657)  acc5: 99.2188 (99.2692)  time: 11.6538  data: 0.0178\n",
      "Epoch: [12]  [31/40]  eta: 0:01:44  lr: 1.000000000000001e-16  img/s: 10.83817446183045  loss: 0.3701 (0.3896)  acc1: 89.8438 (89.5020)  acc5: 99.2188 (99.2432)  time: 11.6592  data: 0.0178\n",
      "Epoch: [12]  [32/40]  eta: 0:01:33  lr: 1.000000000000001e-16  img/s: 11.011361534994357  loss: 0.3682 (0.3871)  acc1: 90.6250 (89.6070)  acc5: 99.2188 (99.2661)  time: 11.6546  data: 0.0177\n",
      "Epoch: [12]  [33/40]  eta: 0:01:21  lr: 1.000000000000001e-16  img/s: 11.007546739245097  loss: 0.3453 (0.3856)  acc1: 90.6250 (89.7059)  acc5: 99.2188 (99.2877)  time: 11.6562  data: 0.0177\n",
      "Epoch: [12]  [34/40]  eta: 0:01:09  lr: 1.000000000000001e-16  img/s: 11.00905951761794  loss: 0.3453 (0.3856)  acc1: 90.6250 (89.7768)  acc5: 99.2188 (99.3080)  time: 11.6583  data: 0.0177\n",
      "Epoch: [12]  [35/40]  eta: 0:00:58  lr: 1.000000000000001e-16  img/s: 10.82185097058355  loss: 0.3399 (0.3831)  acc1: 91.4062 (89.8655)  acc5: 100.0000 (99.3273)  time: 11.6702  data: 0.0177\n",
      "Epoch: [12]  [36/40]  eta: 0:00:46  lr: 1.000000000000001e-16  img/s: 11.119597470716842  loss: 0.3453 (0.3831)  acc1: 90.6250 (89.8226)  acc5: 100.0000 (99.3454)  time: 11.6608  data: 0.0177\n",
      "Epoch: [12]  [37/40]  eta: 0:00:34  lr: 1.000000000000001e-16  img/s: 11.045965185527335  loss: 0.3682 (0.3850)  acc1: 90.6250 (89.7204)  acc5: 100.0000 (99.3010)  time: 11.6582  data: 0.0177\n",
      "Epoch: [12]  [38/40]  eta: 0:00:23  lr: 1.000000000000001e-16  img/s: 11.065125885147252  loss: 0.3453 (0.3836)  acc1: 90.6250 (89.8237)  acc5: 100.0000 (99.2788)  time: 11.6512  data: 0.0176\n",
      "Epoch: [12]  [39/40]  eta: 0:00:11  lr: 1.000000000000001e-16  img/s: 11.344817979448813  loss: 0.3682 (0.3898)  acc1: 90.6250 (89.8200)  acc5: 100.0000 (99.2800)  time: 11.1094  data: 0.0168\n",
      "Epoch: [12] Total time: 0:07:34\n",
      "Epoch 14/15\n",
      "Epoch: [13]  [ 0/40]  eta: 0:07:46  lr: 1.000000000000001e-17  img/s: 10.997163770180295  loss: 0.4433 (0.4433)  acc1: 88.2812 (88.2812)  acc5: 98.4375 (98.4375)  time: 11.6566  data: 0.0173\n",
      "Epoch: [13]  [ 1/40]  eta: 0:07:36  lr: 1.000000000000001e-17  img/s: 10.917773304323442  loss: 0.3836 (0.4135)  acc1: 88.2812 (89.4531)  acc5: 98.4375 (99.2188)  time: 11.6992  data: 0.0176\n",
      "Epoch: [13]  [ 2/40]  eta: 0:07:23  lr: 1.000000000000001e-17  img/s: 11.002906039477836  loss: 0.3939 (0.4069)  acc1: 89.8438 (89.5833)  acc5: 100.0000 (99.4792)  time: 11.6832  data: 0.0176\n",
      "Epoch: [13]  [ 3/40]  eta: 0:07:12  lr: 1.000000000000001e-17  img/s: 10.965997438364344  loss: 0.3939 (0.4498)  acc1: 88.2812 (87.8906)  acc5: 99.2188 (99.4141)  time: 11.6849  data: 0.0176\n",
      "Epoch: [13]  [ 4/40]  eta: 0:07:00  lr: 1.000000000000001e-17  img/s: 11.022461293543287  loss: 0.4433 (0.4531)  acc1: 88.2812 (87.5000)  acc5: 100.0000 (99.5312)  time: 11.6740  data: 0.0176\n",
      "Epoch: [13]  [ 5/40]  eta: 0:06:48  lr: 1.000000000000001e-17  img/s: 10.933230429001947  loss: 0.4433 (0.4602)  acc1: 85.9375 (87.2396)  acc5: 99.2188 (99.3490)  time: 11.6825  data: 0.0177\n",
      "Epoch: [13]  [ 6/40]  eta: 0:06:37  lr: 1.000000000000001e-17  img/s: 10.854131412583744  loss: 0.4433 (0.4447)  acc1: 88.2812 (87.6116)  acc5: 100.0000 (99.4420)  time: 11.7008  data: 0.0177\n",
      "Epoch: [13]  [ 7/40]  eta: 0:06:25  lr: 1.000000000000001e-17  img/s: 11.006677677904323  loss: 0.4378 (0.4438)  acc1: 87.5000 (87.5977)  acc5: 99.2188 (99.4141)  time: 11.6941  data: 0.0177\n",
      "Epoch: [13]  [ 8/40]  eta: 0:06:13  lr: 1.000000000000001e-17  img/s: 11.161941244299753  loss: 0.4378 (0.4338)  acc1: 88.2812 (88.0208)  acc5: 99.2188 (99.3924)  time: 11.6709  data: 0.0177\n",
      "Epoch: [13]  [ 9/40]  eta: 0:06:01  lr: 1.000000000000001e-17  img/s: 11.156655220206257  loss: 0.4378 (0.4346)  acc1: 87.5000 (87.7344)  acc5: 99.2188 (99.2969)  time: 11.6529  data: 0.0177\n",
      "Epoch: [13]  [10/40]  eta: 0:05:49  lr: 1.000000000000001e-17  img/s: 11.041682625597307  loss: 0.4424 (0.4465)  acc1: 87.5000 (87.4290)  acc5: 99.2188 (99.3608)  time: 11.6490  data: 0.0177\n",
      "Epoch: [13]  [11/40]  eta: 0:05:38  lr: 1.000000000000001e-17  img/s: 10.869294080766652  loss: 0.4378 (0.4354)  acc1: 87.5000 (88.0208)  acc5: 99.2188 (99.2839)  time: 11.6611  data: 0.0177\n",
      "Epoch: [13]  [12/40]  eta: 0:05:26  lr: 1.000000000000001e-17  img/s: 11.057521546652314  loss: 0.4378 (0.4284)  acc1: 88.2812 (88.1611)  acc5: 99.2188 (99.3389)  time: 11.6560  data: 0.0178\n",
      "Epoch: [13]  [13/40]  eta: 0:05:14  lr: 1.000000000000001e-17  img/s: 11.045590661273147  loss: 0.4378 (0.4319)  acc1: 87.5000 (87.8906)  acc5: 99.2188 (99.2746)  time: 11.6524  data: 0.0178\n",
      "Epoch: [13]  [14/40]  eta: 0:05:03  lr: 1.000000000000001e-17  img/s: 10.939566774982193  loss: 0.4424 (0.4334)  acc1: 88.2812 (87.9688)  acc5: 99.2188 (99.2188)  time: 11.6568  data: 0.0178\n",
      "Epoch: [13]  [15/40]  eta: 0:04:51  lr: 1.000000000000001e-17  img/s: 11.059541088019518  loss: 0.4378 (0.4245)  acc1: 88.2812 (88.3301)  acc5: 99.2188 (99.2676)  time: 11.6527  data: 0.0177\n",
      "Epoch: [13]  [16/40]  eta: 0:04:39  lr: 1.000000000000001e-17  img/s: 11.128098630350964  loss: 0.4378 (0.4158)  acc1: 89.0625 (88.6489)  acc5: 99.2188 (99.3107)  time: 11.6449  data: 0.0177\n",
      "Epoch: [13]  [17/40]  eta: 0:04:27  lr: 1.000000000000001e-17  img/s: 10.989102084607858  loss: 0.3939 (0.4075)  acc1: 89.0625 (89.0191)  acc5: 99.2188 (99.3490)  time: 11.6460  data: 0.0177\n",
      "Epoch: [13]  [18/40]  eta: 0:04:16  lr: 1.000000000000001e-17  img/s: 11.050816068456609  loss: 0.4283 (0.4086)  acc1: 89.8438 (89.0625)  acc5: 99.2188 (99.3421)  time: 11.6436  data: 0.0177\n",
      "Epoch: [13]  [19/40]  eta: 0:04:04  lr: 1.000000000000001e-17  img/s: 11.083036985204366  loss: 0.3939 (0.4045)  acc1: 89.8438 (89.1797)  acc5: 99.2188 (99.3750)  time: 11.6398  data: 0.0177\n",
      "Epoch: [13]  [20/40]  eta: 0:03:52  lr: 1.000000000000001e-17  img/s: 11.04916763195194  loss: 0.3836 (0.4002)  acc1: 89.8438 (89.3973)  acc5: 100.0000 (99.4048)  time: 11.6371  data: 0.0178\n",
      "Epoch: [13]  [21/40]  eta: 0:03:41  lr: 1.000000000000001e-17  img/s: 10.972740062285375  loss: 0.3531 (0.3973)  acc1: 89.8438 (89.5241)  acc5: 100.0000 (99.4318)  time: 11.6342  data: 0.0178\n",
      "Epoch: [13]  [22/40]  eta: 0:03:29  lr: 1.000000000000001e-17  img/s: 11.19296380805385  loss: 0.3531 (0.4023)  acc1: 89.8438 (89.3682)  acc5: 99.2188 (99.4226)  time: 11.6243  data: 0.0178\n",
      "Epoch: [13]  [23/40]  eta: 0:03:17  lr: 1.000000000000001e-17  img/s: 11.184264734744302  loss: 0.3531 (0.4005)  acc1: 89.8438 (89.4857)  acc5: 100.0000 (99.4466)  time: 11.6129  data: 0.0178\n",
      "Epoch: [13]  [24/40]  eta: 0:03:05  lr: 1.000000000000001e-17  img/s: 11.189787557803575  loss: 0.3516 (0.3980)  acc1: 89.8438 (89.5000)  acc5: 100.0000 (99.4688)  time: 11.6042  data: 0.0178\n",
      "Epoch: [13]  [25/40]  eta: 0:02:54  lr: 1.000000000000001e-17  img/s: 11.12223396610168  loss: 0.3516 (0.4003)  acc1: 89.8438 (89.4531)  acc5: 100.0000 (99.4892)  time: 11.5943  data: 0.0178\n",
      "Epoch: [13]  [26/40]  eta: 0:02:42  lr: 1.000000000000001e-17  img/s: 11.079540490113693  loss: 0.3450 (0.3965)  acc1: 89.8438 (89.5544)  acc5: 100.0000 (99.4792)  time: 11.5823  data: 0.0178\n",
      "Epoch: [13]  [27/40]  eta: 0:02:31  lr: 1.000000000000001e-17  img/s: 10.973467175598232  loss: 0.3388 (0.3933)  acc1: 91.4062 (89.6205)  acc5: 100.0000 (99.4978)  time: 11.5840  data: 0.0177\n",
      "Epoch: [13]  [28/40]  eta: 0:02:19  lr: 1.000000000000001e-17  img/s: 10.919459386491642  loss: 0.3388 (0.3925)  acc1: 89.8438 (89.6013)  acc5: 100.0000 (99.5151)  time: 11.5968  data: 0.0178\n",
      "Epoch: [13]  [29/40]  eta: 0:02:07  lr: 1.000000000000001e-17  img/s: 11.110762526857982  loss: 0.3388 (0.3959)  acc1: 89.8438 (89.5052)  acc5: 100.0000 (99.5052)  time: 11.5991  data: 0.0178\n",
      "Epoch: [13]  [30/40]  eta: 0:01:56  lr: 1.000000000000001e-17  img/s: 11.117669669821904  loss: 0.3388 (0.3950)  acc1: 91.4062 (89.5665)  acc5: 100.0000 (99.4456)  time: 11.5952  data: 0.0177\n",
      "Epoch: [13]  [31/40]  eta: 0:01:44  lr: 1.000000000000001e-17  img/s: 11.09471831796528  loss: 0.3450 (0.3938)  acc1: 90.6250 (89.5996)  acc5: 100.0000 (99.4385)  time: 11.5832  data: 0.0177\n",
      "Epoch: [13]  [32/40]  eta: 0:01:32  lr: 1.000000000000001e-17  img/s: 11.058482707151187  loss: 0.3525 (0.3925)  acc1: 91.4062 (89.6544)  acc5: 100.0000 (99.4555)  time: 11.5831  data: 0.0177\n",
      "Epoch: [13]  [33/40]  eta: 0:01:21  lr: 1.000000000000001e-17  img/s: 11.072894157196345  loss: 0.3525 (0.3935)  acc1: 91.4062 (89.6829)  acc5: 100.0000 (99.4715)  time: 11.5817  data: 0.0177\n",
      "Epoch: [13]  [34/40]  eta: 0:01:09  lr: 1.000000000000001e-17  img/s: 11.015547824214996  loss: 0.3525 (0.3945)  acc1: 91.4062 (89.7098)  acc5: 100.0000 (99.4866)  time: 11.5777  data: 0.0177\n",
      "Epoch: [13]  [35/40]  eta: 0:00:58  lr: 1.000000000000001e-17  img/s: 11.014902129682275  loss: 0.3551 (0.3935)  acc1: 91.4062 (89.7569)  acc5: 100.0000 (99.4792)  time: 11.5800  data: 0.0177\n",
      "Epoch: [13]  [36/40]  eta: 0:00:46  lr: 1.000000000000001e-17  img/s: 10.90482186813721  loss: 0.3551 (0.3921)  acc1: 91.4062 (89.8226)  acc5: 100.0000 (99.4721)  time: 11.5918  data: 0.0177\n",
      "Epoch: [13]  [37/40]  eta: 0:00:34  lr: 1.000000000000001e-17  img/s: 10.944620481603256  loss: 0.3596 (0.3926)  acc1: 90.6250 (89.8438)  acc5: 100.0000 (99.4449)  time: 11.5942  data: 0.0177\n",
      "Epoch: [13]  [38/40]  eta: 0:00:23  lr: 1.000000000000001e-17  img/s: 10.902379970863135  loss: 0.3551 (0.3901)  acc1: 91.4062 (89.9239)  acc5: 100.0000 (99.4591)  time: 11.6020  data: 0.0177\n",
      "Epoch: [13]  [39/40]  eta: 0:00:11  lr: 1.000000000000001e-17  img/s: 10.162568103685087  loss: 0.3596 (0.3985)  acc1: 90.6250 (89.9200)  acc5: 100.0000 (99.4600)  time: 11.0632  data: 0.0170\n",
      "Epoch: [13] Total time: 0:07:34\n",
      "Epoch 15/15\n",
      "Epoch: [14]  [ 0/40]  eta: 0:07:48  lr: 1.000000000000001e-18  img/s: 10.951425075047029  loss: 0.3885 (0.3885)  acc1: 90.6250 (90.6250)  acc5: 100.0000 (100.0000)  time: 11.7059  data: 0.0179\n",
      "Epoch: [14]  [ 1/40]  eta: 0:07:39  lr: 1.000000000000001e-18  img/s: 10.812711039544322  loss: 0.3716 (0.3801)  acc1: 90.6250 (90.6250)  acc5: 99.2188 (99.6094)  time: 11.7808  data: 0.0179\n",
      "Epoch: [14]  [ 2/40]  eta: 0:07:24  lr: 1.000000000000001e-18  img/s: 11.081430615277911  loss: 0.3885 (0.3883)  acc1: 90.6250 (89.8438)  acc5: 99.2188 (99.4792)  time: 11.7101  data: 0.0179\n",
      "Epoch: [14]  [ 3/40]  eta: 0:07:13  lr: 1.000000000000001e-18  img/s: 10.90046212017171  loss: 0.3885 (0.4116)  acc1: 89.0625 (89.6484)  acc5: 99.2188 (99.2188)  time: 11.7226  data: 0.0178\n",
      "Epoch: [14]  [ 4/40]  eta: 0:07:01  lr: 1.000000000000001e-18  img/s: 10.960362759687548  loss: 0.3885 (0.4019)  acc1: 90.6250 (89.8438)  acc5: 99.2188 (99.3750)  time: 11.7174  data: 0.0178\n",
      "Epoch: [14]  [ 5/40]  eta: 0:06:49  lr: 1.000000000000001e-18  img/s: 11.101481787961871  loss: 0.3885 (0.4280)  acc1: 89.0625 (88.8021)  acc5: 99.2188 (99.3490)  time: 11.6891  data: 0.0178\n",
      "Epoch: [14]  [ 6/40]  eta: 0:06:37  lr: 1.000000000000001e-18  img/s: 11.031672437289846  loss: 0.3885 (0.4097)  acc1: 90.6250 (89.2857)  acc5: 99.2188 (99.4420)  time: 11.6793  data: 0.0178\n",
      "Epoch: [14]  [ 7/40]  eta: 0:06:24  lr: 1.000000000000001e-18  img/s: 11.087383288848882  loss: 0.3885 (0.4198)  acc1: 89.0625 (88.8672)  acc5: 99.2188 (99.3164)  time: 11.6647  data: 0.0178\n",
      "Epoch: [14]  [ 8/40]  eta: 0:06:14  lr: 1.000000000000001e-18  img/s: 10.784100020806065  loss: 0.3885 (0.4134)  acc1: 89.0625 (88.8889)  acc5: 99.2188 (99.3056)  time: 11.6894  data: 0.0178\n",
      "Epoch: [14]  [ 9/40]  eta: 0:06:03  lr: 1.000000000000001e-18  img/s: 10.76379470364539  loss: 0.3885 (0.4212)  acc1: 89.0625 (88.4375)  acc5: 99.2188 (99.3750)  time: 11.7114  data: 0.0178\n",
      "Epoch: [14]  [10/40]  eta: 0:05:51  lr: 1.000000000000001e-18  img/s: 10.793627566661248  loss: 0.4049 (0.4224)  acc1: 89.0625 (88.1392)  acc5: 99.2188 (99.2898)  time: 11.7264  data: 0.0178\n",
      "Epoch: [14]  [11/40]  eta: 0:05:39  lr: 1.000000000000001e-18  img/s: 10.975229513089626  loss: 0.3885 (0.4185)  acc1: 89.0625 (88.2812)  acc5: 99.2188 (99.2839)  time: 11.7226  data: 0.0178\n",
      "Epoch: [14]  [12/40]  eta: 0:05:28  lr: 1.000000000000001e-18  img/s: 10.921423474846563  loss: 0.3885 (0.4128)  acc1: 89.0625 (88.5817)  acc5: 99.2188 (99.3389)  time: 11.7238  data: 0.0178\n",
      "Epoch: [14]  [13/40]  eta: 0:05:16  lr: 1.000000000000001e-18  img/s: 11.101147561888219  loss: 0.3885 (0.4165)  acc1: 89.0625 (88.6161)  acc5: 99.2188 (99.3862)  time: 11.7112  data: 0.0178\n",
      "Epoch: [14]  [14/40]  eta: 0:05:04  lr: 1.000000000000001e-18  img/s: 10.878786107282638  loss: 0.4049 (0.4187)  acc1: 89.0625 (88.6458)  acc5: 99.2188 (99.3750)  time: 11.7161  data: 0.0178\n",
      "Epoch: [14]  [15/40]  eta: 0:04:52  lr: 1.000000000000001e-18  img/s: 11.08236413816058  loss: 0.3885 (0.4109)  acc1: 89.0625 (88.8184)  acc5: 99.2188 (99.4141)  time: 11.7068  data: 0.0178\n",
      "Epoch: [14]  [16/40]  eta: 0:04:40  lr: 1.000000000000001e-18  img/s: 11.112991109456619  loss: 0.3885 (0.4043)  acc1: 89.0625 (89.0165)  acc5: 99.2188 (99.4485)  time: 11.6967  data: 0.0177\n",
      "Epoch: [14]  [17/40]  eta: 0:04:28  lr: 1.000000000000001e-18  img/s: 11.083071075855397  loss: 0.3755 (0.3984)  acc1: 89.0625 (89.1059)  acc5: 99.2188 (99.4792)  time: 11.6895  data: 0.0177\n",
      "Epoch: [14]  [18/40]  eta: 0:04:17  lr: 1.000000000000001e-18  img/s: 10.902735325466516  loss: 0.3865 (0.3977)  acc1: 89.8438 (89.2270)  acc5: 100.0000 (99.5066)  time: 11.6931  data: 0.0177\n",
      "Epoch: [14]  [19/40]  eta: 0:04:05  lr: 1.000000000000001e-18  img/s: 11.06880315282324  loss: 0.3755 (0.3931)  acc1: 89.8438 (89.3359)  acc5: 100.0000 (99.5312)  time: 11.6875  data: 0.0178\n",
      "Epoch: [14]  [20/40]  eta: 0:03:53  lr: 1.000000000000001e-18  img/s: 10.919181112637181  loss: 0.3716 (0.3915)  acc1: 89.8438 (89.3973)  acc5: 100.0000 (99.5536)  time: 11.6892  data: 0.0177\n",
      "Epoch: [14]  [21/40]  eta: 0:03:41  lr: 1.000000000000001e-18  img/s: 11.216172486961772  loss: 0.3632 (0.3892)  acc1: 89.8438 (89.4531)  acc5: 100.0000 (99.5739)  time: 11.6680  data: 0.0177\n",
      "Epoch: [14]  [22/40]  eta: 0:03:30  lr: 1.000000000000001e-18  img/s: 10.986613307046435  loss: 0.3632 (0.3936)  acc1: 89.8438 (89.3342)  acc5: 100.0000 (99.5584)  time: 11.6729  data: 0.0177\n",
      "Epoch: [14]  [23/40]  eta: 0:03:18  lr: 1.000000000000001e-18  img/s: 11.052956722149107  loss: 0.3632 (0.3938)  acc1: 89.8438 (89.2578)  acc5: 100.0000 (99.5768)  time: 11.6649  data: 0.0177\n",
      "Epoch: [14]  [24/40]  eta: 0:03:06  lr: 1.000000000000001e-18  img/s: 10.912300070404308  loss: 0.3755 (0.3932)  acc1: 89.8438 (89.2812)  acc5: 100.0000 (99.5312)  time: 11.6674  data: 0.0177\n",
      "Epoch: [14]  [25/40]  eta: 0:02:55  lr: 1.000000000000001e-18  img/s: 11.1057107049089  loss: 0.3755 (0.3946)  acc1: 89.8438 (89.2428)  acc5: 100.0000 (99.5493)  time: 11.6672  data: 0.0177\n",
      "Epoch: [14]  [26/40]  eta: 0:02:43  lr: 1.000000000000001e-18  img/s: 10.987606029286862  loss: 0.3799 (0.3946)  acc1: 89.8438 (89.2940)  acc5: 100.0000 (99.5660)  time: 11.6695  data: 0.0177\n",
      "Epoch: [14]  [27/40]  eta: 0:02:31  lr: 1.000000000000001e-18  img/s: 10.933447741937428  loss: 0.3755 (0.3925)  acc1: 89.8438 (89.3973)  acc5: 100.0000 (99.5815)  time: 11.6777  data: 0.0177\n",
      "Epoch: [14]  [28/40]  eta: 0:02:20  lr: 1.000000000000001e-18  img/s: 11.027194835682197  loss: 0.3799 (0.3922)  acc1: 90.6250 (89.4397)  acc5: 100.0000 (99.5959)  time: 11.6646  data: 0.0177\n",
      "Epoch: [14]  [29/40]  eta: 0:02:08  lr: 1.000000000000001e-18  img/s: 10.941236182934908  loss: 0.3799 (0.3958)  acc1: 90.6250 (89.2708)  acc5: 100.0000 (99.6094)  time: 11.6549  data: 0.0177\n",
      "Epoch: [14]  [30/40]  eta: 0:01:56  lr: 1.000000000000001e-18  img/s: 11.08204844754465  loss: 0.3755 (0.3942)  acc1: 90.6250 (89.3901)  acc5: 100.0000 (99.5968)  time: 11.6395  data: 0.0177\n",
      "Epoch: [14]  [31/40]  eta: 0:01:45  lr: 1.000000000000001e-18  img/s: 10.963090162058934  loss: 0.3799 (0.3940)  acc1: 90.6250 (89.3555)  acc5: 100.0000 (99.6094)  time: 11.6401  data: 0.0177\n",
      "Epoch: [14]  [32/40]  eta: 0:01:33  lr: 1.000000000000001e-18  img/s: 10.986191313902621  loss: 0.3799 (0.3929)  acc1: 90.6250 (89.3939)  acc5: 100.0000 (99.6212)  time: 11.6367  data: 0.0177\n",
      "Epoch: [14]  [33/40]  eta: 0:01:21  lr: 1.000000000000001e-18  img/s: 11.101238921137666  loss: 0.3799 (0.3943)  acc1: 90.6250 (89.3382)  acc5: 100.0000 (99.5864)  time: 11.6367  data: 0.0177\n",
      "Epoch: [14]  [34/40]  eta: 0:01:09  lr: 1.000000000000001e-18  img/s: 11.119185695988348  loss: 0.3799 (0.3940)  acc1: 90.6250 (89.3973)  acc5: 100.0000 (99.5536)  time: 11.6240  data: 0.0177\n",
      "Epoch: [14]  [35/40]  eta: 0:00:58  lr: 1.000000000000001e-18  img/s: 11.071443696317912  loss: 0.3799 (0.3919)  acc1: 90.6250 (89.4314)  acc5: 100.0000 (99.5660)  time: 11.6245  data: 0.0177\n",
      "Epoch: [14]  [36/40]  eta: 0:00:46  lr: 1.000000000000001e-18  img/s: 11.176797135970116  loss: 0.3799 (0.3910)  acc1: 90.6250 (89.4637)  acc5: 100.0000 (99.5777)  time: 11.6213  data: 0.0177\n",
      "Epoch: [14]  [37/40]  eta: 0:00:34  lr: 1.000000000000001e-18  img/s: 11.119812351854234  loss: 0.3827 (0.3908)  acc1: 90.6250 (89.5148)  acc5: 100.0000 (99.5271)  time: 11.6193  data: 0.0177\n",
      "Epoch: [14]  [38/40]  eta: 0:00:23  lr: 1.000000000000001e-18  img/s: 11.08922432528876  loss: 0.3799 (0.3895)  acc1: 90.6250 (89.5433)  acc5: 100.0000 (99.5393)  time: 11.6097  data: 0.0177\n",
      "Epoch: [14]  [39/40]  eta: 0:00:11  lr: 1.000000000000001e-18  img/s: 11.33882210774885  loss: 0.3827 (0.4170)  acc1: 90.6250 (89.5000)  acc5: 100.0000 (99.5400)  time: 11.0659  data: 0.0169\n",
      "Epoch: [14] Total time: 0:07:35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [01:53<00:00, 12.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113.51266868971288\n",
      "Accuracy of the network on the 10000 test images: 84.7656 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "axx_mult = 'appro1'\n",
    "model = densenet121(pretrained=True, axx_mult = axx_mult)\n",
    "model.eval() # for evaluation\n",
    "# It is a bit slow since we collect histograms on CPU\n",
    "with torch.no_grad():\n",
    "    stats = collect_stats(model, data_t, num_batches=2)\n",
    "    amax = compute_amax(model, method=\"percentile\", percentile=99.99)\n",
    "    \n",
    "# Inference without retraining\n",
    "import timeit\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "model.eval()\n",
    "start_time = timeit.default_timer()\n",
    "with torch.no_grad():\n",
    "    for iteraction, (images, labels) in tqdm(enumerate(data), total=len(data)):\n",
    "        images, labels = images.to(\"cpu\"), labels.to(\"cpu\")\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(timeit.default_timer() - start_time)\n",
    "print('Accuracy of the network on the 10000 test images: %.4f %%' % (\n",
    "    100 * correct / total))\n",
    "\n",
    "# Retraining for 15 epochs\n",
    "from adapt.references.classification.train import evaluate, train_one_epoch, load_data\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "for epoch in range(15):\n",
    "    print(f\"Epoch {epoch + 1}/15\")\n",
    "    train_one_epoch(model, criterion, optimizer, data_t, \"cpu\", epoch, 1)\n",
    "    lr_scheduler.step()\n",
    "\n",
    "# Inference after retraining\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "model.eval()\n",
    "start_time = timeit.default_timer()\n",
    "with torch.no_grad():\n",
    "    for iteraction, (images, labels) in tqdm(enumerate(data), total=len(data)):\n",
    "        images, labels = images.to(\"cpu\"), labels.to(\"cpu\")\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(timeit.default_timer() - start_time)\n",
    "print('Accuracy of the network on the 10000 test images: %.4f %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b591026",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "Creating extension directory /root/.cache/torch_extensions/PyInit_conv2d_appro2...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/PyInit_conv2d_appro2/build.ninja...\n",
      "Building extension module PyInit_conv2d_appro2...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/2] c++ -MMD -MF axx_conv2d.o.d -DTORCH_EXTENSION_NAME=PyInit_conv2d_appro2 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /root/miniconda3/lib/python3.8/site-packages/torch/include -isystem /root/miniconda3/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -isystem /root/miniconda3/lib/python3.8/site-packages/torch/include/TH -isystem /root/miniconda3/lib/python3.8/site-packages/torch/include/THC -isystem /root/miniconda3/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -DAXX_MULT=appro2 -march=native -fopenmp -O3 -c /root/autodl-tmp/adapt-main/adapt-main/adapt/cpu-kernels/axx_conv2d.cpp -o axx_conv2d.o \n",
      "[2/2] c++ axx_conv2d.o -shared -lgomp -L/root/miniconda3/lib/python3.8/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o PyInit_conv2d_appro2.so\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro2, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [00:05<00:00,  3.00s/it]\n",
      "W0222 04:22:57.087216 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.087850 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.088297 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.088697 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.089094 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.089447 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.089828 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.090205 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.090579 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.090940 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.091270 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.091626 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.092007 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.092411 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.092826 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.093229 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.093641 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.094052 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.094475 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.094885 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.095302 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.095723 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.095972 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.096204 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.096446 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.096673 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.096924 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.097156 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.097414 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.097645 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.097884 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.098109 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.098354 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.098582 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.098815 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.099046 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.099286 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.099510 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.099767 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.099995 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.100231 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.100462 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.100697 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.100920 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.101161 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.101385 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.101619 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.101841 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.102075 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.102298 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.102533 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.102756 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.102993 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.103219 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.103455 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.103695 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.104036 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.104373 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.104720 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.105053 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.105400 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.105731 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.106072 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.106404 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.106750 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.107083 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.107424 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.107769 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.108117 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.108449 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.108796 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.109128 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.109480 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.109813 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.110157 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.110489 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.110836 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.111168 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.111531 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.111845 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.112081 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.112308 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.112543 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.112767 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.113000 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.113224 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.113464 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.113687 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.113918 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.114145 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.114380 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.114610 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.114855 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.115082 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.115324 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.115555 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.115791 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.116014 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.116255 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.116479 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.116712 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.116936 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.117171 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.117398 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.117635 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.117859 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.118095 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.118319 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.118558 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.118786 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.119026 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.119249 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.119484 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.119725 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.119961 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.120184 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.120420 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.120649 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.120893 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.121120 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.121354 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.121578 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.121816 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.122148 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.122494 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.122829 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.123176 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.123520 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.123871 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.124207 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.124557 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.124893 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.125235 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.125574 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.125921 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.126253 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.126595 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.126928 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.127275 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.127621 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.127969 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.128314 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.128665 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.129002 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.129343 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.129678 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.130033 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.130365 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.130706 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.130931 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.131172 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.131398 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.131636 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.131860 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.132095 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.132322 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.132560 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.132788 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.133029 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.133257 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.133491 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.133717 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.133956 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.134183 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.134416 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.134639 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.134875 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.135098 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.135330 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.135555 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.135794 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.136018 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.136250 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.136471 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.136710 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.136937 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.137180 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.137403 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.137633 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.137853 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.138090 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.138314 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.138545 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.138764 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.139106 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.139436 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.139792 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.140127 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.140472 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.140802 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.141143 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.141475 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.141819 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.142422 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.142770 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.143102 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.143448 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.143796 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.144138 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.144471 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.144817 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.145149 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.145492 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.145823 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.146166 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.146500 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.146842 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.147172 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.147528 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.147863 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.148203 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.148535 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.148879 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.149211 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.149558 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.149919 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.150280 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.150620 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.150967 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.151306 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.151668 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.152004 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.152346 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.152681 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.153042 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.153376 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.153720 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.154055 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.154402 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.154737 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.155081 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.155413 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.155783 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.156011 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.156252 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.156481 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.156718 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.156947 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.157178 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.157401 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 04:22:57.170663 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.171358 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.171950 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.172528 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.173076 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.173592 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.174125 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.174630 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.175151 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.175668 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.176403 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.177126 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.177866 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.178570 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.179305 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.180041 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.180782 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.181504 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.182250 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.182968 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.183727 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.184457 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.185193 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.185911 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.186642 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.187359 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.188123 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.188846 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.189586 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.190301 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.191028 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.191754 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.192491 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.193200 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.193921 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.194625 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.195345 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.196071 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.196799 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.197507 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.198234 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.198952 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.199701 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.200420 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.201146 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.201857 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.202572 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.203264 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.204016 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.204459 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.205261 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.205654 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.206082 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.206481 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.206894 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.207299 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.207720 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.209291 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.209974 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.210370 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.211079 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.211454 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.211864 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.212272 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.212983 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.213493 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.215159 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.216056 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.216930 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.217732 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.218355 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.218947 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.219539 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.220072 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.220597 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.221100 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.221625 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.222119 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.222642 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.223132 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.223642 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.224139 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.224642 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.225131 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.225632 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.226297 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.227026 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.227762 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.228493 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.229216 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.229945 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.230660 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.231387 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.232107 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.232833 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.233543 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.234269 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.234976 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.235710 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.236428 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.237148 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.237864 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.238588 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.239305 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.240036 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.240752 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.241472 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.242175 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.242897 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.243616 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.244339 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.245043 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.245751 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.246454 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.247170 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.247899 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.248619 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.249323 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.250040 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.250742 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.251470 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.252198 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.252922 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.253634 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.254348 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.255059 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.255786 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.256507 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.257228 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.257936 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.258656 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.259367 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.260110 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.260827 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.261556 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.262267 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.262984 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.263696 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.264427 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.265138 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.265859 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.266568 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.267289 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.268020 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.268763 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.269472 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.270192 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.270900 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.271628 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.272341 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.273066 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.273777 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.274495 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.275200 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.275952 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.276664 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.277386 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.278111 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.278838 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.279549 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.280279 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.280992 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.281730 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.282438 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.283285 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.284304 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.285199 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.286016 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.286844 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.287655 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.288496 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.289288 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.289766 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.290199 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.290621 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.291847 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.292516 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.292940 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.293368 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.294408 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.294856 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.295280 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.295695 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.296123 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.296538 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.296936 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.297862 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.298245 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.298646 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.299044 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.299438 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.299859 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.300393 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.300921 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.301442 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.301944 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.302473 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.303002 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.303526 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.304027 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.304541 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.305038 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.305550 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.306154 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.306904 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.307662 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.308411 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.309146 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.309892 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.310621 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.311387 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.312135 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.312893 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.313629 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.314381 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.315118 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.315888 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.316630 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.317399 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.318169 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.318744 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.319757 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.320345 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.320926 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.321525 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.322264 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.323649 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.324466 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.325234 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.325969 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.326714 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.327451 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.328226 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.328955 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.329694 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.330414 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.331159 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.331900 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.332669 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 04:22:57.333422 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.conv0.quantizer                : TensorQuantizer(8bit per-tensor amax=2.1255 calibrator=HistogramCalibrator quant)\n",
      "features.conv0.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.1544 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer1.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.5031 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer1.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0649 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer1.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.1361 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer1.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0350 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer2.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.4313 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer2.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0556 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer2.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.1253 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer2.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0295 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer3.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.2479 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer3.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0524 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer3.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.1068 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer3.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0293 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer4.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.2380 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer4.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0360 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer4.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.1344 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer4.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0259 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer5.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.1756 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer5.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0332 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer5.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0792 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer5.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0227 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer6.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.1627 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer6.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0406 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer6.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0921 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer6.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0247 calibrator=HistogramCalibrator quant)\n",
      "features.transition1.conv.quantizer     : TensorQuantizer(8bit per-tensor amax=0.2234 calibrator=HistogramCalibrator quant)\n",
      "features.transition1.conv.quantizer_w   : TensorQuantizer(8bit per-tensor amax=0.0519 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer1.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.1810 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer1.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0308 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer1.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0891 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer1.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0197 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer2.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.1528 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer2.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0267 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer2.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0785 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer2.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0230 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer3.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.1490 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer3.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0284 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer3.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0799 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer3.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0236 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer4.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.1178 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer4.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0266 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer4.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0826 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer4.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0222 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer5.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.1085 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer5.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0265 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer5.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0758 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer5.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0243 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer6.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.1118 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer6.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0289 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer6.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0801 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer6.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0233 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer7.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.1006 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer7.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0265 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer7.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0786 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer7.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0243 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer8.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0987 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer8.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0293 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer8.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0768 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer8.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0282 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer9.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0976 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer9.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0266 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer9.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0744 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer9.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0246 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer10.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0990 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer10.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0283 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer10.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0817 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer10.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0228 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer11.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0972 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer11.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0305 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer11.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0888 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer11.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0218 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer12.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0950 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer12.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0251 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer12.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0828 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer12.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0244 calibrator=HistogramCalibrator quant)\n",
      "features.transition2.conv.quantizer     : TensorQuantizer(8bit per-tensor amax=0.1687 calibrator=HistogramCalibrator quant)\n",
      "features.transition2.conv.quantizer_w   : TensorQuantizer(8bit per-tensor amax=0.0310 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer1.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0700 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer1.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0260 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer1.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0561 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer1.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0208 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer2.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0935 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer2.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0287 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer2.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0608 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer2.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0174 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer3.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0757 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer3.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0251 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer3.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0628 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer3.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0235 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer4.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0826 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer4.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0288 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer4.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0712 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer4.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0217 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer5.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0820 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer5.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0237 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer5.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0670 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer5.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0202 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer6.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0822 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer6.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0209 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer6.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0441 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer6.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0242 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer7.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0728 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer7.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0236 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer7.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0996 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer7.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0146 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer8.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0697 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer8.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0249 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer8.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0748 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer8.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0200 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer9.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0702 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer9.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0181 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer9.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0771 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer9.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0135 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer10.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0696 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer10.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0194 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer10.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0737 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer10.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0137 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer11.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0664 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer11.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0220 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer11.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0652 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer11.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0175 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer12.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0725 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer12.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0202 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer12.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0821 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer12.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0133 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer13.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0626 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer13.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0203 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer13.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0841 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer13.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0139 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer14.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0725 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer14.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0191 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer14.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0850 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer14.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0110 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer15.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0650 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer15.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0187 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer15.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0756 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer15.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0125 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer16.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0596 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer16.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0191 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer16.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0869 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer16.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0150 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer17.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0543 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer17.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0214 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer17.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0920 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer17.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0173 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer18.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0646 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer18.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0196 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer18.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0951 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer18.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0235 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer19.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0606 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer19.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0197 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer19.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0979 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer19.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0163 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer20.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0552 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer20.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0219 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer20.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0918 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer20.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0224 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer21.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0779 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer21.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0192 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer21.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0943 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer21.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0210 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer22.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0790 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer22.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0218 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer22.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0819 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer22.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0200 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer23.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0830 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer23.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0194 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer23.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.1488 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer23.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0256 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer24.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0768 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer24.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0184 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer24.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.1674 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer24.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0201 calibrator=HistogramCalibrator quant)\n",
      "features.transition3.conv.quantizer     : TensorQuantizer(8bit per-tensor amax=0.8334 calibrator=HistogramCalibrator quant)\n",
      "features.transition3.conv.quantizer_w   : TensorQuantizer(8bit per-tensor amax=0.0380 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer1.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0584 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer1.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0102 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer1.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.2481 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer1.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0348 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer2.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0564 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer2.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0118 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer2.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.2577 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer2.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0350 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer3.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0529 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer3.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0116 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer3.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.3204 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer3.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0352 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer4.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0622 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer4.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0117 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer4.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.2853 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer4.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0349 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer5.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0551 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer5.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0119 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer5.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.3230 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer5.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0382 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer6.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0486 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer6.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0104 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer6.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.2774 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer6.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0333 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer7.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0524 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer7.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0112 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer7.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.3402 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer7.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0326 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer8.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0536 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer8.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0100 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer8.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.2887 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer8.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0319 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer9.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0561 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer9.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0100 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer9.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.3530 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer9.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0332 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer10.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0526 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer10.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0108 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer10.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.4279 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer10.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0354 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer11.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0463 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer11.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0092 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer11.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.3235 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer11.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0376 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer12.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0444 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer12.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0095 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer12.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.2720 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer12.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0363 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer13.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0529 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer13.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0101 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer13.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.2978 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer13.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0405 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer14.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0550 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer14.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0102 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer14.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.3351 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer14.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0362 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer15.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0581 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer15.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0090 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer15.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.3223 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer15.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0319 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer16.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0623 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer16.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0108 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer16.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.3536 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer16.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0330 calibrator=HistogramCalibrator quant)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [01:53<00:00, 12.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113.99896082095802\n",
      "Accuracy of the network on the 10000 test images: 10.1128 %\n",
      "Epoch 1/15\n",
      "Epoch: [0]  [ 0/40]  eta: 0:07:50  lr: 0.0001  img/s: 10.90302250433474  loss: 2.3043 (2.3043)  acc1: 10.9375 (10.9375)  acc5: 50.7812 (50.7812)  time: 11.7582  data: 0.0184\n",
      "Epoch: [0]  [ 1/40]  eta: 0:07:40  lr: 0.0001  img/s: 10.814878073823268  loss: 2.2886 (2.2965)  acc1: 6.2500 (8.5938)  acc5: 50.7812 (57.4219)  time: 11.8060  data: 0.0183\n",
      "Epoch: [0]  [ 2/40]  eta: 0:07:28  lr: 0.0001  img/s: 10.90200449359933  loss: 2.3043 (2.3104)  acc1: 10.9375 (9.8958)  acc5: 50.7812 (52.8646)  time: 11.7903  data: 0.0182\n",
      "Epoch: [0]  [ 3/40]  eta: 0:07:16  lr: 0.0001  img/s: 10.873744109702397  loss: 2.3043 (2.3119)  acc1: 8.5938 (9.5703)  acc5: 45.3125 (50.9766)  time: 11.7901  data: 0.0181\n",
      "Epoch: [0]  [ 4/40]  eta: 0:07:05  lr: 0.0001  img/s: 10.753213420128558  loss: 2.3043 (2.3087)  acc1: 10.9375 (10.0000)  acc5: 50.7812 (51.8750)  time: 11.8163  data: 0.0181\n",
      "Epoch: [0]  [ 5/40]  eta: 0:06:53  lr: 0.0001  img/s: 10.909769690595052  loss: 2.3033 (2.3078)  acc1: 10.9375 (10.2865)  acc5: 50.7812 (51.8229)  time: 11.8053  data: 0.0180\n",
      "Epoch: [0]  [ 6/40]  eta: 0:06:40  lr: 0.0001  img/s: 10.955864371172064  loss: 2.3033 (2.3070)  acc1: 10.9375 (10.1562)  acc5: 51.5625 (52.0089)  time: 11.7904  data: 0.0180\n",
      "Epoch: [0]  [ 7/40]  eta: 0:06:28  lr: 0.0001  img/s: 10.934559379269539  loss: 2.3033 (2.3073)  acc1: 9.3750 (9.5703)  acc5: 50.7812 (51.8555)  time: 11.7822  data: 0.0180\n",
      "Epoch: [0]  [ 8/40]  eta: 0:06:17  lr: 0.0001  img/s: 10.77777973176188  loss: 2.3043 (2.3077)  acc1: 10.9375 (10.0694)  acc5: 50.7812 (51.3889)  time: 11.7946  data: 0.0180\n",
      "Epoch: [0]  [ 9/40]  eta: 0:06:05  lr: 0.0001  img/s: 10.991996844026518  loss: 2.3043 (2.3075)  acc1: 10.9375 (10.2344)  acc5: 50.7812 (51.1719)  time: 11.7814  data: 0.0180\n",
      "Epoch: [0]  [10/40]  eta: 0:05:53  lr: 0.0001  img/s: 10.788698650879414  loss: 2.3058 (2.3080)  acc1: 10.9375 (10.2273)  acc5: 50.7812 (50.8523)  time: 11.7906  data: 0.0180\n",
      "Epoch: [0]  [11/40]  eta: 0:05:42  lr: 0.0001  img/s: 10.744607886921454  loss: 2.3044 (2.3077)  acc1: 10.9375 (10.4818)  acc5: 50.7812 (50.9766)  time: 11.8023  data: 0.0180\n",
      "Epoch: [0]  [12/40]  eta: 0:05:30  lr: 0.0001  img/s: 10.74968771856822  loss: 2.3058 (2.3087)  acc1: 10.9375 (10.3365)  acc5: 50.7812 (50.8413)  time: 11.8118  data: 0.0180\n",
      "Epoch: [0]  [13/40]  eta: 0:05:19  lr: 0.0001  img/s: 10.765514510389796  loss: 2.3058 (2.3103)  acc1: 10.1562 (10.2121)  acc5: 49.2188 (50.3906)  time: 11.8186  data: 0.0180\n",
      "Epoch: [0]  [14/40]  eta: 0:05:07  lr: 0.0001  img/s: 10.774308826965  loss: 2.3092 (2.3111)  acc1: 10.1562 (10.1562)  acc5: 49.2188 (50.1042)  time: 11.8239  data: 0.0180\n",
      "Epoch: [0]  [15/40]  eta: 0:04:55  lr: 0.0001  img/s: 10.878518278906927  loss: 2.3092 (2.3110)  acc1: 10.1562 (10.1562)  acc5: 49.2188 (50.5371)  time: 11.8214  data: 0.0180\n",
      "Epoch: [0]  [16/40]  eta: 0:04:43  lr: 0.0001  img/s: 10.94268640405677  loss: 2.3092 (2.3107)  acc1: 10.1562 (10.1103)  acc5: 50.7812 (50.5974)  time: 11.8152  data: 0.0180\n",
      "Epoch: [0]  [17/40]  eta: 0:04:31  lr: 0.0001  img/s: 10.981191690962174  loss: 2.3058 (2.3103)  acc1: 10.1562 (10.1997)  acc5: 50.7812 (50.6076)  time: 11.8074  data: 0.0180\n",
      "Epoch: [0]  [18/40]  eta: 0:04:19  lr: 0.0001  img/s: 10.95826944915516  loss: 2.3092 (2.3117)  acc1: 10.1562 (10.0329)  acc5: 50.7812 (50.2878)  time: 11.8016  data: 0.0180\n",
      "Epoch: [0]  [19/40]  eta: 0:04:07  lr: 0.0001  img/s: 10.782329012449233  loss: 2.3092 (2.3120)  acc1: 9.3750 (9.9609)  acc5: 49.2188 (50.0391)  time: 11.8060  data: 0.0180\n",
      "Epoch: [0]  [20/40]  eta: 0:03:55  lr: 0.0001  img/s: 11.072117958081936  loss: 2.3102 (2.3124)  acc1: 9.3750 (10.0074)  acc5: 49.2188 (49.9628)  time: 11.7970  data: 0.0180\n",
      "Epoch: [0]  [21/40]  eta: 0:03:44  lr: 0.0001  img/s: 10.966050075997476  loss: 2.3102 (2.3118)  acc1: 10.1562 (10.0852)  acc5: 49.2188 (50.0710)  time: 11.7889  data: 0.0180\n",
      "Epoch: [0]  [22/40]  eta: 0:03:32  lr: 0.0001  img/s: 11.082290017878567  loss: 2.3102 (2.3128)  acc1: 9.3750 (9.9864)  acc5: 49.2188 (49.6943)  time: 11.7793  data: 0.0180\n",
      "Epoch: [0]  [23/40]  eta: 0:03:20  lr: 0.0001  img/s: 10.963057477121469  loss: 2.3102 (2.3132)  acc1: 10.1562 (10.0260)  acc5: 49.2188 (49.6094)  time: 11.7745  data: 0.0179\n",
      "Epoch: [0]  [24/40]  eta: 0:03:08  lr: 0.0001  img/s: 10.912040348078191  loss: 2.3102 (2.3129)  acc1: 9.3750 (9.9375)  acc5: 49.2188 (49.7188)  time: 11.7658  data: 0.0179\n",
      "Epoch: [0]  [25/40]  eta: 0:02:56  lr: 0.0001  img/s: 10.99961406042789  loss: 2.3110 (2.3128)  acc1: 9.3750 (10.0060)  acc5: 49.2188 (49.7296)  time: 11.7611  data: 0.0179\n",
      "Epoch: [0]  [26/40]  eta: 0:02:44  lr: 0.0001  img/s: 10.84156031692351  loss: 2.3110 (2.3127)  acc1: 10.1562 (10.0405)  acc5: 49.2188 (49.7396)  time: 11.7672  data: 0.0179\n",
      "Epoch: [0]  [27/40]  eta: 0:02:33  lr: 0.0001  img/s: 10.76752747708417  loss: 2.3110 (2.3125)  acc1: 10.1562 (10.1004)  acc5: 49.2188 (49.7489)  time: 11.7763  data: 0.0179\n",
      "Epoch: [0]  [28/40]  eta: 0:02:21  lr: 0.0001  img/s: 10.98787993077895  loss: 2.3119 (2.3128)  acc1: 10.1562 (10.2101)  acc5: 49.2188 (49.7037)  time: 11.7649  data: 0.0179\n",
      "Epoch: [0]  [29/40]  eta: 0:02:09  lr: 0.0001  img/s: 10.87173284601139  loss: 2.3119 (2.3121)  acc1: 10.1562 (10.3385)  acc5: 49.2188 (49.8698)  time: 11.7714  data: 0.0179\n",
      "Epoch: [0]  [30/40]  eta: 0:01:57  lr: 0.0001  img/s: 11.025223541895768  loss: 2.3119 (2.3124)  acc1: 10.1562 (10.2823)  acc5: 49.2188 (49.6976)  time: 11.7586  data: 0.0179\n",
      "Epoch: [0]  [31/40]  eta: 0:01:45  lr: 0.0001  img/s: 10.877413157921609  loss: 2.3153 (2.3125)  acc1: 10.1562 (10.3760)  acc5: 49.2188 (49.7803)  time: 11.7513  data: 0.0179\n",
      "Epoch: [0]  [32/40]  eta: 0:01:34  lr: 0.0001  img/s: 10.955888740911416  loss: 2.3153 (2.3129)  acc1: 10.1562 (10.3693)  acc5: 48.4375 (49.6212)  time: 11.7401  data: 0.0179\n",
      "Epoch: [0]  [33/40]  eta: 0:01:22  lr: 0.0001  img/s: 11.139087266146634  loss: 2.3141 (2.3130)  acc1: 10.9375 (10.4090)  acc5: 48.4375 (49.5864)  time: 11.7201  data: 0.0178\n",
      "Epoch: [0]  [34/40]  eta: 0:01:10  lr: 0.0001  img/s: 11.298999295126489  loss: 2.3119 (2.3126)  acc1: 10.9375 (10.2902)  acc5: 50.0000 (49.7768)  time: 11.6925  data: 0.0178\n",
      "Epoch: [0]  [35/40]  eta: 0:00:58  lr: 0.0001  img/s: 11.056206029815975  loss: 2.3141 (2.3131)  acc1: 10.9375 (10.1997)  acc5: 48.4375 (49.6311)  time: 11.6831  data: 0.0178\n",
      "Epoch: [0]  [36/40]  eta: 0:00:46  lr: 0.0001  img/s: 11.026072663024204  loss: 2.3141 (2.3128)  acc1: 10.9375 (10.2618)  acc5: 48.4375 (49.7466)  time: 11.6787  data: 0.0178\n",
      "Epoch: [0]  [37/40]  eta: 0:00:35  lr: 0.0001  img/s: 11.285728117734116  loss: 2.3141 (2.3127)  acc1: 10.9375 (10.3002)  acc5: 48.4375 (49.8561)  time: 11.6629  data: 0.0178\n",
      "Epoch: [0]  [38/40]  eta: 0:00:23  lr: 0.0001  img/s: 10.747432482638674  loss: 2.3141 (2.3128)  acc1: 10.9375 (10.2764)  acc5: 48.4375 (49.8197)  time: 11.6744  data: 0.0178\n",
      "Epoch: [0]  [39/40]  eta: 0:00:11  lr: 0.0001  img/s: 10.254677257182674  loss: 2.3119 (2.3117)  acc1: 10.9375 (10.2800)  acc5: 50.0000 (49.8400)  time: 11.1190  data: 0.0170\n",
      "Epoch: [0] Total time: 0:07:38\n",
      "Epoch 2/15\n",
      "Epoch: [1]  [ 0/40]  eta: 0:07:35  lr: 1e-05  img/s: 11.25888403755013  loss: 2.3328 (2.3328)  acc1: 7.8125 (7.8125)  acc5: 44.5312 (44.5312)  time: 11.3867  data: 0.0179\n",
      "Epoch: [1]  [ 1/40]  eta: 0:07:34  lr: 1e-05  img/s: 10.775046426127465  loss: 2.3328 (2.3341)  acc1: 7.0312 (7.4219)  acc5: 43.7500 (44.1406)  time: 11.6419  data: 0.0178\n",
      "Epoch: [1]  [ 2/40]  eta: 0:07:22  lr: 1e-05  img/s: 10.973709418581416  loss: 2.3328 (2.3299)  acc1: 7.8125 (9.1146)  acc5: 44.5312 (45.3125)  time: 11.6553  data: 0.0178\n",
      "Epoch: [1]  [ 3/40]  eta: 0:07:10  lr: 1e-05  img/s: 11.066298220801905  loss: 2.3214 (2.3243)  acc1: 7.8125 (9.3750)  acc5: 44.5312 (46.6797)  time: 11.6376  data: 0.0179\n",
      "Epoch: [1]  [ 4/40]  eta: 0:06:58  lr: 1e-05  img/s: 11.068900142204622  loss: 2.3214 (2.3192)  acc1: 10.1562 (9.8438)  acc5: 47.6562 (48.7500)  time: 11.6265  data: 0.0179\n",
      "Epoch: [1]  [ 5/40]  eta: 0:06:47  lr: 1e-05  img/s: 10.93436006067361  loss: 2.3075 (2.3156)  acc1: 10.1562 (10.5469)  acc5: 47.6562 (50.1302)  time: 11.6427  data: 0.0178\n",
      "Epoch: [1]  [ 6/40]  eta: 0:06:35  lr: 1e-05  img/s: 11.077121659176568  loss: 2.3166 (2.3158)  acc1: 11.7188 (10.9375)  acc5: 47.6562 (49.6652)  time: 11.6328  data: 0.0178\n",
      "Epoch: [1]  [ 7/40]  eta: 0:06:23  lr: 1e-05  img/s: 11.079476468127197  loss: 2.3166 (2.3161)  acc1: 10.9375 (10.9375)  acc5: 47.6562 (50.0000)  time: 11.6251  data: 0.0179\n",
      "Epoch: [1]  [ 8/40]  eta: 0:06:11  lr: 1e-05  img/s: 11.063758626113785  loss: 2.3166 (2.3150)  acc1: 10.9375 (10.5903)  acc5: 50.7812 (50.4340)  time: 11.6208  data: 0.0179\n",
      "Epoch: [1]  [ 9/40]  eta: 0:05:59  lr: 1e-05  img/s: 11.14371381093546  loss: 2.3123 (2.3148)  acc1: 10.9375 (10.7812)  acc5: 50.7812 (50.7812)  time: 11.6092  data: 0.0179\n",
      "Epoch: [1]  [10/40]  eta: 0:05:48  lr: 1e-05  img/s: 11.112725426521292  loss: 2.3123 (2.3139)  acc1: 10.9375 (10.3693)  acc5: 50.7812 (50.7812)  time: 11.6025  data: 0.0178\n",
      "Epoch: [1]  [11/40]  eta: 0:05:36  lr: 1e-05  img/s: 10.928048873028937  loss: 2.3075 (2.3128)  acc1: 10.9375 (10.6120)  acc5: 50.7812 (50.6510)  time: 11.6132  data: 0.0178\n",
      "Epoch: [1]  [12/40]  eta: 0:05:25  lr: 1e-05  img/s: 11.056959504621375  loss: 2.3123 (2.3145)  acc1: 10.9375 (10.6370)  acc5: 50.7812 (50.6010)  time: 11.6117  data: 0.0178\n",
      "Epoch: [1]  [13/40]  eta: 0:05:13  lr: 1e-05  img/s: 10.928781866204147  loss: 2.3123 (2.3171)  acc1: 10.9375 (10.2679)  acc5: 50.0000 (49.9442)  time: 11.6202  data: 0.0178\n",
      "Epoch: [1]  [14/40]  eta: 0:05:02  lr: 1e-05  img/s: 10.989593811694437  loss: 2.3166 (2.3173)  acc1: 10.9375 (10.3125)  acc5: 50.0000 (49.8438)  time: 11.6232  data: 0.0178\n",
      "Epoch: [1]  [15/40]  eta: 0:04:50  lr: 1e-05  img/s: 11.102390911238038  loss: 2.3123 (2.3162)  acc1: 10.9375 (10.3027)  acc5: 50.0000 (50.0977)  time: 11.6184  data: 0.0178\n",
      "Epoch: [1]  [16/40]  eta: 0:04:38  lr: 1e-05  img/s: 11.13135396258381  loss: 2.3160 (2.3162)  acc1: 10.9375 (10.2482)  acc5: 50.0000 (49.9081)  time: 11.6124  data: 0.0178\n",
      "Epoch: [1]  [17/40]  eta: 0:04:27  lr: 1e-05  img/s: 11.084596217835742  loss: 2.3123 (2.3159)  acc1: 10.9375 (10.3733)  acc5: 49.2188 (49.6094)  time: 11.6098  data: 0.0178\n",
      "Epoch: [1]  [18/40]  eta: 0:04:15  lr: 1e-05  img/s: 11.127741580316115  loss: 2.3160 (2.3163)  acc1: 10.9375 (10.3618)  acc5: 49.2188 (49.5888)  time: 11.6051  data: 0.0178\n",
      "Epoch: [1]  [19/40]  eta: 0:04:03  lr: 1e-05  img/s: 11.023711071707325  loss: 2.3160 (2.3170)  acc1: 10.1562 (10.2734)  acc5: 49.2188 (49.2188)  time: 11.6063  data: 0.0178\n",
      "Epoch: [1]  [20/40]  eta: 0:03:52  lr: 1e-05  img/s: 11.02303929769354  loss: 2.3123 (2.3165)  acc1: 10.9375 (10.5655)  acc5: 49.2188 (49.1443)  time: 11.6185  data: 0.0178\n",
      "Epoch: [1]  [21/40]  eta: 0:03:40  lr: 1e-05  img/s: 11.018567795498585  loss: 2.3111 (2.3162)  acc1: 10.9375 (10.5469)  acc5: 49.2188 (49.1477)  time: 11.6053  data: 0.0178\n",
      "Epoch: [1]  [22/40]  eta: 0:03:28  lr: 1e-05  img/s: 11.042895426753281  loss: 2.3111 (2.3165)  acc1: 10.9375 (10.5978)  acc5: 49.2188 (48.9810)  time: 11.6017  data: 0.0178\n",
      "Epoch: [1]  [23/40]  eta: 0:03:17  lr: 1e-05  img/s: 11.104330871970925  loss: 2.3111 (2.3161)  acc1: 10.9375 (10.4492)  acc5: 49.2188 (49.0234)  time: 11.5997  data: 0.0178\n",
      "Epoch: [1]  [24/40]  eta: 0:03:05  lr: 1e-05  img/s: 11.048130333432283  loss: 2.3123 (2.3160)  acc1: 10.9375 (10.5000)  acc5: 49.2188 (49.0625)  time: 11.6008  data: 0.0178\n",
      "Epoch: [1]  [25/40]  eta: 0:02:54  lr: 1e-05  img/s: 11.071058538594297  loss: 2.3123 (2.3155)  acc1: 10.9375 (10.5469)  acc5: 49.2188 (49.0685)  time: 11.5936  data: 0.0178\n",
      "Epoch: [1]  [26/40]  eta: 0:02:42  lr: 1e-05  img/s: 11.108759405479292  loss: 2.3111 (2.3148)  acc1: 10.9375 (10.5613)  acc5: 49.2188 (49.3345)  time: 11.5919  data: 0.0178\n",
      "Epoch: [1]  [27/40]  eta: 0:02:30  lr: 1e-05  img/s: 11.118778326953825  loss: 2.3111 (2.3148)  acc1: 10.1562 (10.5190)  acc5: 49.2188 (49.2467)  time: 11.5899  data: 0.0178\n",
      "Epoch: [1]  [28/40]  eta: 0:02:19  lr: 1e-05  img/s: 11.021307503359177  loss: 2.3123 (2.3152)  acc1: 10.1562 (10.4795)  acc5: 49.2188 (49.1918)  time: 11.5921  data: 0.0178\n",
      "Epoch: [1]  [29/40]  eta: 0:02:07  lr: 1e-05  img/s: 11.057067217327052  loss: 2.3111 (2.3144)  acc1: 10.1562 (10.6510)  acc5: 49.2188 (49.4010)  time: 11.5966  data: 0.0178\n",
      "Epoch: [1]  [30/40]  eta: 0:01:56  lr: 1e-05  img/s: 11.024856535619628  loss: 2.3126 (2.3146)  acc1: 10.1562 (10.6351)  acc5: 49.2188 (49.3952)  time: 11.6012  data: 0.0178\n",
      "Epoch: [1]  [31/40]  eta: 0:01:44  lr: 1e-05  img/s: 11.074977576029488  loss: 2.3144 (2.3147)  acc1: 10.1562 (10.6934)  acc5: 48.4375 (49.2920)  time: 11.5934  data: 0.0178\n",
      "Epoch: [1]  [32/40]  eta: 0:01:32  lr: 1e-05  img/s: 10.833914398802115  loss: 2.3144 (2.3148)  acc1: 10.1562 (10.5587)  acc5: 47.6562 (49.1477)  time: 11.6053  data: 0.0178\n",
      "Epoch: [1]  [33/40]  eta: 0:01:21  lr: 1e-05  img/s: 11.029936343197406  loss: 2.3126 (2.3144)  acc1: 10.1562 (10.5469)  acc5: 48.4375 (49.2647)  time: 11.6000  data: 0.0178\n",
      "Epoch: [1]  [34/40]  eta: 0:01:09  lr: 1e-05  img/s: 11.0000995164427  loss: 2.3111 (2.3139)  acc1: 10.1562 (10.6473)  acc5: 49.2188 (49.3973)  time: 11.5994  data: 0.0178\n",
      "Epoch: [1]  [35/40]  eta: 0:00:58  lr: 1e-05  img/s: 10.915860022098444  loss: 2.3126 (2.3141)  acc1: 10.1562 (10.5469)  acc5: 47.6562 (49.2405)  time: 11.6093  data: 0.0178\n",
      "Epoch: [1]  [36/40]  eta: 0:00:46  lr: 1e-05  img/s: 10.829818249303054  loss: 2.3111 (2.3139)  acc1: 10.1562 (10.5152)  acc5: 49.2188 (49.3666)  time: 11.6253  data: 0.0178\n",
      "Epoch: [1]  [37/40]  eta: 0:00:34  lr: 1e-05  img/s: 11.00747903283529  loss: 2.3098 (2.3137)  acc1: 10.1562 (10.4441)  acc5: 49.2188 (49.4655)  time: 11.6293  data: 0.0178\n",
      "Epoch: [1]  [38/40]  eta: 0:00:23  lr: 1e-05  img/s: 10.81131944652953  loss: 2.3068 (2.3135)  acc1: 10.1562 (10.4567)  acc5: 49.2188 (49.6595)  time: 11.6462  data: 0.0178\n",
      "Epoch: [1]  [39/40]  eta: 0:00:11  lr: 1e-05  img/s: 10.178743644813318  loss: 2.3060 (2.3123)  acc1: 10.1562 (10.5000)  acc5: 49.2188 (49.6600)  time: 11.1041  data: 0.0170\n",
      "Epoch: [1] Total time: 0:07:34\n",
      "Epoch 3/15\n",
      "Epoch: [2]  [ 0/40]  eta: 0:07:44  lr: 1.0000000000000002e-06  img/s: 11.048001196352015  loss: 2.3104 (2.3104)  acc1: 7.8125 (7.8125)  acc5: 53.1250 (53.1250)  time: 11.6038  data: 0.0180\n",
      "Epoch: [2]  [ 1/40]  eta: 0:07:36  lr: 1.0000000000000002e-06  img/s: 10.867917586073348  loss: 2.3104 (2.3134)  acc1: 7.8125 (9.7656)  acc5: 49.2188 (51.1719)  time: 11.6998  data: 0.0180\n",
      "Epoch: [2]  [ 2/40]  eta: 0:07:24  lr: 1.0000000000000002e-06  img/s: 11.00058952339213  loss: 2.3163 (2.3173)  acc1: 9.3750 (9.6354)  acc5: 49.2188 (49.2188)  time: 11.6844  data: 0.0179\n",
      "Epoch: [2]  [ 3/40]  eta: 0:07:11  lr: 1.0000000000000002e-06  img/s: 11.072021825665582  loss: 2.3163 (2.3206)  acc1: 7.8125 (8.5938)  acc5: 47.6562 (48.8281)  time: 11.6579  data: 0.0179\n",
      "Epoch: [2]  [ 4/40]  eta: 0:07:00  lr: 1.0000000000000002e-06  img/s: 10.955037876407255  loss: 2.3163 (2.3156)  acc1: 8.5938 (8.5938)  acc5: 49.2188 (50.1562)  time: 11.6668  data: 0.0179\n",
      "Epoch: [2]  [ 5/40]  eta: 0:06:47  lr: 1.0000000000000002e-06  img/s: 11.092888521525667  loss: 2.3104 (2.3135)  acc1: 8.5938 (8.5938)  acc5: 49.2188 (50.2604)  time: 11.6484  data: 0.0179\n",
      "Epoch: [2]  [ 6/40]  eta: 0:06:35  lr: 1.0000000000000002e-06  img/s: 11.07733833018719  loss: 2.3104 (2.3106)  acc1: 8.5938 (9.7098)  acc5: 50.7812 (51.3393)  time: 11.6376  data: 0.0179\n",
      "Epoch: [2]  [ 7/40]  eta: 0:06:24  lr: 1.0000000000000002e-06  img/s: 10.940237107692083  loss: 2.3029 (2.3078)  acc1: 8.5938 (9.6680)  acc5: 50.7812 (52.4414)  time: 11.6477  data: 0.0179\n",
      "Epoch: [2]  [ 8/40]  eta: 0:06:12  lr: 1.0000000000000002e-06  img/s: 11.063143515374183  loss: 2.3029 (2.3068)  acc1: 8.5938 (9.5486)  acc5: 53.1250 (52.7778)  time: 11.6410  data: 0.0179\n",
      "Epoch: [2]  [ 9/40]  eta: 0:06:00  lr: 1.0000000000000002e-06  img/s: 10.99934813704402  loss: 2.2988 (2.3057)  acc1: 8.5938 (10.2344)  acc5: 53.1250 (52.8906)  time: 11.6424  data: 0.0179\n",
      "Epoch: [2]  [10/40]  eta: 0:05:49  lr: 1.0000000000000002e-06  img/s: 11.03921900685978  loss: 2.3029 (2.3066)  acc1: 8.5938 (9.9432)  acc5: 53.1250 (52.4148)  time: 11.6397  data: 0.0179\n",
      "Epoch: [2]  [11/40]  eta: 0:05:37  lr: 1.0000000000000002e-06  img/s: 11.102688474832519  loss: 2.2988 (2.3055)  acc1: 8.5938 (9.9609)  acc5: 53.1250 (52.4740)  time: 11.6319  data: 0.0178\n",
      "Epoch: [2]  [12/40]  eta: 0:05:25  lr: 1.0000000000000002e-06  img/s: 10.889931954825977  loss: 2.3029 (2.3060)  acc1: 8.5938 (9.8558)  acc5: 53.1250 (52.4038)  time: 11.6427  data: 0.0178\n",
      "Epoch: [2]  [13/40]  eta: 0:05:14  lr: 1.0000000000000002e-06  img/s: 10.951957895528777  loss: 2.3029 (2.3066)  acc1: 8.5938 (10.1004)  acc5: 51.5625 (52.0089)  time: 11.6472  data: 0.0178\n",
      "Epoch: [2]  [14/40]  eta: 0:05:02  lr: 1.0000000000000002e-06  img/s: 11.093549351808404  loss: 2.3104 (2.3071)  acc1: 8.5938 (10.0000)  acc5: 51.5625 (51.5625)  time: 11.6411  data: 0.0178\n",
      "Epoch: [2]  [15/40]  eta: 0:04:50  lr: 1.0000000000000002e-06  img/s: 11.035808173571814  loss: 2.3104 (2.3075)  acc1: 8.5938 (9.8145)  acc5: 51.5625 (51.5625)  time: 11.6395  data: 0.0178\n",
      "Epoch: [2]  [16/40]  eta: 0:04:39  lr: 1.0000000000000002e-06  img/s: 10.861298518055444  loss: 2.3104 (2.3071)  acc1: 8.5938 (9.7886)  acc5: 51.5625 (51.7004)  time: 11.6491  data: 0.0178\n",
      "Epoch: [2]  [17/40]  eta: 0:04:27  lr: 1.0000000000000002e-06  img/s: 11.105487868650188  loss: 2.3052 (2.3070)  acc1: 8.5938 (9.7656)  acc5: 51.5625 (51.6059)  time: 11.6433  data: 0.0178\n",
      "Epoch: [2]  [18/40]  eta: 0:04:16  lr: 1.0000000000000002e-06  img/s: 10.884179426975393  loss: 2.3104 (2.3083)  acc1: 9.3750 (9.8273)  acc5: 51.5625 (51.3980)  time: 11.6504  data: 0.0178\n",
      "Epoch: [2]  [19/40]  eta: 0:04:04  lr: 1.0000000000000002e-06  img/s: 11.116950484875634  loss: 2.3104 (2.3086)  acc1: 9.3750 (9.8438)  acc5: 51.5625 (51.4062)  time: 11.6444  data: 0.0178\n",
      "Epoch: [2]  [20/40]  eta: 0:03:52  lr: 1.0000000000000002e-06  img/s: 11.164207818899875  loss: 2.3113 (2.3096)  acc1: 9.3750 (9.7470)  acc5: 50.7812 (50.8185)  time: 11.6384  data: 0.0178\n",
      "Epoch: [2]  [21/40]  eta: 0:03:41  lr: 1.0000000000000002e-06  img/s: 10.968695151637402  loss: 2.3052 (2.3092)  acc1: 9.3750 (9.8011)  acc5: 51.5625 (50.8523)  time: 11.6330  data: 0.0178\n",
      "Epoch: [2]  [22/40]  eta: 0:03:29  lr: 1.0000000000000002e-06  img/s: 11.107884631550704  loss: 2.3052 (2.3091)  acc1: 9.3750 (10.0204)  acc5: 51.5625 (50.8152)  time: 11.6274  data: 0.0178\n",
      "Epoch: [2]  [23/40]  eta: 0:03:17  lr: 1.0000000000000002e-06  img/s: 11.059815625830264  loss: 2.3029 (2.3087)  acc1: 9.3750 (9.9935)  acc5: 51.5625 (50.8789)  time: 11.6280  data: 0.0178\n",
      "Epoch: [2]  [24/40]  eta: 0:03:06  lr: 1.0000000000000002e-06  img/s: 11.178078436051921  loss: 2.3052 (2.3092)  acc1: 9.3750 (10.0625)  acc5: 51.5625 (50.9375)  time: 11.6163  data: 0.0178\n",
      "Epoch: [2]  [25/40]  eta: 0:02:54  lr: 1.0000000000000002e-06  img/s: 11.299407610825382  loss: 2.3061 (2.3099)  acc1: 9.3750 (9.8858)  acc5: 51.5625 (50.7512)  time: 11.6058  data: 0.0178\n",
      "Epoch: [2]  [26/40]  eta: 0:02:42  lr: 1.0000000000000002e-06  img/s: 11.34781541555208  loss: 2.3113 (2.3102)  acc1: 9.3750 (9.8958)  acc5: 51.5625 (50.7523)  time: 11.5920  data: 0.0178\n",
      "Epoch: [2]  [27/40]  eta: 0:02:30  lr: 1.0000000000000002e-06  img/s: 11.122195256210613  loss: 2.3133 (2.3108)  acc1: 9.3750 (9.8772)  acc5: 50.7812 (50.5580)  time: 11.5824  data: 0.0178\n",
      "Epoch: [2]  [28/40]  eta: 0:02:19  lr: 1.0000000000000002e-06  img/s: 10.912941333633684  loss: 2.3133 (2.3107)  acc1: 9.3750 (9.8599)  acc5: 50.7812 (50.6466)  time: 11.5904  data: 0.0178\n",
      "Epoch: [2]  [29/40]  eta: 0:02:07  lr: 1.0000000000000002e-06  img/s: 10.839196561440476  loss: 2.3133 (2.3108)  acc1: 9.3750 (10.0521)  acc5: 50.0000 (50.4948)  time: 11.5990  data: 0.0178\n",
      "Epoch: [2]  [30/40]  eta: 0:01:56  lr: 1.0000000000000002e-06  img/s: 11.008203534647015  loss: 2.3133 (2.3110)  acc1: 9.3750 (9.9546)  acc5: 50.0000 (50.4788)  time: 11.6006  data: 0.0178\n",
      "Epoch: [2]  [31/40]  eta: 0:01:44  lr: 1.0000000000000002e-06  img/s: 10.966541982305468  loss: 2.3133 (2.3109)  acc1: 9.3750 (9.9609)  acc5: 50.0000 (50.6104)  time: 11.6078  data: 0.0178\n",
      "Epoch: [2]  [32/40]  eta: 0:01:32  lr: 1.0000000000000002e-06  img/s: 10.93392470286636  loss: 2.3133 (2.3102)  acc1: 9.3750 (10.1089)  acc5: 50.0000 (50.7339)  time: 11.6054  data: 0.0178\n",
      "Epoch: [2]  [33/40]  eta: 0:01:21  lr: 1.0000000000000002e-06  img/s: 10.802883280564892  loss: 2.3133 (2.3105)  acc1: 9.3750 (10.1333)  acc5: 50.0000 (50.6434)  time: 11.6135  data: 0.0178\n",
      "Epoch: [2]  [34/40]  eta: 0:01:09  lr: 1.0000000000000002e-06  img/s: 10.888650047186369  loss: 2.3129 (2.3101)  acc1: 10.1562 (10.2232)  acc5: 50.7812 (50.8259)  time: 11.6243  data: 0.0178\n",
      "Epoch: [2]  [35/40]  eta: 0:00:58  lr: 1.0000000000000002e-06  img/s: 10.826853681372047  loss: 2.3129 (2.3102)  acc1: 10.1562 (10.2431)  acc5: 50.7812 (50.8898)  time: 11.6355  data: 0.0178\n",
      "Epoch: [2]  [36/40]  eta: 0:00:46  lr: 1.0000000000000002e-06  img/s: 10.838953436995467  loss: 2.3129 (2.3095)  acc1: 10.1562 (10.3252)  acc5: 50.7812 (51.1191)  time: 11.6368  data: 0.0178\n",
      "Epoch: [2]  [37/40]  eta: 0:00:34  lr: 1.0000000000000002e-06  img/s: 10.812041871794941  loss: 2.3131 (2.3101)  acc1: 10.1562 (10.1974)  acc5: 50.7812 (51.0074)  time: 11.6524  data: 0.0178\n",
      "Epoch: [2]  [38/40]  eta: 0:00:23  lr: 1.0000000000000002e-06  img/s: 10.938409100776772  loss: 2.3131 (2.3106)  acc1: 10.1562 (10.1963)  acc5: 50.7812 (50.8213)  time: 11.6495  data: 0.0178\n",
      "Epoch: [2]  [39/40]  eta: 0:00:11  lr: 1.0000000000000002e-06  img/s: 11.426107198484257  loss: 2.3129 (2.3094)  acc1: 10.1562 (10.1800)  acc5: 50.7812 (50.8400)  time: 11.1080  data: 0.0170\n",
      "Epoch: [2] Total time: 0:07:35\n",
      "Epoch 4/15\n",
      "Epoch: [3]  [ 0/40]  eta: 0:07:34  lr: 1.0000000000000002e-07  img/s: 11.277727155845733  loss: 2.3078 (2.3078)  acc1: 10.9375 (10.9375)  acc5: 53.1250 (53.1250)  time: 11.3680  data: 0.0182\n",
      "Epoch: [3]  [ 1/40]  eta: 0:07:33  lr: 1.0000000000000002e-07  img/s: 10.783936692029675  loss: 2.3049 (2.3063)  acc1: 7.8125 (9.3750)  acc5: 50.7812 (51.9531)  time: 11.6277  data: 0.0180\n",
      "Epoch: [3]  [ 2/40]  eta: 0:07:22  lr: 1.0000000000000002e-07  img/s: 10.957240648736446  loss: 2.3078 (2.3069)  acc1: 10.1562 (9.6354)  acc5: 51.5625 (51.8229)  time: 11.6516  data: 0.0179\n",
      "Epoch: [3]  [ 3/40]  eta: 0:07:11  lr: 1.0000000000000002e-07  img/s: 11.006478880510132  loss: 2.3078 (2.3083)  acc1: 10.1562 (10.1562)  acc5: 50.7812 (50.7812)  time: 11.6505  data: 0.0179\n",
      "Epoch: [3]  [ 4/40]  eta: 0:06:59  lr: 1.0000000000000002e-07  img/s: 11.050887493626458  loss: 2.3078 (2.3065)  acc1: 10.9375 (10.3125)  acc5: 51.5625 (51.0938)  time: 11.6405  data: 0.0178\n",
      "Epoch: [3]  [ 5/40]  eta: 0:06:46  lr: 1.0000000000000002e-07  img/s: 11.108104116527656  loss: 2.3049 (2.3042)  acc1: 10.9375 (11.5885)  acc5: 50.7812 (50.6510)  time: 11.6239  data: 0.0178\n",
      "Epoch: [3]  [ 6/40]  eta: 0:06:34  lr: 1.0000000000000002e-07  img/s: 11.133741123942901  loss: 2.3078 (2.3053)  acc1: 10.9375 (10.9375)  acc5: 51.5625 (50.8929)  time: 11.6083  data: 0.0178\n",
      "Epoch: [3]  [ 7/40]  eta: 0:06:22  lr: 1.0000000000000002e-07  img/s: 11.089914041176444  loss: 2.3049 (2.3028)  acc1: 10.9375 (11.3281)  acc5: 51.5625 (52.1484)  time: 11.6022  data: 0.0178\n",
      "Epoch: [3]  [ 8/40]  eta: 0:06:10  lr: 1.0000000000000002e-07  img/s: 11.150236107463279  loss: 2.3049 (2.3019)  acc1: 10.9375 (11.2847)  acc5: 52.3438 (52.3438)  time: 11.5905  data: 0.0177\n",
      "Epoch: [3]  [ 9/40]  eta: 0:05:59  lr: 1.0000000000000002e-07  img/s: 10.99487398943923  loss: 2.3049 (2.3025)  acc1: 10.9375 (11.0938)  acc5: 52.3438 (52.3438)  time: 11.5974  data: 0.0177\n",
      "Epoch: [3]  [10/40]  eta: 0:05:48  lr: 1.0000000000000002e-07  img/s: 10.872802241850673  loss: 2.3074 (2.3039)  acc1: 10.9375 (11.0085)  acc5: 52.3438 (51.9176)  time: 11.6149  data: 0.0177\n",
      "Epoch: [3]  [11/40]  eta: 0:05:36  lr: 1.0000000000000002e-07  img/s: 11.04897866631005  loss: 2.3049 (2.3034)  acc1: 10.9375 (11.0677)  acc5: 51.5625 (51.8229)  time: 11.6139  data: 0.0177\n",
      "Epoch: [3]  [12/40]  eta: 0:05:25  lr: 1.0000000000000002e-07  img/s: 10.996520904722079  loss: 2.3074 (2.3049)  acc1: 10.9375 (10.4567)  acc5: 51.5625 (51.5625)  time: 11.6173  data: 0.0177\n",
      "Epoch: [3]  [13/40]  eta: 0:05:13  lr: 1.0000000000000002e-07  img/s: 10.902834740340273  loss: 2.3074 (2.3056)  acc1: 10.1562 (10.2679)  acc5: 50.7812 (51.3951)  time: 11.6273  data: 0.0177\n",
      "Epoch: [3]  [14/40]  eta: 0:05:02  lr: 1.0000000000000002e-07  img/s: 10.928564516242632  loss: 2.3078 (2.3068)  acc1: 10.9375 (10.3125)  acc5: 50.7812 (50.8333)  time: 11.6342  data: 0.0178\n",
      "Epoch: [3]  [15/40]  eta: 0:04:50  lr: 1.0000000000000002e-07  img/s: 11.10682157367384  loss: 2.3078 (2.3076)  acc1: 10.1562 (10.2051)  acc5: 50.7812 (50.5859)  time: 11.6284  data: 0.0178\n",
      "Epoch: [3]  [16/40]  eta: 0:04:39  lr: 1.0000000000000002e-07  img/s: 11.001780237359107  loss: 2.3078 (2.3074)  acc1: 10.1562 (10.1562)  acc5: 50.7812 (50.8272)  time: 11.6299  data: 0.0178\n",
      "Epoch: [3]  [17/40]  eta: 0:04:27  lr: 1.0000000000000002e-07  img/s: 10.818964453534202  loss: 2.3074 (2.3072)  acc1: 10.1562 (10.1562)  acc5: 50.7812 (51.0851)  time: 11.6420  data: 0.0178\n",
      "Epoch: [3]  [18/40]  eta: 0:04:16  lr: 1.0000000000000002e-07  img/s: 10.812415968384869  loss: 2.3078 (2.3086)  acc1: 10.1562 (10.1974)  acc5: 50.7812 (50.7401)  time: 11.6533  data: 0.0178\n",
      "Epoch: [3]  [19/40]  eta: 0:04:04  lr: 1.0000000000000002e-07  img/s: 11.032250727800168  loss: 2.3078 (2.3095)  acc1: 10.1562 (10.1562)  acc5: 50.7812 (50.5078)  time: 11.6516  data: 0.0178\n",
      "Epoch: [3]  [20/40]  eta: 0:03:53  lr: 1.0000000000000002e-07  img/s: 11.035175752557441  loss: 2.3081 (2.3097)  acc1: 10.1562 (10.1935)  acc5: 49.2188 (50.3348)  time: 11.6641  data: 0.0178\n",
      "Epoch: [3]  [21/40]  eta: 0:03:41  lr: 1.0000000000000002e-07  img/s: 10.93885952594213  loss: 2.3081 (2.3091)  acc1: 10.1562 (10.0852)  acc5: 49.2188 (50.4972)  time: 11.6557  data: 0.0178\n",
      "Epoch: [3]  [22/40]  eta: 0:03:29  lr: 1.0000000000000002e-07  img/s: 11.027367654713323  loss: 2.3117 (2.3102)  acc1: 10.1562 (10.0204)  acc5: 48.4375 (49.9660)  time: 11.6520  data: 0.0178\n",
      "Epoch: [3]  [23/40]  eta: 0:03:18  lr: 1.0000000000000002e-07  img/s: 10.892552367174824  loss: 2.3117 (2.3106)  acc1: 9.3750 (9.9935)  acc5: 48.4375 (49.9023)  time: 11.6581  data: 0.0178\n",
      "Epoch: [3]  [24/40]  eta: 0:03:06  lr: 1.0000000000000002e-07  img/s: 11.036037069773199  loss: 2.3117 (2.3105)  acc1: 9.3750 (9.9062)  acc5: 48.4375 (49.9062)  time: 11.6588  data: 0.0178\n",
      "Epoch: [3]  [25/40]  eta: 0:02:54  lr: 1.0000000000000002e-07  img/s: 10.821802980226796  loss: 2.3117 (2.3104)  acc1: 9.3750 (9.9760)  acc5: 48.4375 (49.7596)  time: 11.6741  data: 0.0178\n",
      "Epoch: [3]  [26/40]  eta: 0:02:43  lr: 1.0000000000000002e-07  img/s: 11.067844989081738  loss: 2.3117 (2.3105)  acc1: 9.3750 (9.9248)  acc5: 48.4375 (49.7685)  time: 11.6775  data: 0.0178\n",
      "Epoch: [3]  [27/40]  eta: 0:02:31  lr: 1.0000000000000002e-07  img/s: 11.100186085505051  loss: 2.3117 (2.3104)  acc1: 9.3750 (9.9330)  acc5: 48.4375 (49.9721)  time: 11.6773  data: 0.0181\n",
      "Epoch: [3]  [28/40]  eta: 0:02:19  lr: 1.0000000000000002e-07  img/s: 11.134430153828133  loss: 2.3117 (2.3103)  acc1: 9.3750 (9.9677)  acc5: 48.4375 (50.0539)  time: 11.6782  data: 0.0181\n",
      "Epoch: [3]  [29/40]  eta: 0:02:08  lr: 1.0000000000000002e-07  img/s: 11.022030658754655  loss: 2.3117 (2.3101)  acc1: 9.3750 (9.9219)  acc5: 48.4375 (50.1823)  time: 11.6767  data: 0.0181\n",
      "Epoch: [3]  [30/40]  eta: 0:01:56  lr: 1.0000000000000002e-07  img/s: 10.92079409886342  loss: 2.3091 (2.3098)  acc1: 9.3750 (9.9546)  acc5: 49.2188 (50.1764)  time: 11.6741  data: 0.0181\n",
      "Epoch: [3]  [31/40]  eta: 0:01:44  lr: 1.0000000000000002e-07  img/s: 10.92517768823942  loss: 2.3117 (2.3104)  acc1: 9.3750 (9.8877)  acc5: 48.4375 (49.9512)  time: 11.6807  data: 0.0182\n",
      "Epoch: [3]  [32/40]  eta: 0:01:33  lr: 1.0000000000000002e-07  img/s: 10.923596523597304  loss: 2.3091 (2.3102)  acc1: 9.3750 (9.7775)  acc5: 48.4375 (49.8816)  time: 11.6846  data: 0.0182\n",
      "Epoch: [3]  [33/40]  eta: 0:01:21  lr: 1.0000000000000002e-07  img/s: 11.044722171819847  loss: 2.3091 (2.3103)  acc1: 9.3750 (9.8346)  acc5: 47.6562 (49.8162)  time: 11.6771  data: 0.0182\n",
      "Epoch: [3]  [34/40]  eta: 0:01:09  lr: 1.0000000000000002e-07  img/s: 10.77313116982798  loss: 2.3091 (2.3104)  acc1: 9.3750 (9.8884)  acc5: 48.4375 (49.9554)  time: 11.6855  data: 0.0181\n",
      "Epoch: [3]  [35/40]  eta: 0:00:58  lr: 1.0000000000000002e-07  img/s: 11.02766845881637  loss: 2.3091 (2.3104)  acc1: 9.3750 (9.9175)  acc5: 48.4375 (49.8915)  time: 11.6896  data: 0.0181\n",
      "Epoch: [3]  [36/40]  eta: 0:00:46  lr: 1.0000000000000002e-07  img/s: 10.968686635879266  loss: 2.3091 (2.3101)  acc1: 10.1562 (9.9662)  acc5: 48.4375 (50.0633)  time: 11.6914  data: 0.0181\n",
      "Epoch: [3]  [37/40]  eta: 0:00:34  lr: 1.0000000000000002e-07  img/s: 10.869467927622955  loss: 2.3091 (2.3099)  acc1: 10.1562 (10.0535)  acc5: 48.4375 (50.0206)  time: 11.6886  data: 0.0181\n",
      "Epoch: [3]  [38/40]  eta: 0:00:23  lr: 1.0000000000000002e-07  img/s: 10.822335697046878  loss: 2.3091 (2.3103)  acc1: 9.3750 (9.9960)  acc5: 48.4375 (49.8998)  time: 11.6881  data: 0.0181\n",
      "Epoch: [3]  [39/40]  eta: 0:00:11  lr: 1.0000000000000002e-07  img/s: 10.681801092615059  loss: 2.3080 (2.3098)  acc1: 9.3750 (9.9800)  acc5: 48.4375 (49.9000)  time: 11.1446  data: 0.0174\n",
      "Epoch: [3] Total time: 0:07:35\n",
      "Epoch 5/15\n",
      "Epoch: [4]  [ 0/40]  eta: 0:07:36  lr: 1.0000000000000004e-08  img/s: 11.232647235051976  loss: 2.3228 (2.3228)  acc1: 9.3750 (9.3750)  acc5: 48.4375 (48.4375)  time: 11.4133  data: 0.0180\n",
      "Epoch: [4]  [ 1/40]  eta: 0:07:35  lr: 1.0000000000000004e-08  img/s: 10.73364493445974  loss: 2.3228 (2.3264)  acc1: 8.5938 (8.9844)  acc5: 45.3125 (46.8750)  time: 11.6781  data: 0.0178\n",
      "Epoch: [4]  [ 2/40]  eta: 0:07:24  lr: 1.0000000000000004e-08  img/s: 10.956332557423508  loss: 2.3228 (2.3177)  acc1: 9.3750 (9.6354)  acc5: 48.4375 (50.5208)  time: 11.6857  data: 0.0179\n",
      "Epoch: [4]  [ 3/40]  eta: 0:07:12  lr: 1.0000000000000004e-08  img/s: 10.986383084072445  loss: 2.3055 (2.3147)  acc1: 9.3750 (9.7656)  acc5: 47.6562 (49.8047)  time: 11.6813  data: 0.0178\n",
      "Epoch: [4]  [ 4/40]  eta: 0:07:00  lr: 1.0000000000000004e-08  img/s: 10.950475733081674  loss: 2.3055 (2.3115)  acc1: 10.1562 (10.0000)  acc5: 48.4375 (49.8438)  time: 11.6864  data: 0.0178\n",
      "Epoch: [4]  [ 5/40]  eta: 0:06:48  lr: 1.0000000000000004e-08  img/s: 11.035180742683442  loss: 2.3055 (2.3107)  acc1: 9.3750 (9.5052)  acc5: 48.4375 (50.5208)  time: 11.6748  data: 0.0178\n",
      "Epoch: [4]  [ 6/40]  eta: 0:06:36  lr: 1.0000000000000004e-08  img/s: 10.99809418861092  loss: 2.3064 (2.3110)  acc1: 10.1562 (10.0446)  acc5: 48.4375 (49.7768)  time: 11.6722  data: 0.0178\n",
      "Epoch: [4]  [ 7/40]  eta: 0:06:25  lr: 1.0000000000000004e-08  img/s: 11.023132770777478  loss: 2.3064 (2.3120)  acc1: 10.1562 (10.2539)  acc5: 48.4375 (49.7070)  time: 11.6669  data: 0.0178\n",
      "Epoch: [4]  [ 8/40]  eta: 0:06:14  lr: 1.0000000000000004e-08  img/s: 10.791914385912833  loss: 2.3078 (2.3115)  acc1: 10.1562 (9.8958)  acc5: 49.2188 (50.1736)  time: 11.6903  data: 0.0177\n",
      "Epoch: [4]  [ 9/40]  eta: 0:06:02  lr: 1.0000000000000004e-08  img/s: 10.895916119671446  loss: 2.3064 (2.3097)  acc1: 10.1562 (10.1562)  acc5: 49.2188 (50.9375)  time: 11.6979  data: 0.0178\n",
      "Epoch: [4]  [10/40]  eta: 0:05:51  lr: 1.0000000000000004e-08  img/s: 10.878969516707864  loss: 2.3069 (2.3095)  acc1: 10.1562 (9.9432)  acc5: 49.2188 (50.7812)  time: 11.7057  data: 0.0178\n",
      "Epoch: [4]  [11/40]  eta: 0:05:39  lr: 1.0000000000000004e-08  img/s: 10.973888864723722  loss: 2.3064 (2.3073)  acc1: 9.3750 (9.7656)  acc5: 49.2188 (51.6927)  time: 11.7037  data: 0.0178\n",
      "Epoch: [4]  [12/40]  eta: 0:05:27  lr: 1.0000000000000004e-08  img/s: 10.995489638846898  loss: 2.3069 (2.3079)  acc1: 10.1562 (9.7957)  acc5: 49.2188 (51.0817)  time: 11.7002  data: 0.0177\n",
      "Epoch: [4]  [13/40]  eta: 0:05:15  lr: 1.0000000000000004e-08  img/s: 11.029292132946061  loss: 2.3069 (2.3080)  acc1: 10.1562 (9.8772)  acc5: 49.2188 (51.0045)  time: 11.6947  data: 0.0177\n",
      "Epoch: [4]  [14/40]  eta: 0:05:04  lr: 1.0000000000000004e-08  img/s: 10.934035821589555  loss: 2.3078 (2.3085)  acc1: 10.1562 (10.1042)  acc5: 49.2188 (50.6250)  time: 11.6967  data: 0.0177\n",
      "Epoch: [4]  [15/40]  eta: 0:04:52  lr: 1.0000000000000004e-08  img/s: 10.911612310079338  loss: 2.3078 (2.3101)  acc1: 10.1562 (9.8633)  acc5: 49.2188 (50.2441)  time: 11.6999  data: 0.0177\n",
      "Epoch: [4]  [16/40]  eta: 0:04:40  lr: 1.0000000000000004e-08  img/s: 11.031980957140414  loss: 2.3089 (2.3102)  acc1: 10.1562 (9.8346)  acc5: 49.2188 (50.2298)  time: 11.6952  data: 0.0177\n",
      "Epoch: [4]  [17/40]  eta: 0:04:28  lr: 1.0000000000000004e-08  img/s: 11.126460955601523  loss: 2.3089 (2.3108)  acc1: 9.3750 (9.7222)  acc5: 49.2188 (49.9566)  time: 11.6856  data: 0.0177\n",
      "Epoch: [4]  [18/40]  eta: 0:04:17  lr: 1.0000000000000004e-08  img/s: 10.953120227259467  loss: 2.3113 (2.3122)  acc1: 9.3750 (9.5806)  acc5: 49.2188 (49.6299)  time: 11.6866  data: 0.0178\n",
      "Epoch: [4]  [19/40]  eta: 0:04:05  lr: 1.0000000000000004e-08  img/s: 11.080910509276436  loss: 2.3089 (2.3118)  acc1: 9.3750 (9.6094)  acc5: 49.2188 (49.7656)  time: 11.6807  data: 0.0178\n",
      "Epoch: [4]  [20/40]  eta: 0:03:53  lr: 1.0000000000000004e-08  img/s: 11.0885053803645  loss: 2.3078 (2.3114)  acc1: 10.1562 (9.7098)  acc5: 49.2188 (49.8884)  time: 11.6881  data: 0.0178\n",
      "Epoch: [4]  [21/40]  eta: 0:03:41  lr: 1.0000000000000004e-08  img/s: 11.027982417949902  loss: 2.3069 (2.3107)  acc1: 10.1562 (9.8011)  acc5: 50.0000 (49.9645)  time: 11.6722  data: 0.0178\n",
      "Epoch: [4]  [22/40]  eta: 0:03:29  lr: 1.0000000000000004e-08  img/s: 11.129975364914115  loss: 2.3069 (2.3104)  acc1: 10.1562 (9.8166)  acc5: 50.0000 (50.0679)  time: 11.6631  data: 0.0178\n",
      "Epoch: [4]  [23/40]  eta: 0:03:18  lr: 1.0000000000000004e-08  img/s: 10.943552750578334  loss: 2.3069 (2.3094)  acc1: 10.1562 (9.9609)  acc5: 50.0000 (50.4883)  time: 11.6654  data: 0.0178\n",
      "Epoch: [4]  [24/40]  eta: 0:03:06  lr: 1.0000000000000004e-08  img/s: 11.032370428587557  loss: 2.3078 (2.3094)  acc1: 10.1562 (10.0000)  acc5: 50.0000 (50.5312)  time: 11.6611  data: 0.0178\n",
      "Epoch: [4]  [25/40]  eta: 0:02:54  lr: 1.0000000000000004e-08  img/s: 10.99090882421264  loss: 2.3082 (2.3096)  acc1: 10.1562 (9.8257)  acc5: 50.0000 (50.3906)  time: 11.6634  data: 0.0178\n",
      "Epoch: [4]  [26/40]  eta: 0:02:43  lr: 1.0000000000000004e-08  img/s: 11.035572708497652  loss: 2.3082 (2.3111)  acc1: 10.1562 (9.5775)  acc5: 50.0000 (49.9421)  time: 11.6614  data: 0.0178\n",
      "Epoch: [4]  [27/40]  eta: 0:02:31  lr: 1.0000000000000004e-08  img/s: 11.024876911638371  loss: 2.3082 (2.3110)  acc1: 9.3750 (9.5424)  acc5: 50.0000 (50.1953)  time: 11.6613  data: 0.0178\n",
      "Epoch: [4]  [28/40]  eta: 0:02:19  lr: 1.0000000000000004e-08  img/s: 10.923340263760162  loss: 2.3082 (2.3108)  acc1: 9.3750 (9.5366)  acc5: 50.0000 (50.1886)  time: 11.6542  data: 0.0178\n",
      "Epoch: [4]  [29/40]  eta: 0:02:08  lr: 1.0000000000000004e-08  img/s: 11.028857112665026  loss: 2.3082 (2.3101)  acc1: 9.3750 (9.6094)  acc5: 50.0000 (50.5208)  time: 11.6471  data: 0.0178\n",
      "Epoch: [4]  [30/40]  eta: 0:01:56  lr: 1.0000000000000004e-08  img/s: 10.88954518625194  loss: 2.3086 (2.3104)  acc1: 10.1562 (9.6774)  acc5: 50.0000 (50.4284)  time: 11.6466  data: 0.0178\n",
      "Epoch: [4]  [31/40]  eta: 0:01:44  lr: 1.0000000000000004e-08  img/s: 11.021372212397887  loss: 2.3089 (2.3105)  acc1: 10.1562 (9.7168)  acc5: 50.0000 (50.2930)  time: 11.6441  data: 0.0178\n",
      "Epoch: [4]  [32/40]  eta: 0:01:33  lr: 1.0000000000000004e-08  img/s: 11.025912565382747  loss: 2.3086 (2.3103)  acc1: 10.1562 (9.6828)  acc5: 50.0000 (50.3314)  time: 11.6425  data: 0.0179\n",
      "Epoch: [4]  [33/40]  eta: 0:01:21  lr: 1.0000000000000004e-08  img/s: 11.035071641412475  loss: 2.3086 (2.3104)  acc1: 10.1562 (9.7886)  acc5: 50.0000 (50.2528)  time: 11.6422  data: 0.0179\n",
      "Epoch: [4]  [34/40]  eta: 0:01:09  lr: 1.0000000000000004e-08  img/s: 10.932186289578862  loss: 2.3086 (2.3104)  acc1: 9.3750 (9.7545)  acc5: 50.0000 (50.3571)  time: 11.6423  data: 0.0179\n",
      "Epoch: [4]  [35/40]  eta: 0:00:58  lr: 1.0000000000000004e-08  img/s: 11.120197915035304  loss: 2.3086 (2.3108)  acc1: 10.1562 (9.7656)  acc5: 50.0000 (50.2821)  time: 11.6313  data: 0.0179\n",
      "Epoch: [4]  [36/40]  eta: 0:00:46  lr: 1.0000000000000004e-08  img/s: 10.83027222876969  loss: 2.3086 (2.3108)  acc1: 10.1562 (9.8184)  acc5: 51.5625 (50.3167)  time: 11.6425  data: 0.0182\n",
      "Epoch: [4]  [37/40]  eta: 0:00:34  lr: 1.0000000000000004e-08  img/s: 11.061893213835612  loss: 2.3086 (2.3110)  acc1: 10.1562 (9.8067)  acc5: 51.5625 (50.2878)  time: 11.6458  data: 0.0182\n",
      "Epoch: [4]  [38/40]  eta: 0:00:23  lr: 1.0000000000000004e-08  img/s: 10.93771581716478  loss: 2.3086 (2.3109)  acc1: 10.1562 (9.8758)  acc5: 51.5625 (50.2204)  time: 11.6466  data: 0.0182\n",
      "Epoch: [4]  [39/40]  eta: 0:00:11  lr: 1.0000000000000004e-08  img/s: 11.465109667825669  loss: 2.3086 (2.3101)  acc1: 10.9375 (9.8800)  acc5: 51.5625 (50.2600)  time: 11.1032  data: 0.0174\n",
      "Epoch: [4] Total time: 0:07:35\n",
      "Epoch 6/15\n",
      "Epoch: [5]  [ 0/40]  eta: 0:07:32  lr: 1.0000000000000005e-09  img/s: 11.34088846279331  loss: 2.3252 (2.3252)  acc1: 7.8125 (7.8125)  acc5: 46.0938 (46.0938)  time: 11.3049  data: 0.0183\n",
      "Epoch: [5]  [ 1/40]  eta: 0:07:31  lr: 1.0000000000000005e-09  img/s: 10.838666778619128  loss: 2.2991 (2.3122)  acc1: 7.8125 (8.5938)  acc5: 46.0938 (50.0000)  time: 11.5661  data: 0.0180\n",
      "Epoch: [5]  [ 2/40]  eta: 0:07:20  lr: 1.0000000000000005e-09  img/s: 11.015088123530289  loss: 2.3252 (2.3213)  acc1: 7.8125 (8.0729)  acc5: 46.0938 (46.8750)  time: 11.5901  data: 0.0179\n",
      "Epoch: [5]  [ 3/40]  eta: 0:07:09  lr: 1.0000000000000005e-09  img/s: 11.042344636653036  loss: 2.2991 (2.3147)  acc1: 7.8125 (9.3750)  acc5: 46.0938 (48.2422)  time: 11.5950  data: 0.0179\n",
      "Epoch: [5]  [ 4/40]  eta: 0:06:58  lr: 1.0000000000000005e-09  img/s: 10.911980465094253  loss: 2.2991 (2.3105)  acc1: 9.3750 (9.8438)  acc5: 50.0000 (48.5938)  time: 11.6256  data: 0.0178\n",
      "Epoch: [5]  [ 5/40]  eta: 0:06:46  lr: 1.0000000000000005e-09  img/s: 11.01407325656176  loss: 2.2991 (2.3095)  acc1: 9.3750 (10.0260)  acc5: 50.0000 (49.3490)  time: 11.6278  data: 0.0178\n",
      "Epoch: [5]  [ 6/40]  eta: 0:06:35  lr: 1.0000000000000005e-09  img/s: 11.037907387206737  loss: 2.2991 (2.3073)  acc1: 10.9375 (10.1562)  acc5: 52.3438 (49.7768)  time: 11.6259  data: 0.0178\n",
      "Epoch: [5]  [ 7/40]  eta: 0:06:23  lr: 1.0000000000000005e-09  img/s: 10.991013453778493  loss: 2.2991 (2.3073)  acc1: 9.3750 (9.8633)  acc5: 52.3438 (50.0977)  time: 11.6306  data: 0.0178\n",
      "Epoch: [5]  [ 8/40]  eta: 0:06:12  lr: 1.0000000000000005e-09  img/s: 11.028779628278148  loss: 2.3034 (2.3069)  acc1: 10.9375 (10.0694)  acc5: 52.3438 (50.3472)  time: 11.6299  data: 0.0178\n",
      "Epoch: [5]  [ 9/40]  eta: 0:06:00  lr: 1.0000000000000005e-09  img/s: 11.050572456430618  loss: 2.2991 (2.3056)  acc1: 10.9375 (10.1562)  acc5: 52.3438 (50.7031)  time: 11.6270  data: 0.0178\n",
      "Epoch: [5]  [10/40]  eta: 0:05:49  lr: 1.0000000000000005e-09  img/s: 10.875196099383084  loss: 2.3034 (2.3061)  acc1: 10.9375 (10.3693)  acc5: 52.3438 (50.6392)  time: 11.6416  data: 0.0178\n",
      "Epoch: [5]  [11/40]  eta: 0:05:37  lr: 1.0000000000000005e-09  img/s: 11.031710292948155  loss: 2.2991 (2.3053)  acc1: 10.9375 (10.4818)  acc5: 52.3438 (51.1068)  time: 11.6398  data: 0.0178\n",
      "Epoch: [5]  [12/40]  eta: 0:05:26  lr: 1.0000000000000005e-09  img/s: 10.873838151294699  loss: 2.3034 (2.3051)  acc1: 10.9375 (10.6370)  acc5: 52.3438 (51.2620)  time: 11.6513  data: 0.0178\n",
      "Epoch: [5]  [13/40]  eta: 0:05:14  lr: 1.0000000000000005e-09  img/s: 11.007554412690755  loss: 2.3034 (2.3062)  acc1: 10.9375 (10.3237)  acc5: 52.3438 (51.1719)  time: 11.6510  data: 0.0178\n",
      "Epoch: [5]  [14/40]  eta: 0:05:03  lr: 1.0000000000000005e-09  img/s: 10.894967753896841  loss: 2.3037 (2.3065)  acc1: 10.9375 (10.3646)  acc5: 52.3438 (50.9896)  time: 11.6587  data: 0.0178\n",
      "Epoch: [5]  [15/40]  eta: 0:04:51  lr: 1.0000000000000005e-09  img/s: 10.903945254638993  loss: 2.3037 (2.3067)  acc1: 10.9375 (10.5469)  acc5: 52.3438 (50.8789)  time: 11.6648  data: 0.0178\n",
      "Epoch: [5]  [16/40]  eta: 0:04:40  lr: 1.0000000000000005e-09  img/s: 10.888143021611022  loss: 2.3044 (2.3078)  acc1: 10.9375 (10.3401)  acc5: 52.3438 (50.7353)  time: 11.6712  data: 0.0179\n",
      "Epoch: [5]  [17/40]  eta: 0:04:28  lr: 1.0000000000000005e-09  img/s: 11.021561366252492  loss: 2.3044 (2.3081)  acc1: 10.9375 (10.2431)  acc5: 50.0000 (50.4774)  time: 11.6690  data: 0.0179\n",
      "Epoch: [5]  [18/40]  eta: 0:04:16  lr: 1.0000000000000005e-09  img/s: 10.950478189989607  loss: 2.3070 (2.3083)  acc1: 10.9375 (10.2796)  acc5: 50.0000 (50.3289)  time: 11.6710  data: 0.0179\n",
      "Epoch: [5]  [19/40]  eta: 0:04:05  lr: 1.0000000000000005e-09  img/s: 10.857451931229553  loss: 2.3070 (2.3088)  acc1: 10.9375 (10.4297)  acc5: 50.0000 (50.1953)  time: 11.6778  data: 0.0179\n",
      "Epoch: [5]  [20/40]  eta: 0:03:53  lr: 1.0000000000000005e-09  img/s: 11.045089592997153  loss: 2.3044 (2.3079)  acc1: 10.9375 (10.4911)  acc5: 50.0000 (50.2976)  time: 11.6929  data: 0.0179\n",
      "Epoch: [5]  [21/40]  eta: 0:03:41  lr: 1.0000000000000005e-09  img/s: 10.833969929977766  loss: 2.3063 (2.3078)  acc1: 10.9375 (10.6179)  acc5: 50.0000 (50.0355)  time: 11.6932  data: 0.0179\n",
      "Epoch: [5]  [22/40]  eta: 0:03:30  lr: 1.0000000000000005e-09  img/s: 11.016473670258154  loss: 2.3063 (2.3081)  acc1: 10.9375 (10.5639)  acc5: 50.0000 (49.8981)  time: 11.6931  data: 0.0179\n",
      "Epoch: [5]  [23/40]  eta: 0:03:18  lr: 1.0000000000000005e-09  img/s: 10.902656946198332  loss: 2.3070 (2.3082)  acc1: 10.9375 (10.5469)  acc5: 50.0000 (49.8047)  time: 11.7005  data: 0.0179\n",
      "Epoch: [5]  [24/40]  eta: 0:03:06  lr: 1.0000000000000005e-09  img/s: 11.01926797432037  loss: 2.3089 (2.3091)  acc1: 10.9375 (10.3125)  acc5: 49.2188 (49.7188)  time: 11.6948  data: 0.0179\n",
      "Epoch: [5]  [25/40]  eta: 0:02:55  lr: 1.0000000000000005e-09  img/s: 10.970537555427052  loss: 2.3089 (2.3086)  acc1: 10.9375 (10.2764)  acc5: 49.2188 (49.7596)  time: 11.6972  data: 0.0179\n",
      "Epoch: [5]  [26/40]  eta: 0:02:43  lr: 1.0000000000000005e-09  img/s: 11.064234255286713  loss: 2.3099 (2.3096)  acc1: 10.9375 (10.0405)  acc5: 48.4375 (49.5081)  time: 11.6958  data: 0.0179\n",
      "Epoch: [5]  [27/40]  eta: 0:02:31  lr: 1.0000000000000005e-09  img/s: 10.839778047305142  loss: 2.3099 (2.3096)  acc1: 10.9375 (10.0725)  acc5: 48.4375 (49.5257)  time: 11.7039  data: 0.0179\n",
      "Epoch: [5]  [28/40]  eta: 0:02:20  lr: 1.0000000000000005e-09  img/s: 11.05647061096169  loss: 2.3101 (2.3097)  acc1: 10.9375 (10.0216)  acc5: 48.4375 (49.6498)  time: 11.7024  data: 0.0179\n",
      "Epoch: [5]  [29/40]  eta: 0:02:08  lr: 1.0000000000000005e-09  img/s: 11.092223873990468  loss: 2.3101 (2.3094)  acc1: 10.9375 (10.1302)  acc5: 48.4375 (49.7917)  time: 11.7002  data: 0.0178\n",
      "Epoch: [5]  [30/40]  eta: 0:01:56  lr: 1.0000000000000005e-09  img/s: 10.935326433920682  loss: 2.3101 (2.3095)  acc1: 10.9375 (10.1562)  acc5: 48.4375 (49.7732)  time: 11.6970  data: 0.0178\n",
      "Epoch: [5]  [31/40]  eta: 0:01:45  lr: 1.0000000000000005e-09  img/s: 10.94290810789701  loss: 2.3116 (2.3098)  acc1: 10.1562 (10.1562)  acc5: 48.4375 (49.7314)  time: 11.7017  data: 0.0179\n",
      "Epoch: [5]  [32/40]  eta: 0:01:33  lr: 1.0000000000000005e-09  img/s: 10.832931984650019  loss: 2.3130 (2.3102)  acc1: 10.1562 (10.1562)  acc5: 48.4375 (49.6449)  time: 11.7039  data: 0.0178\n",
      "Epoch: [5]  [33/40]  eta: 0:01:21  lr: 1.0000000000000005e-09  img/s: 10.84528917351813  loss: 2.3130 (2.3107)  acc1: 10.1562 (10.0873)  acc5: 47.6562 (49.4715)  time: 11.7126  data: 0.0178\n",
      "Epoch: [5]  [34/40]  eta: 0:01:10  lr: 1.0000000000000005e-09  img/s: 10.996591179276965  loss: 2.3130 (2.3103)  acc1: 10.1562 (10.0670)  acc5: 47.6562 (49.7098)  time: 11.7072  data: 0.0178\n",
      "Epoch: [5]  [35/40]  eta: 0:00:58  lr: 1.0000000000000005e-09  img/s: 10.7945464339835  loss: 2.3131 (2.3108)  acc1: 9.3750 (10.0260)  acc5: 47.6562 (49.6311)  time: 11.7131  data: 0.0178\n",
      "Epoch: [5]  [36/40]  eta: 0:00:46  lr: 1.0000000000000005e-09  img/s: 10.884967236379293  loss: 2.3130 (2.3105)  acc1: 10.1562 (10.1985)  acc5: 47.6562 (49.7466)  time: 11.7133  data: 0.0178\n",
      "Epoch: [5]  [37/40]  eta: 0:00:35  lr: 1.0000000000000005e-09  img/s: 10.908108097345094  loss: 2.3130 (2.3109)  acc1: 10.1562 (10.1562)  acc5: 47.6562 (49.6711)  time: 11.7193  data: 0.0178\n",
      "Epoch: [5]  [38/40]  eta: 0:00:23  lr: 1.0000000000000005e-09  img/s: 11.075971250675497  loss: 2.3131 (2.3115)  acc1: 9.3750 (10.0962)  acc5: 47.6562 (49.6595)  time: 11.7127  data: 0.0178\n",
      "Epoch: [5]  [39/40]  eta: 0:00:11  lr: 1.0000000000000005e-09  img/s: 11.504588370140297  loss: 2.3130 (2.3095)  acc1: 9.3750 (10.1200)  acc5: 48.4375 (49.7000)  time: 11.1572  data: 0.0170\n",
      "Epoch: [5] Total time: 0:07:36\n",
      "Epoch 7/15\n",
      "Epoch: [6]  [ 0/40]  eta: 0:07:30  lr: 1.0000000000000006e-10  img/s: 11.37949437546125  loss: 2.2985 (2.2985)  acc1: 10.9375 (10.9375)  acc5: 52.3438 (52.3438)  time: 11.2664  data: 0.0181\n",
      "Epoch: [6]  [ 1/40]  eta: 0:07:29  lr: 1.0000000000000006e-10  img/s: 10.883860363513872  loss: 2.2985 (2.3059)  acc1: 6.2500 (8.5938)  acc5: 52.3438 (54.6875)  time: 11.5224  data: 0.0180\n",
      "Epoch: [6]  [ 2/40]  eta: 0:07:17  lr: 1.0000000000000006e-10  img/s: 11.124272145893046  loss: 2.3133 (2.3089)  acc1: 10.9375 (9.3750)  acc5: 52.3438 (53.9062)  time: 11.5230  data: 0.0179\n",
      "Epoch: [6]  [ 3/40]  eta: 0:07:06  lr: 1.0000000000000006e-10  img/s: 11.12483759426578  loss: 2.3133 (2.3104)  acc1: 10.9375 (10.1562)  acc5: 52.3438 (51.7578)  time: 11.5232  data: 0.0179\n",
      "Epoch: [6]  [ 4/40]  eta: 0:06:54  lr: 1.0000000000000006e-10  img/s: 11.124948016698065  loss: 2.3133 (2.3066)  acc1: 10.9375 (10.1562)  acc5: 52.3438 (51.8750)  time: 11.5233  data: 0.0179\n",
      "Epoch: [6]  [ 5/40]  eta: 0:06:43  lr: 1.0000000000000006e-10  img/s: 11.138572826488709  loss: 2.3046 (2.3063)  acc1: 10.9375 (10.2865)  acc5: 52.3438 (51.3021)  time: 11.5210  data: 0.0179\n",
      "Epoch: [6]  [ 6/40]  eta: 0:06:32  lr: 1.0000000000000006e-10  img/s: 11.051682559047158  loss: 2.3068 (2.3064)  acc1: 10.9375 (9.9330)  acc5: 52.3438 (51.7857)  time: 11.5322  data: 0.0179\n",
      "Epoch: [6]  [ 7/40]  eta: 0:06:20  lr: 1.0000000000000006e-10  img/s: 11.218936568156392  loss: 2.3046 (2.3049)  acc1: 10.9375 (10.2539)  acc5: 52.3438 (51.7578)  time: 11.5191  data: 0.0179\n",
      "Epoch: [6]  [ 8/40]  eta: 0:06:08  lr: 1.0000000000000006e-10  img/s: 11.114584553488518  loss: 2.3068 (2.3051)  acc1: 10.9375 (10.5903)  acc5: 52.3438 (51.8229)  time: 11.5208  data: 0.0179\n",
      "Epoch: [6]  [ 9/40]  eta: 0:05:57  lr: 1.0000000000000006e-10  img/s: 11.132573154496658  loss: 2.3046 (2.3050)  acc1: 10.9375 (10.8594)  acc5: 52.3438 (51.9531)  time: 11.5203  data: 0.0179\n",
      "Epoch: [6]  [10/40]  eta: 0:05:45  lr: 1.0000000000000006e-10  img/s: 11.03284200241663  loss: 2.3068 (2.3073)  acc1: 10.9375 (10.5114)  acc5: 52.3438 (51.2784)  time: 11.5293  data: 0.0179\n",
      "Epoch: [6]  [11/40]  eta: 0:05:34  lr: 1.0000000000000006e-10  img/s: 11.076363834424455  loss: 2.3046 (2.3064)  acc1: 10.9375 (10.4167)  acc5: 52.3438 (51.4323)  time: 11.5330  data: 0.0179\n",
      "Epoch: [6]  [12/40]  eta: 0:05:23  lr: 1.0000000000000006e-10  img/s: 10.995933291437698  loss: 2.3068 (2.3073)  acc1: 10.9375 (10.5168)  acc5: 52.3438 (51.3822)  time: 11.5427  data: 0.0179\n",
      "Epoch: [6]  [13/40]  eta: 0:05:11  lr: 1.0000000000000006e-10  img/s: 11.153306989272656  loss: 2.3068 (2.3075)  acc1: 10.9375 (10.4911)  acc5: 52.3438 (51.1719)  time: 11.5392  data: 0.0179\n",
      "Epoch: [6]  [14/40]  eta: 0:04:59  lr: 1.0000000000000006e-10  img/s: 11.16196932423182  loss: 2.3075 (2.3077)  acc1: 10.9375 (10.3125)  acc5: 52.3438 (51.1979)  time: 11.5356  data: 0.0179\n",
      "Epoch: [6]  [15/40]  eta: 0:04:48  lr: 1.0000000000000006e-10  img/s: 11.002199595960292  loss: 2.3075 (2.3092)  acc1: 10.9375 (10.4492)  acc5: 51.5625 (50.8301)  time: 11.5429  data: 0.0179\n",
      "Epoch: [6]  [16/40]  eta: 0:04:37  lr: 1.0000000000000006e-10  img/s: 10.931669859366394  loss: 2.3101 (2.3100)  acc1: 10.9375 (10.2482)  acc5: 51.5625 (50.5055)  time: 11.5537  data: 0.0179\n",
      "Epoch: [6]  [17/40]  eta: 0:04:25  lr: 1.0000000000000006e-10  img/s: 11.134738443806024  loss: 2.3075 (2.3091)  acc1: 10.9375 (10.2865)  acc5: 51.5625 (50.9549)  time: 11.5515  data: 0.0178\n",
      "Epoch: [6]  [18/40]  eta: 0:04:14  lr: 1.0000000000000006e-10  img/s: 11.054525711473149  loss: 2.3101 (2.3095)  acc1: 10.9375 (10.2796)  acc5: 52.3438 (51.2336)  time: 11.5539  data: 0.0178\n",
      "Epoch: [6]  [19/40]  eta: 0:04:02  lr: 1.0000000000000006e-10  img/s: 11.149761391731758  loss: 2.3101 (2.3102)  acc1: 10.1562 (10.1953)  acc5: 51.5625 (50.9766)  time: 11.5510  data: 0.0178\n",
      "Epoch: [6]  [20/40]  eta: 0:03:51  lr: 1.0000000000000006e-10  img/s: 11.060797922097272  loss: 2.3108 (2.3111)  acc1: 10.1562 (10.2679)  acc5: 51.5625 (50.4836)  time: 11.5672  data: 0.0178\n",
      "Epoch: [6]  [21/40]  eta: 0:03:39  lr: 1.0000000000000006e-10  img/s: 11.10266321804378  loss: 2.3101 (2.3110)  acc1: 10.9375 (10.2983)  acc5: 50.7812 (50.4972)  time: 11.5556  data: 0.0178\n",
      "Epoch: [6]  [22/40]  eta: 0:03:27  lr: 1.0000000000000006e-10  img/s: 11.053359054665682  loss: 2.3101 (2.3116)  acc1: 10.1562 (10.1902)  acc5: 50.7812 (50.2717)  time: 11.5593  data: 0.0178\n",
      "Epoch: [6]  [23/40]  eta: 0:03:16  lr: 1.0000000000000006e-10  img/s: 10.99081657183938  loss: 2.3098 (2.3112)  acc1: 10.1562 (10.2539)  acc5: 50.7812 (50.3581)  time: 11.5663  data: 0.0178\n",
      "Epoch: [6]  [24/40]  eta: 0:03:05  lr: 1.0000000000000006e-10  img/s: 10.822244071254904  loss: 2.3098 (2.3111)  acc1: 10.1562 (10.2188)  acc5: 50.7812 (50.5625)  time: 11.5824  data: 0.0178\n",
      "Epoch: [6]  [25/40]  eta: 0:02:53  lr: 1.0000000000000006e-10  img/s: 10.973658726109333  loss: 2.3101 (2.3111)  acc1: 10.1562 (10.2464)  acc5: 50.7812 (50.3606)  time: 11.5911  data: 0.0178\n",
      "Epoch: [6]  [26/40]  eta: 0:02:42  lr: 1.0000000000000006e-10  img/s: 11.07789673136469  loss: 2.3101 (2.3108)  acc1: 10.1562 (10.2431)  acc5: 50.7812 (50.3183)  time: 11.5897  data: 0.0178\n",
      "Epoch: [6]  [27/40]  eta: 0:02:30  lr: 1.0000000000000006e-10  img/s: 10.899054487769975  loss: 2.3103 (2.3108)  acc1: 10.1562 (10.2121)  acc5: 49.2188 (50.0279)  time: 11.6065  data: 0.0178\n",
      "Epoch: [6]  [28/40]  eta: 0:02:18  lr: 1.0000000000000006e-10  img/s: 11.01992549067599  loss: 2.3103 (2.3108)  acc1: 10.1562 (10.2640)  acc5: 49.2188 (50.1616)  time: 11.6114  data: 0.0178\n",
      "Epoch: [6]  [29/40]  eta: 0:02:07  lr: 1.0000000000000006e-10  img/s: 10.892857795745192  loss: 2.3103 (2.3104)  acc1: 10.1562 (10.4167)  acc5: 49.2188 (50.1823)  time: 11.6240  data: 0.0178\n",
      "Epoch: [6]  [30/40]  eta: 0:01:55  lr: 1.0000000000000006e-10  img/s: 10.96391182691152  loss: 2.3101 (2.3102)  acc1: 10.1562 (10.4587)  acc5: 50.7812 (50.2016)  time: 11.6277  data: 0.0178\n",
      "Epoch: [6]  [31/40]  eta: 0:01:44  lr: 1.0000000000000006e-10  img/s: 10.981785142629494  loss: 2.3103 (2.3111)  acc1: 10.1562 (10.3516)  acc5: 49.2188 (50.0488)  time: 11.6327  data: 0.0178\n",
      "Epoch: [6]  [32/40]  eta: 0:01:32  lr: 1.0000000000000006e-10  img/s: 11.006228419555887  loss: 2.3103 (2.3113)  acc1: 10.1562 (10.2983)  acc5: 48.4375 (50.0000)  time: 11.6322  data: 0.0179\n",
      "Epoch: [6]  [33/40]  eta: 0:01:21  lr: 1.0000000000000006e-10  img/s: 10.990595847446441  loss: 2.3103 (2.3113)  acc1: 10.1562 (10.3631)  acc5: 49.2188 (50.1149)  time: 11.6407  data: 0.0179\n",
      "Epoch: [6]  [34/40]  eta: 0:01:09  lr: 1.0000000000000006e-10  img/s: 10.891867314733734  loss: 2.3103 (2.3110)  acc1: 10.1562 (10.3348)  acc5: 49.2188 (50.2009)  time: 11.6549  data: 0.0179\n",
      "Epoch: [6]  [35/40]  eta: 0:00:58  lr: 1.0000000000000006e-10  img/s: 10.891350709600104  loss: 2.3098 (2.3105)  acc1: 10.1562 (10.3082)  acc5: 50.7812 (50.3689)  time: 11.6608  data: 0.0179\n",
      "Epoch: [6]  [36/40]  eta: 0:00:46  lr: 1.0000000000000006e-10  img/s: 10.86664261531855  loss: 2.3095 (2.3102)  acc1: 10.1562 (10.4730)  acc5: 50.7812 (50.5490)  time: 11.6643  data: 0.0179\n",
      "Epoch: [6]  [37/40]  eta: 0:00:34  lr: 1.0000000000000006e-10  img/s: 10.964568351755549  loss: 2.3098 (2.3105)  acc1: 10.1562 (10.4235)  acc5: 50.7812 (50.5140)  time: 11.6732  data: 0.0179\n",
      "Epoch: [6]  [38/40]  eta: 0:00:23  lr: 1.0000000000000006e-10  img/s: 10.901544037923266  loss: 2.3098 (2.3106)  acc1: 9.3750 (10.3966)  acc5: 50.7812 (50.6010)  time: 11.6814  data: 0.0179\n",
      "Epoch: [6]  [39/40]  eta: 0:00:11  lr: 1.0000000000000006e-10  img/s: 10.861735582910063  loss: 2.3095 (2.3078)  acc1: 10.1562 (10.4600)  acc5: 50.7812 (50.6400)  time: 11.1434  data: 0.0171\n",
      "Epoch: [6] Total time: 0:07:33\n",
      "Epoch 8/15\n",
      "Epoch: [7]  [ 0/40]  eta: 0:07:31  lr: 1.0000000000000006e-11  img/s: 11.351011947116938  loss: 2.3224 (2.3224)  acc1: 7.0312 (7.0312)  acc5: 49.2188 (49.2188)  time: 11.2943  data: 0.0177\n",
      "Epoch: [7]  [ 1/40]  eta: 0:07:31  lr: 1.0000000000000006e-11  img/s: 10.83165275241378  loss: 2.3025 (2.3125)  acc1: 7.0312 (8.9844)  acc5: 49.2188 (51.1719)  time: 11.5645  data: 0.0176\n",
      "Epoch: [7]  [ 2/40]  eta: 0:07:20  lr: 1.0000000000000006e-11  img/s: 11.009483269415812  loss: 2.3025 (2.3057)  acc1: 10.9375 (10.6771)  acc5: 53.1250 (51.8229)  time: 11.5911  data: 0.0177\n",
      "Epoch: [7]  [ 3/40]  eta: 0:07:11  lr: 1.0000000000000006e-11  img/s: 10.826934249682953  loss: 2.3025 (2.3076)  acc1: 7.0312 (9.7656)  acc5: 49.2188 (51.1719)  time: 11.6533  data: 0.0177\n",
      "Epoch: [7]  [ 4/40]  eta: 0:06:59  lr: 1.0000000000000006e-11  img/s: 11.053899564414875  loss: 2.3029 (2.3067)  acc1: 10.1562 (9.8438)  acc5: 49.2188 (50.4688)  time: 11.6422  data: 0.0177\n",
      "Epoch: [7]  [ 5/40]  eta: 0:06:47  lr: 1.0000000000000006e-11  img/s: 11.070390569240054  loss: 2.3029 (2.3087)  acc1: 9.3750 (9.7656)  acc5: 49.2188 (49.8698)  time: 11.6319  data: 0.0178\n",
      "Epoch: [7]  [ 6/40]  eta: 0:06:36  lr: 1.0000000000000006e-11  img/s: 10.82983048309188  loss: 2.3134 (2.3110)  acc1: 10.1562 (9.8214)  acc5: 49.2188 (48.5491)  time: 11.6612  data: 0.0178\n",
      "Epoch: [7]  [ 7/40]  eta: 0:06:24  lr: 1.0000000000000006e-11  img/s: 11.005737458903829  loss: 2.3114 (2.3111)  acc1: 9.3750 (9.4727)  acc5: 49.2188 (48.8281)  time: 11.6595  data: 0.0178\n",
      "Epoch: [7]  [ 8/40]  eta: 0:06:12  lr: 1.0000000000000006e-11  img/s: 11.057440926092418  loss: 2.3114 (2.3111)  acc1: 10.1562 (9.8958)  acc5: 49.2188 (48.8715)  time: 11.6522  data: 0.0178\n",
      "Epoch: [7]  [ 9/40]  eta: 0:06:01  lr: 1.0000000000000006e-11  img/s: 10.976896582442585  loss: 2.3114 (2.3111)  acc1: 10.1562 (10.0000)  acc5: 49.2188 (49.3750)  time: 11.6549  data: 0.0178\n",
      "Epoch: [7]  [10/40]  eta: 0:05:49  lr: 1.0000000000000006e-11  img/s: 10.955857440348563  loss: 2.3114 (2.3108)  acc1: 10.1562 (10.0852)  acc5: 49.2188 (49.0057)  time: 11.6591  data: 0.0178\n",
      "Epoch: [7]  [11/40]  eta: 0:05:38  lr: 1.0000000000000006e-11  img/s: 10.92370120908871  loss: 2.3110 (2.3103)  acc1: 10.1562 (10.2214)  acc5: 49.2188 (49.4141)  time: 11.6654  data: 0.0178\n",
      "Epoch: [7]  [12/40]  eta: 0:05:26  lr: 1.0000000000000006e-11  img/s: 11.029498779601706  loss: 2.3114 (2.3112)  acc1: 10.1562 (9.9159)  acc5: 49.2188 (49.0986)  time: 11.6622  data: 0.0178\n",
      "Epoch: [7]  [13/40]  eta: 0:05:14  lr: 1.0000000000000006e-11  img/s: 11.102217799016719  loss: 2.3111 (2.3112)  acc1: 10.1562 (9.7656)  acc5: 49.2188 (49.0513)  time: 11.6540  data: 0.0178\n",
      "Epoch: [7]  [14/40]  eta: 0:05:02  lr: 1.0000000000000006e-11  img/s: 11.077994565937912  loss: 2.3114 (2.3115)  acc1: 10.1562 (9.5312)  acc5: 49.2188 (49.0104)  time: 11.6485  data: 0.0178\n",
      "Epoch: [7]  [15/40]  eta: 0:04:51  lr: 1.0000000000000006e-11  img/s: 11.069998180537224  loss: 2.3114 (2.3118)  acc1: 10.1562 (9.7168)  acc5: 48.4375 (48.7793)  time: 11.6443  data: 0.0178\n",
      "Epoch: [7]  [16/40]  eta: 0:04:39  lr: 1.0000000000000006e-11  img/s: 10.870963240181808  loss: 2.3114 (2.3120)  acc1: 10.1562 (9.6507)  acc5: 49.2188 (48.8051)  time: 11.6530  data: 0.0178\n",
      "Epoch: [7]  [17/40]  eta: 0:04:27  lr: 1.0000000000000006e-11  img/s: 11.12139991976166  loss: 2.3114 (2.3115)  acc1: 9.3750 (9.5920)  acc5: 49.2188 (49.3490)  time: 11.6460  data: 0.0178\n",
      "Epoch: [7]  [18/40]  eta: 0:04:16  lr: 1.0000000000000006e-11  img/s: 11.02264980585514  loss: 2.3114 (2.3114)  acc1: 9.3750 (9.5806)  acc5: 49.2188 (49.6711)  time: 11.6452  data: 0.0178\n",
      "Epoch: [7]  [19/40]  eta: 0:04:04  lr: 1.0000000000000006e-11  img/s: 10.87227753566124  loss: 2.3114 (2.3124)  acc1: 9.3750 (9.4922)  acc5: 49.2188 (49.4531)  time: 11.6525  data: 0.0178\n",
      "Epoch: [7]  [20/40]  eta: 0:03:53  lr: 1.0000000000000006e-11  img/s: 10.863870212838977  loss: 2.3111 (2.3112)  acc1: 9.3750 (9.5610)  acc5: 49.2188 (49.9628)  time: 11.6777  data: 0.0178\n",
      "Epoch: [7]  [21/40]  eta: 0:03:41  lr: 1.0000000000000006e-11  img/s: 10.812709732920663  loss: 2.3111 (2.3110)  acc1: 9.3750 (9.5881)  acc5: 49.2188 (50.0710)  time: 11.6788  data: 0.0178\n",
      "Epoch: [7]  [22/40]  eta: 0:03:30  lr: 1.0000000000000006e-11  img/s: 10.837646310348587  loss: 2.3114 (2.3114)  acc1: 9.3750 (9.5448)  acc5: 48.4375 (49.8302)  time: 11.6880  data: 0.0178\n",
      "Epoch: [7]  [23/40]  eta: 0:03:18  lr: 1.0000000000000006e-11  img/s: 10.985715851679439  loss: 2.3114 (2.3119)  acc1: 9.3750 (9.4727)  acc5: 48.4375 (49.7721)  time: 11.6795  data: 0.0178\n",
      "Epoch: [7]  [24/40]  eta: 0:03:06  lr: 1.0000000000000006e-11  img/s: 10.849259568139699  loss: 2.3114 (2.3115)  acc1: 9.3750 (9.7188)  acc5: 48.4375 (49.9375)  time: 11.6904  data: 0.0178\n",
      "Epoch: [7]  [25/40]  eta: 0:02:55  lr: 1.0000000000000006e-11  img/s: 10.964505875508598  loss: 2.3111 (2.3111)  acc1: 8.5938 (9.6755)  acc5: 49.2188 (50.0601)  time: 11.6959  data: 0.0178\n",
      "Epoch: [7]  [26/40]  eta: 0:02:43  lr: 1.0000000000000006e-11  img/s: 10.7538051032925  loss: 2.3111 (2.3111)  acc1: 8.5938 (9.6933)  acc5: 49.2188 (50.0289)  time: 11.7001  data: 0.0178\n",
      "Epoch: [7]  [27/40]  eta: 0:02:32  lr: 1.0000000000000006e-11  img/s: 10.836285255788235  loss: 2.3110 (2.3111)  acc1: 9.3750 (9.7656)  acc5: 49.2188 (49.9721)  time: 11.7092  data: 0.0178\n",
      "Epoch: [7]  [28/40]  eta: 0:02:20  lr: 1.0000000000000006e-11  img/s: 10.872860374397836  loss: 2.3092 (2.3110)  acc1: 9.3750 (9.7522)  acc5: 49.2188 (49.9461)  time: 11.7190  data: 0.0178\n",
      "Epoch: [7]  [29/40]  eta: 0:02:08  lr: 1.0000000000000006e-11  img/s: 10.711668460188553  loss: 2.3091 (2.3108)  acc1: 9.3750 (9.8698)  acc5: 49.2188 (50.0260)  time: 11.7335  data: 0.0178\n",
      "Epoch: [7]  [30/40]  eta: 0:01:57  lr: 1.0000000000000006e-11  img/s: 10.888224283856642  loss: 2.3091 (2.3105)  acc1: 9.3750 (9.8790)  acc5: 49.2188 (49.8236)  time: 11.7371  data: 0.0178\n",
      "Epoch: [7]  [31/40]  eta: 0:01:45  lr: 1.0000000000000006e-11  img/s: 10.82338164994892  loss: 2.3092 (2.3105)  acc1: 9.3750 (9.8877)  acc5: 49.2188 (49.9023)  time: 11.7425  data: 0.0178\n",
      "Epoch: [7]  [32/40]  eta: 0:01:33  lr: 1.0000000000000006e-11  img/s: 10.82265683587998  loss: 2.3092 (2.3106)  acc1: 9.3750 (9.8958)  acc5: 49.2188 (49.7159)  time: 11.7536  data: 0.0178\n",
      "Epoch: [7]  [33/40]  eta: 0:01:22  lr: 1.0000000000000006e-11  img/s: 10.922141358598394  loss: 2.3092 (2.3112)  acc1: 10.1562 (9.9724)  acc5: 49.2188 (49.4945)  time: 11.7631  data: 0.0178\n",
      "Epoch: [7]  [34/40]  eta: 0:01:10  lr: 1.0000000000000006e-11  img/s: 10.858887273283164  loss: 2.3092 (2.3113)  acc1: 10.1562 (10.0223)  acc5: 49.2188 (49.4196)  time: 11.7748  data: 0.0178\n",
      "Epoch: [7]  [35/40]  eta: 0:00:58  lr: 1.0000000000000006e-11  img/s: 10.962336443247223  loss: 2.3092 (2.3118)  acc1: 10.1562 (10.0911)  acc5: 49.2188 (49.1319)  time: 11.7805  data: 0.0178\n",
      "Epoch: [7]  [36/40]  eta: 0:00:46  lr: 1.0000000000000006e-11  img/s: 10.791856031051676  loss: 2.3091 (2.3114)  acc1: 10.1562 (10.1351)  acc5: 49.2188 (49.1343)  time: 11.7848  data: 0.0178\n",
      "Epoch: [7]  [37/40]  eta: 0:00:35  lr: 1.0000000000000006e-11  img/s: 10.868505017982692  loss: 2.3092 (2.3116)  acc1: 10.1562 (10.0123)  acc5: 48.4375 (49.0132)  time: 11.7982  data: 0.0178\n",
      "Epoch: [7]  [38/40]  eta: 0:00:23  lr: 1.0000000000000006e-11  img/s: 10.739025872173064  loss: 2.3091 (2.3109)  acc1: 10.1562 (10.0761)  acc5: 48.4375 (49.2388)  time: 11.8135  data: 0.0178\n",
      "Epoch: [7]  [39/40]  eta: 0:00:11  lr: 1.0000000000000006e-11  img/s: 11.2881840938703  loss: 2.3085 (2.3088)  acc1: 10.1562 (10.1000)  acc5: 49.2188 (49.2400)  time: 11.2595  data: 0.0170\n",
      "Epoch: [7] Total time: 0:07:38\n",
      "Epoch 9/15\n",
      "Epoch: [8]  [ 0/40]  eta: 0:07:34  lr: 1.0000000000000006e-12  img/s: 11.274062723724256  loss: 2.3131 (2.3131)  acc1: 9.3750 (9.3750)  acc5: 50.7812 (50.7812)  time: 11.3717  data: 0.0182\n",
      "Epoch: [8]  [ 1/40]  eta: 0:07:33  lr: 1.0000000000000006e-12  img/s: 10.802862847358325  loss: 2.3048 (2.3090)  acc1: 7.8125 (8.5938)  acc5: 50.7812 (52.3438)  time: 11.6190  data: 0.0179\n",
      "Epoch: [8]  [ 2/40]  eta: 0:07:22  lr: 1.0000000000000006e-12  img/s: 10.978481766502137  loss: 2.3048 (2.3046)  acc1: 9.3750 (9.8958)  acc5: 53.9062 (53.3854)  time: 11.6383  data: 0.0179\n",
      "Epoch: [8]  [ 3/40]  eta: 0:07:11  lr: 1.0000000000000006e-12  img/s: 10.96679355254055  loss: 2.3048 (2.3069)  acc1: 9.3750 (10.1562)  acc5: 50.7812 (52.7344)  time: 11.6510  data: 0.0178\n",
      "Epoch: [8]  [ 4/40]  eta: 0:07:00  lr: 1.0000000000000006e-12  img/s: 10.8705864020123  loss: 2.3131 (2.3081)  acc1: 10.9375 (10.3125)  acc5: 50.7812 (52.3438)  time: 11.6793  data: 0.0177\n",
      "Epoch: [8]  [ 5/40]  eta: 0:06:48  lr: 1.0000000000000006e-12  img/s: 11.074285148032374  loss: 2.3048 (2.3065)  acc1: 9.3750 (10.0260)  acc5: 50.7812 (52.9948)  time: 11.6621  data: 0.0177\n",
      "Epoch: [8]  [ 6/40]  eta: 0:06:36  lr: 1.0000000000000006e-12  img/s: 11.063054149897319  loss: 2.3106 (2.3071)  acc1: 10.9375 (10.1562)  acc5: 51.5625 (52.7902)  time: 11.6515  data: 0.0177\n",
      "Epoch: [8]  [ 7/40]  eta: 0:06:24  lr: 1.0000000000000006e-12  img/s: 10.948490018643215  loss: 2.3048 (2.3065)  acc1: 10.9375 (10.3516)  acc5: 50.7812 (52.3438)  time: 11.6587  data: 0.0178\n",
      "Epoch: [8]  [ 8/40]  eta: 0:06:12  lr: 1.0000000000000006e-12  img/s: 11.072381017788231  loss: 2.3106 (2.3078)  acc1: 10.9375 (10.5035)  acc5: 50.7812 (51.2153)  time: 11.6497  data: 0.0178\n",
      "Epoch: [8]  [ 9/40]  eta: 0:06:00  lr: 1.0000000000000006e-12  img/s: 11.108244085822363  loss: 2.3106 (2.3091)  acc1: 10.9375 (10.3125)  acc5: 50.7812 (50.6250)  time: 11.6389  data: 0.0178\n",
      "Epoch: [8]  [10/40]  eta: 0:05:49  lr: 1.0000000000000006e-12  img/s: 10.808207459412015  loss: 2.3106 (2.3075)  acc1: 10.9375 (10.4403)  acc5: 50.7812 (51.1364)  time: 11.6590  data: 0.0178\n",
      "Epoch: [8]  [11/40]  eta: 0:05:37  lr: 1.0000000000000006e-12  img/s: 11.070370481184701  loss: 2.3055 (2.3073)  acc1: 10.9375 (10.4167)  acc5: 50.7812 (51.1068)  time: 11.6525  data: 0.0178\n",
      "Epoch: [8]  [12/40]  eta: 0:05:26  lr: 1.0000000000000006e-12  img/s: 10.908222238106498  loss: 2.3055 (2.3070)  acc1: 10.9375 (10.3966)  acc5: 50.7812 (50.9615)  time: 11.6601  data: 0.0178\n",
      "Epoch: [8]  [13/40]  eta: 0:05:14  lr: 1.0000000000000006e-12  img/s: 10.922248016046911  loss: 2.3055 (2.3071)  acc1: 10.1562 (10.1562)  acc5: 50.7812 (51.0603)  time: 11.6656  data: 0.0177\n",
      "Epoch: [8]  [14/40]  eta: 0:05:03  lr: 1.0000000000000006e-12  img/s: 10.979359628357683  loss: 2.3075 (2.3071)  acc1: 10.1562 (9.9479)  acc5: 50.7812 (51.1979)  time: 11.6662  data: 0.0177\n",
      "Epoch: [8]  [15/40]  eta: 0:04:51  lr: 1.0000000000000006e-12  img/s: 10.938865543748634  loss: 2.3075 (2.3086)  acc1: 10.1562 (9.8633)  acc5: 50.7812 (50.9277)  time: 11.6696  data: 0.0177\n",
      "Epoch: [8]  [16/40]  eta: 0:04:39  lr: 1.0000000000000006e-12  img/s: 11.07475894127407  loss: 2.3082 (2.3092)  acc1: 10.1562 (9.8805)  acc5: 50.7812 (50.4596)  time: 11.6640  data: 0.0177\n",
      "Epoch: [8]  [17/40]  eta: 0:04:28  lr: 1.0000000000000006e-12  img/s: 11.083404900761302  loss: 2.3075 (2.3089)  acc1: 10.1562 (9.8524)  acc5: 50.7812 (50.5208)  time: 11.6586  data: 0.0177\n",
      "Epoch: [8]  [18/40]  eta: 0:04:16  lr: 1.0000000000000006e-12  img/s: 10.91155997198571  loss: 2.3082 (2.3102)  acc1: 10.1562 (9.8684)  acc5: 50.7812 (50.1645)  time: 11.6634  data: 0.0178\n",
      "Epoch: [8]  [19/40]  eta: 0:04:05  lr: 1.0000000000000006e-12  img/s: 10.84626835222782  loss: 2.3075 (2.3099)  acc1: 10.1562 (9.8047)  acc5: 50.7812 (50.1953)  time: 11.6712  data: 0.0178\n",
      "Epoch: [8]  [20/40]  eta: 0:03:53  lr: 1.0000000000000006e-12  img/s: 11.091627821656711  loss: 2.3055 (2.3087)  acc1: 10.1562 (9.6726)  acc5: 50.7812 (50.4836)  time: 11.6805  data: 0.0178\n",
      "Epoch: [8]  [21/40]  eta: 0:03:41  lr: 1.0000000000000006e-12  img/s: 10.832531768640097  loss: 2.3075 (2.3090)  acc1: 10.1562 (9.5526)  acc5: 50.7812 (50.5327)  time: 11.6789  data: 0.0178\n",
      "Epoch: [8]  [22/40]  eta: 0:03:30  lr: 1.0000000000000006e-12  img/s: 11.068687680623288  loss: 2.3082 (2.3094)  acc1: 10.1562 (9.5788)  acc5: 50.7812 (50.2717)  time: 11.6741  data: 0.0178\n",
      "Epoch: [8]  [23/40]  eta: 0:03:18  lr: 1.0000000000000006e-12  img/s: 11.111570830953964  loss: 2.3075 (2.3090)  acc1: 10.1562 (9.6680)  acc5: 50.7812 (50.3255)  time: 11.6666  data: 0.0178\n",
      "Epoch: [8]  [24/40]  eta: 0:03:06  lr: 1.0000000000000006e-12  img/s: 11.006708592559738  loss: 2.3055 (2.3088)  acc1: 10.1562 (9.7188)  acc5: 50.7812 (50.2500)  time: 11.6593  data: 0.0178\n",
      "Epoch: [8]  [25/40]  eta: 0:02:54  lr: 1.0000000000000006e-12  img/s: 11.129049260910454  loss: 2.3075 (2.3090)  acc1: 10.1562 (9.6454)  acc5: 50.7812 (50.3005)  time: 11.6565  data: 0.0178\n",
      "Epoch: [8]  [26/40]  eta: 0:02:43  lr: 1.0000000000000006e-12  img/s: 10.9969288249996  loss: 2.3075 (2.3095)  acc1: 9.3750 (9.6065)  acc5: 50.7812 (50.3762)  time: 11.6600  data: 0.0178\n",
      "Epoch: [8]  [27/40]  eta: 0:02:31  lr: 1.0000000000000006e-12  img/s: 11.127302680046931  loss: 2.3082 (2.3095)  acc1: 9.3750 (9.5982)  acc5: 50.7812 (50.5301)  time: 11.6506  data: 0.0179\n",
      "Epoch: [8]  [28/40]  eta: 0:02:19  lr: 1.0000000000000006e-12  img/s: 10.854117587738683  loss: 2.3075 (2.3094)  acc1: 9.3750 (9.7252)  acc5: 51.5625 (50.5927)  time: 11.6622  data: 0.0179\n",
      "Epoch: [8]  [29/40]  eta: 0:02:08  lr: 1.0000000000000006e-12  img/s: 11.066664569610092  loss: 2.3075 (2.3094)  acc1: 9.3750 (9.7396)  acc5: 51.5625 (50.5729)  time: 11.6644  data: 0.0179\n",
      "Epoch: [8]  [30/40]  eta: 0:01:56  lr: 1.0000000000000006e-12  img/s: 11.116733412439178  loss: 2.3082 (2.3094)  acc1: 9.3750 (9.7530)  acc5: 51.5625 (50.6300)  time: 11.6479  data: 0.0179\n",
      "Epoch: [8]  [31/40]  eta: 0:01:44  lr: 1.0000000000000006e-12  img/s: 10.856187537141334  loss: 2.3085 (2.3097)  acc1: 9.3750 (9.8145)  acc5: 51.5625 (50.5371)  time: 11.6593  data: 0.0179\n",
      "Epoch: [8]  [32/40]  eta: 0:01:33  lr: 1.0000000000000006e-12  img/s: 10.958834924345446  loss: 2.3100 (2.3103)  acc1: 9.3750 (9.7064)  acc5: 51.5625 (50.2841)  time: 11.6566  data: 0.0179\n",
      "Epoch: [8]  [33/40]  eta: 0:01:21  lr: 1.0000000000000006e-12  img/s: 10.863857682188458  loss: 2.3102 (2.3105)  acc1: 9.3750 (9.7426)  acc5: 50.7812 (50.1608)  time: 11.6598  data: 0.0179\n",
      "Epoch: [8]  [34/40]  eta: 0:01:09  lr: 1.0000000000000006e-12  img/s: 11.057667304320896  loss: 2.3102 (2.3100)  acc1: 10.1562 (9.7991)  acc5: 50.7812 (50.2679)  time: 11.6557  data: 0.0179\n",
      "Epoch: [8]  [35/40]  eta: 0:00:58  lr: 1.0000000000000006e-12  img/s: 11.018755722356643  loss: 2.3100 (2.3100)  acc1: 10.1562 (9.8090)  acc5: 50.7812 (50.1736)  time: 11.6514  data: 0.0179\n",
      "Epoch: [8]  [36/40]  eta: 0:00:46  lr: 1.0000000000000006e-12  img/s: 11.041936519595831  loss: 2.3097 (2.3095)  acc1: 10.1562 (9.8606)  acc5: 51.5625 (50.2745)  time: 11.6532  data: 0.0179\n",
      "Epoch: [8]  [37/40]  eta: 0:00:34  lr: 1.0000000000000006e-12  img/s: 11.10502522481086  loss: 2.3097 (2.3093)  acc1: 10.1562 (9.9301)  acc5: 51.5625 (50.4317)  time: 11.6520  data: 0.0179\n",
      "Epoch: [8]  [38/40]  eta: 0:00:23  lr: 1.0000000000000006e-12  img/s: 11.102666662144568  loss: 2.3085 (2.3091)  acc1: 10.1562 (9.8758)  acc5: 51.5625 (50.4207)  time: 11.6419  data: 0.0179\n",
      "Epoch: [8]  [39/40]  eta: 0:00:11  lr: 1.0000000000000006e-12  img/s: 11.396767753343346  loss: 2.3085 (2.3082)  acc1: 10.1562 (9.8600)  acc5: 51.5625 (50.4800)  time: 11.0862  data: 0.0171\n",
      "Epoch: [8] Total time: 0:07:35\n",
      "Epoch 10/15\n",
      "Epoch: [9]  [ 0/40]  eta: 0:07:34  lr: 1.0000000000000007e-13  img/s: 11.292905891320643  loss: 2.3198 (2.3198)  acc1: 14.8438 (14.8438)  acc5: 47.6562 (47.6562)  time: 11.3622  data: 0.0277\n",
      "Epoch: [9]  [ 1/40]  eta: 0:07:30  lr: 1.0000000000000007e-13  img/s: 10.941955558742235  loss: 2.3198 (2.3221)  acc1: 4.6875 (9.7656)  acc5: 47.6562 (47.6562)  time: 11.5390  data: 0.0227\n",
      "Epoch: [9]  [ 2/40]  eta: 0:07:20  lr: 1.0000000000000007e-13  img/s: 10.980087843825855  loss: 2.3198 (2.3196)  acc1: 7.0312 (8.8542)  acc5: 47.6562 (47.6562)  time: 11.5844  data: 0.0210\n",
      "Epoch: [9]  [ 3/40]  eta: 0:07:08  lr: 1.0000000000000007e-13  img/s: 11.105727015955361  loss: 2.3198 (2.3205)  acc1: 7.0312 (8.3984)  acc5: 47.6562 (47.2656)  time: 11.5741  data: 0.0201\n",
      "Epoch: [9]  [ 4/40]  eta: 0:06:57  lr: 1.0000000000000007e-13  img/s: 10.943909232671148  loss: 2.3198 (2.3153)  acc1: 7.0312 (9.8438)  acc5: 47.6562 (48.5938)  time: 11.6020  data: 0.0197\n",
      "Epoch: [9]  [ 5/40]  eta: 0:06:46  lr: 1.0000000000000007e-13  img/s: 10.991419165319758  loss: 2.3146 (2.3136)  acc1: 7.0312 (9.7656)  acc5: 47.6562 (49.8698)  time: 11.6122  data: 0.0194\n",
      "Epoch: [9]  [ 6/40]  eta: 0:06:34  lr: 1.0000000000000007e-13  img/s: 11.098493291187507  loss: 2.3146 (2.3101)  acc1: 9.3750 (9.9330)  acc5: 47.6562 (50.7812)  time: 11.6034  data: 0.0191\n",
      "Epoch: [9]  [ 7/40]  eta: 0:06:22  lr: 1.0000000000000007e-13  img/s: 11.055180612911087  loss: 2.3052 (2.3094)  acc1: 9.3750 (9.8633)  acc5: 47.6562 (50.5859)  time: 11.6026  data: 0.0190\n",
      "Epoch: [9]  [ 8/40]  eta: 0:06:11  lr: 1.0000000000000007e-13  img/s: 10.940147041586467  loss: 2.3077 (2.3092)  acc1: 9.3750 (9.9826)  acc5: 49.2188 (50.6944)  time: 11.6154  data: 0.0189\n",
      "Epoch: [9]  [ 9/40]  eta: 0:06:00  lr: 1.0000000000000007e-13  img/s: 10.87072132996642  loss: 2.3052 (2.3079)  acc1: 9.3750 (10.4688)  acc5: 49.2188 (50.9375)  time: 11.6331  data: 0.0188\n",
      "Epoch: [9]  [10/40]  eta: 0:05:49  lr: 1.0000000000000007e-13  img/s: 10.943787651375267  loss: 2.3052 (2.3076)  acc1: 10.9375 (10.7955)  acc5: 51.5625 (51.4915)  time: 11.6405  data: 0.0187\n",
      "Epoch: [9]  [11/40]  eta: 0:05:37  lr: 1.0000000000000007e-13  img/s: 10.888148100465838  loss: 2.3046 (2.3070)  acc1: 10.1562 (10.7422)  acc5: 51.5625 (51.7578)  time: 11.6516  data: 0.0186\n",
      "Epoch: [9]  [12/40]  eta: 0:05:26  lr: 1.0000000000000007e-13  img/s: 11.013013845497198  loss: 2.3052 (2.3087)  acc1: 10.1562 (10.4567)  acc5: 51.5625 (51.2620)  time: 11.6507  data: 0.0186\n",
      "Epoch: [9]  [13/40]  eta: 0:05:14  lr: 1.0000000000000007e-13  img/s: 11.058715050641112  loss: 2.3052 (2.3104)  acc1: 9.3750 (10.0446)  acc5: 49.2188 (50.5022)  time: 11.6466  data: 0.0185\n",
      "Epoch: [9]  [14/40]  eta: 0:05:02  lr: 1.0000000000000007e-13  img/s: 11.099007247616473  loss: 2.3052 (2.3099)  acc1: 10.1562 (10.3125)  acc5: 51.5625 (50.6771)  time: 11.6401  data: 0.0185\n",
      "Epoch: [9]  [15/40]  eta: 0:04:50  lr: 1.0000000000000007e-13  img/s: 11.055810775549382  loss: 2.3046 (2.3087)  acc1: 10.1562 (10.7422)  acc5: 51.5625 (50.9277)  time: 11.6373  data: 0.0184\n",
      "Epoch: [9]  [16/40]  eta: 0:04:39  lr: 1.0000000000000007e-13  img/s: 11.032468594240008  loss: 2.3052 (2.3101)  acc1: 10.1562 (10.5699)  acc5: 51.5625 (50.4136)  time: 11.6363  data: 0.0184\n",
      "Epoch: [9]  [17/40]  eta: 0:04:27  lr: 1.0000000000000007e-13  img/s: 11.016062942492585  loss: 2.3052 (2.3101)  acc1: 10.1562 (10.5469)  acc5: 50.0000 (50.3906)  time: 11.6364  data: 0.0183\n",
      "Epoch: [9]  [18/40]  eta: 0:04:16  lr: 1.0000000000000007e-13  img/s: 10.862658830479662  loss: 2.3077 (2.3123)  acc1: 10.1562 (10.4441)  acc5: 50.0000 (49.5066)  time: 11.6451  data: 0.0183\n",
      "Epoch: [9]  [19/40]  eta: 0:04:04  lr: 1.0000000000000007e-13  img/s: 10.965404123995558  loss: 2.3077 (2.3130)  acc1: 9.3750 (10.3125)  acc5: 49.2188 (49.1406)  time: 11.6473  data: 0.0183\n",
      "Epoch: [9]  [20/40]  eta: 0:03:52  lr: 1.0000000000000007e-13  img/s: 11.065929843439012  loss: 2.3077 (2.3128)  acc1: 9.3750 (10.3423)  acc5: 50.0000 (49.2932)  time: 11.6585  data: 0.0178\n",
      "Epoch: [9]  [21/40]  eta: 0:03:41  lr: 1.0000000000000007e-13  img/s: 10.864001016969299  loss: 2.3077 (2.3127)  acc1: 9.3750 (10.2273)  acc5: 50.7812 (49.3608)  time: 11.6629  data: 0.0180\n",
      "Epoch: [9]  [22/40]  eta: 0:03:29  lr: 1.0000000000000007e-13  img/s: 10.94202625266595  loss: 2.3052 (2.3120)  acc1: 10.1562 (10.2582)  acc5: 50.7812 (49.4226)  time: 11.6650  data: 0.0181\n",
      "Epoch: [9]  [23/40]  eta: 0:03:18  lr: 1.0000000000000007e-13  img/s: 11.057122099237827  loss: 2.3052 (2.3119)  acc1: 10.1562 (10.3841)  acc5: 51.5625 (49.5443)  time: 11.6675  data: 0.0181\n",
      "Epoch: [9]  [24/40]  eta: 0:03:06  lr: 1.0000000000000007e-13  img/s: 10.996045674096578  loss: 2.3052 (2.3116)  acc1: 10.1562 (10.4688)  acc5: 50.7812 (49.4375)  time: 11.6647  data: 0.0181\n",
      "Epoch: [9]  [25/40]  eta: 0:02:54  lr: 1.0000000000000007e-13  img/s: 11.02996444271922  loss: 2.3077 (2.3119)  acc1: 10.1562 (10.3666)  acc5: 50.7812 (49.3990)  time: 11.6627  data: 0.0181\n",
      "Epoch: [9]  [26/40]  eta: 0:02:43  lr: 1.0000000000000007e-13  img/s: 10.980908240469681  loss: 2.3080 (2.3123)  acc1: 10.1562 (10.3299)  acc5: 50.0000 (49.2477)  time: 11.6689  data: 0.0181\n",
      "Epoch: [9]  [27/40]  eta: 0:02:31  lr: 1.0000000000000007e-13  img/s: 10.925020951780624  loss: 2.3080 (2.3114)  acc1: 10.1562 (10.3237)  acc5: 50.7812 (49.4699)  time: 11.6758  data: 0.0181\n",
      "Epoch: [9]  [28/40]  eta: 0:02:19  lr: 1.0000000000000007e-13  img/s: 10.966366808424187  loss: 2.3096 (2.3115)  acc1: 10.1562 (10.2640)  acc5: 50.7812 (49.5151)  time: 11.6744  data: 0.0181\n",
      "Epoch: [9]  [29/40]  eta: 0:02:08  lr: 1.0000000000000007e-13  img/s: 11.012664819950508  loss: 2.3096 (2.3110)  acc1: 10.1562 (10.4427)  acc5: 50.7812 (49.5833)  time: 11.6668  data: 0.0181\n",
      "Epoch: [9]  [30/40]  eta: 0:01:56  lr: 1.0000000000000007e-13  img/s: 11.134928737676379  loss: 2.3097 (2.3114)  acc1: 9.3750 (10.3075)  acc5: 50.7812 (49.6724)  time: 11.6567  data: 0.0181\n",
      "Epoch: [9]  [31/40]  eta: 0:01:44  lr: 1.0000000000000007e-13  img/s: 10.939188509077145  loss: 2.3100 (2.3117)  acc1: 9.3750 (10.4004)  acc5: 50.0000 (49.5361)  time: 11.6540  data: 0.0181\n",
      "Epoch: [9]  [32/40]  eta: 0:01:33  lr: 1.0000000000000007e-13  img/s: 11.12025918380912  loss: 2.3100 (2.3117)  acc1: 9.3750 (10.3693)  acc5: 50.0000 (49.3608)  time: 11.6484  data: 0.0181\n",
      "Epoch: [9]  [33/40]  eta: 0:01:21  lr: 1.0000000000000007e-13  img/s: 10.881990489366864  loss: 2.3100 (2.3118)  acc1: 10.1562 (10.3860)  acc5: 50.7812 (49.4485)  time: 11.6578  data: 0.0181\n",
      "Epoch: [9]  [34/40]  eta: 0:01:09  lr: 1.0000000000000007e-13  img/s: 11.055081815023614  loss: 2.3113 (2.3118)  acc1: 9.3750 (10.2902)  acc5: 50.7812 (49.4866)  time: 11.6601  data: 0.0181\n",
      "Epoch: [9]  [35/40]  eta: 0:00:58  lr: 1.0000000000000007e-13  img/s: 11.030693267425839  loss: 2.3116 (2.3121)  acc1: 9.3750 (10.1128)  acc5: 50.7812 (49.5226)  time: 11.6614  data: 0.0181\n",
      "Epoch: [9]  [36/40]  eta: 0:00:46  lr: 1.0000000000000007e-13  img/s: 11.133364779690314  loss: 2.3113 (2.3117)  acc1: 9.3750 (10.1774)  acc5: 50.7812 (49.5988)  time: 11.6562  data: 0.0181\n",
      "Epoch: [9]  [37/40]  eta: 0:00:34  lr: 1.0000000000000007e-13  img/s: 11.116220805243115  loss: 2.3116 (2.3121)  acc1: 9.3750 (10.1974)  acc5: 50.7812 (49.5271)  time: 11.6509  data: 0.0181\n",
      "Epoch: [9]  [38/40]  eta: 0:00:23  lr: 1.0000000000000007e-13  img/s: 11.147201633987166  loss: 2.3113 (2.3114)  acc1: 10.1562 (10.2364)  acc5: 50.7812 (49.6995)  time: 11.6359  data: 0.0181\n",
      "Epoch: [9]  [39/40]  eta: 0:00:11  lr: 1.0000000000000007e-13  img/s: 10.690867636502436  loss: 2.3097 (2.3102)  acc1: 10.9375 (10.2400)  acc5: 50.7812 (49.7400)  time: 11.0889  data: 0.0173\n",
      "Epoch: [9] Total time: 0:07:34\n",
      "Epoch 11/15\n",
      "Epoch: [10]  [ 0/40]  eta: 0:07:33  lr: 1.0000000000000008e-14  img/s: 11.314766893267915  loss: 2.3128 (2.3128)  acc1: 8.5938 (8.5938)  acc5: 53.1250 (53.1250)  time: 11.3300  data: 0.0173\n",
      "Epoch: [10]  [ 1/40]  eta: 0:07:30  lr: 1.0000000000000008e-14  img/s: 10.905271524996454  loss: 2.2904 (2.3016)  acc1: 7.0312 (7.8125)  acc5: 53.1250 (56.6406)  time: 11.5427  data: 0.0176\n",
      "Epoch: [10]  [ 2/40]  eta: 0:07:19  lr: 1.0000000000000008e-14  img/s: 11.039391068001029  loss: 2.3128 (2.3085)  acc1: 7.8125 (7.8125)  acc5: 53.1250 (54.1667)  time: 11.5660  data: 0.0177\n",
      "Epoch: [10]  [ 3/40]  eta: 0:07:08  lr: 1.0000000000000008e-14  img/s: 11.076768101371888  loss: 2.3128 (2.3100)  acc1: 7.8125 (8.7891)  acc5: 50.0000 (53.1250)  time: 11.5679  data: 0.0177\n",
      "Epoch: [10]  [ 4/40]  eta: 0:06:55  lr: 1.0000000000000008e-14  img/s: 11.18480367616437  loss: 2.3128 (2.3078)  acc1: 8.5938 (9.8438)  acc5: 53.1250 (53.2812)  time: 11.5467  data: 0.0177\n",
      "Epoch: [10]  [ 5/40]  eta: 0:06:43  lr: 1.0000000000000008e-14  img/s: 11.132922434558033  loss: 2.3114 (2.3084)  acc1: 8.5938 (10.1562)  acc5: 53.1250 (53.5156)  time: 11.5414  data: 0.0177\n",
      "Epoch: [10]  [ 6/40]  eta: 0:06:32  lr: 1.0000000000000008e-14  img/s: 11.132420567773012  loss: 2.3114 (2.3082)  acc1: 10.1562 (10.1562)  acc5: 53.1250 (52.6786)  time: 11.5378  data: 0.0177\n",
      "Epoch: [10]  [ 7/40]  eta: 0:06:20  lr: 1.0000000000000008e-14  img/s: 11.212661750469234  loss: 2.3114 (2.3086)  acc1: 10.1562 (10.1562)  acc5: 53.1250 (53.0273)  time: 11.5247  data: 0.0177\n",
      "Epoch: [10]  [ 8/40]  eta: 0:06:08  lr: 1.0000000000000008e-14  img/s: 11.11061214698044  loss: 2.3114 (2.3071)  acc1: 10.1562 (10.4167)  acc5: 53.1250 (52.6042)  time: 11.5263  data: 0.0178\n",
      "Epoch: [10]  [ 9/40]  eta: 0:05:57  lr: 1.0000000000000008e-14  img/s: 11.047131873171397  loss: 2.3114 (2.3077)  acc1: 10.1562 (10.3125)  acc5: 50.0000 (52.2656)  time: 11.5341  data: 0.0178\n",
      "Epoch: [10]  [10/40]  eta: 0:05:46  lr: 1.0000000000000008e-14  img/s: 11.033751936431788  loss: 2.3116 (2.3087)  acc1: 10.1562 (10.2983)  acc5: 50.0000 (51.8466)  time: 11.5418  data: 0.0178\n",
      "Epoch: [10]  [11/40]  eta: 0:05:34  lr: 1.0000000000000008e-14  img/s: 11.125183853157687  loss: 2.3114 (2.3085)  acc1: 10.1562 (10.3516)  acc5: 50.0000 (51.8880)  time: 11.5402  data: 0.0178\n",
      "Epoch: [10]  [12/40]  eta: 0:05:23  lr: 1.0000000000000008e-14  img/s: 10.904786428779  loss: 2.3114 (2.3084)  acc1: 10.1562 (10.5168)  acc5: 50.0000 (51.3822)  time: 11.5568  data: 0.0178\n",
      "Epoch: [10]  [13/40]  eta: 0:05:11  lr: 1.0000000000000008e-14  img/s: 11.178227156630875  loss: 2.3114 (2.3091)  acc1: 10.1562 (10.3795)  acc5: 49.2188 (50.9487)  time: 11.5505  data: 0.0178\n",
      "Epoch: [10]  [14/40]  eta: 0:05:00  lr: 1.0000000000000008e-14  img/s: 11.04848388437091  loss: 2.3114 (2.3082)  acc1: 10.1562 (10.4167)  acc5: 49.2188 (50.6771)  time: 11.5540  data: 0.0178\n",
      "Epoch: [10]  [15/40]  eta: 0:04:48  lr: 1.0000000000000008e-14  img/s: 11.051783798763388  loss: 2.3105 (2.3084)  acc1: 10.1562 (10.3516)  acc5: 49.2188 (50.7812)  time: 11.5569  data: 0.0178\n",
      "Epoch: [10]  [16/40]  eta: 0:04:37  lr: 1.0000000000000008e-14  img/s: 10.96463597917656  loss: 2.3114 (2.3086)  acc1: 10.1562 (10.1562)  acc5: 50.0000 (50.7353)  time: 11.5648  data: 0.0178\n",
      "Epoch: [10]  [17/40]  eta: 0:04:26  lr: 1.0000000000000008e-14  img/s: 10.91834815253838  loss: 2.3114 (2.3088)  acc1: 10.1562 (9.9392)  acc5: 49.2188 (50.5642)  time: 11.5746  data: 0.0178\n",
      "Epoch: [10]  [18/40]  eta: 0:04:14  lr: 1.0000000000000008e-14  img/s: 11.13133988411583  loss: 2.3116 (2.3091)  acc1: 10.1562 (9.7862)  acc5: 50.0000 (50.9046)  time: 11.5716  data: 0.0178\n",
      "Epoch: [10]  [19/40]  eta: 0:04:03  lr: 1.0000000000000008e-14  img/s: 11.016801686743408  loss: 2.3116 (2.3106)  acc1: 9.3750 (9.6484)  acc5: 49.2188 (50.5859)  time: 11.5748  data: 0.0178\n",
      "Epoch: [10]  [20/40]  eta: 0:03:51  lr: 1.0000000000000008e-14  img/s: 11.031014425050213  loss: 2.3116 (2.3107)  acc1: 10.1562 (9.9330)  acc5: 49.2188 (50.4464)  time: 11.5894  data: 0.0178\n",
      "Epoch: [10]  [21/40]  eta: 0:03:39  lr: 1.0000000000000008e-14  img/s: 11.073947075422307  loss: 2.3120 (2.3109)  acc1: 10.1562 (9.9077)  acc5: 49.2188 (50.1065)  time: 11.5804  data: 0.0178\n",
      "Epoch: [10]  [22/40]  eta: 0:03:28  lr: 1.0000000000000008e-14  img/s: 10.925172130132056  loss: 2.3120 (2.3115)  acc1: 10.1562 (9.8166)  acc5: 49.2188 (50.0679)  time: 11.5865  data: 0.0178\n",
      "Epoch: [10]  [23/40]  eta: 0:03:16  lr: 1.0000000000000008e-14  img/s: 11.024235782349384  loss: 2.3120 (2.3116)  acc1: 10.1562 (9.8633)  acc5: 49.2188 (50.0326)  time: 11.5893  data: 0.0178\n",
      "Epoch: [10]  [24/40]  eta: 0:03:05  lr: 1.0000000000000008e-14  img/s: 10.837262590481865  loss: 2.3120 (2.3114)  acc1: 10.1562 (10.0000)  acc5: 49.2188 (50.0938)  time: 11.6076  data: 0.0178\n",
      "Epoch: [10]  [25/40]  eta: 0:02:53  lr: 1.0000000000000008e-14  img/s: 11.103042081940837  loss: 2.3120 (2.3110)  acc1: 10.1562 (10.0661)  acc5: 49.2188 (50.1803)  time: 11.6091  data: 0.0178\n",
      "Epoch: [10]  [26/40]  eta: 0:02:42  lr: 1.0000000000000008e-14  img/s: 10.80937125211455  loss: 2.3126 (2.3113)  acc1: 9.3750 (10.0405)  acc5: 49.2188 (50.1736)  time: 11.6263  data: 0.0178\n",
      "Epoch: [10]  [27/40]  eta: 0:02:30  lr: 1.0000000000000008e-14  img/s: 10.897794543621755  loss: 2.3126 (2.3111)  acc1: 9.3750 (10.1004)  acc5: 49.2188 (50.1674)  time: 11.6428  data: 0.0178\n",
      "Epoch: [10]  [28/40]  eta: 0:02:19  lr: 1.0000000000000008e-14  img/s: 10.914669415195336  loss: 2.3126 (2.3107)  acc1: 9.3750 (10.1293)  acc5: 49.2188 (50.2694)  time: 11.6532  data: 0.0178\n",
      "Epoch: [10]  [29/40]  eta: 0:02:07  lr: 1.0000000000000008e-14  img/s: 10.914133338537598  loss: 2.3120 (2.3100)  acc1: 10.1562 (10.1302)  acc5: 49.2188 (50.5990)  time: 11.6602  data: 0.0178\n",
      "Epoch: [10]  [30/40]  eta: 0:01:56  lr: 1.0000000000000008e-14  img/s: 10.90898161201585  loss: 2.3105 (2.3096)  acc1: 10.1562 (10.2319)  acc5: 50.0000 (50.8569)  time: 11.6668  data: 0.0178\n",
      "Epoch: [10]  [31/40]  eta: 0:01:44  lr: 1.0000000000000008e-14  img/s: 10.911398303672396  loss: 2.3120 (2.3102)  acc1: 10.1562 (10.2295)  acc5: 49.2188 (50.7324)  time: 11.6781  data: 0.0178\n",
      "Epoch: [10]  [32/40]  eta: 0:01:33  lr: 1.0000000000000008e-14  img/s: 11.02235515919646  loss: 2.3120 (2.3102)  acc1: 9.3750 (10.2036)  acc5: 49.2188 (50.6155)  time: 11.6719  data: 0.0178\n",
      "Epoch: [10]  [33/40]  eta: 0:01:21  lr: 1.0000000000000008e-14  img/s: 10.985071177153333  loss: 2.3120 (2.3103)  acc1: 10.1562 (10.2252)  acc5: 49.2188 (50.4825)  time: 11.6822  data: 0.0181\n",
      "Epoch: [10]  [34/40]  eta: 0:01:09  lr: 1.0000000000000008e-14  img/s: 10.807565391628287  loss: 2.3126 (2.3105)  acc1: 9.3750 (10.2009)  acc5: 49.2188 (50.4018)  time: 11.6951  data: 0.0181\n",
      "Epoch: [10]  [35/40]  eta: 0:00:58  lr: 1.0000000000000008e-14  img/s: 11.04721984500131  loss: 2.3126 (2.3105)  acc1: 9.3750 (10.1128)  acc5: 49.2188 (50.5642)  time: 11.6954  data: 0.0181\n",
      "Epoch: [10]  [36/40]  eta: 0:00:46  lr: 1.0000000000000008e-14  img/s: 11.262911921522473  loss: 2.3120 (2.3098)  acc1: 9.3750 (10.0929)  acc5: 49.2188 (50.8446)  time: 11.6799  data: 0.0181\n",
      "Epoch: [10]  [37/40]  eta: 0:00:34  lr: 1.0000000000000008e-14  img/s: 10.963795846042158  loss: 2.3111 (2.3095)  acc1: 10.1562 (10.1562)  acc5: 50.0000 (50.8635)  time: 11.6775  data: 0.0181\n",
      "Epoch: [10]  [38/40]  eta: 0:00:23  lr: 1.0000000000000008e-14  img/s: 11.070563831743552  loss: 2.3108 (2.3094)  acc1: 10.1562 (10.2364)  acc5: 50.0000 (50.8614)  time: 11.6806  data: 0.0181\n",
      "Epoch: [10]  [39/40]  eta: 0:00:11  lr: 1.0000000000000008e-14  img/s: 11.374896563133186  loss: 2.3056 (2.3068)  acc1: 10.9375 (10.2600)  acc5: 50.0000 (50.9000)  time: 11.1341  data: 0.0173\n",
      "Epoch: [10] Total time: 0:07:34\n",
      "Epoch 12/15\n",
      "Epoch: [11]  [ 0/40]  eta: 0:07:31  lr: 1.0000000000000009e-15  img/s: 11.347125145513788  loss: 2.3245 (2.3245)  acc1: 9.3750 (9.3750)  acc5: 42.9688 (42.9688)  time: 11.2978  data: 0.0174\n",
      "Epoch: [11]  [ 1/40]  eta: 0:07:29  lr: 1.0000000000000009e-15  img/s: 10.889996455741896  loss: 2.3245 (2.3268)  acc1: 9.3750 (9.3750)  acc5: 42.9688 (45.3125)  time: 11.5348  data: 0.0176\n",
      "Epoch: [11]  [ 2/40]  eta: 0:07:21  lr: 1.0000000000000009e-15  img/s: 10.856115094238975  loss: 2.3245 (2.3204)  acc1: 9.3750 (9.6354)  acc5: 47.6562 (46.6146)  time: 11.6260  data: 0.0177\n",
      "Epoch: [11]  [ 3/40]  eta: 0:07:09  lr: 1.0000000000000009e-15  img/s: 11.087580439911456  loss: 2.3078 (2.3133)  acc1: 9.3750 (10.9375)  acc5: 47.6562 (48.8281)  time: 11.6100  data: 0.0177\n",
      "Epoch: [11]  [ 4/40]  eta: 0:06:58  lr: 1.0000000000000009e-15  img/s: 10.937251671566015  loss: 2.3078 (2.3083)  acc1: 10.1562 (11.2500)  acc5: 49.2188 (50.6250)  time: 11.6322  data: 0.0177\n",
      "Epoch: [11]  [ 5/40]  eta: 0:06:47  lr: 1.0000000000000009e-15  img/s: 10.960135873035187  loss: 2.3078 (2.3092)  acc1: 9.3750 (10.8073)  acc5: 49.2188 (50.6510)  time: 11.6429  data: 0.0177\n",
      "Epoch: [11]  [ 6/40]  eta: 0:06:35  lr: 1.0000000000000009e-15  img/s: 10.991539331891532  loss: 2.3136 (2.3124)  acc1: 9.3750 (10.2679)  acc5: 49.2188 (49.5536)  time: 11.6458  data: 0.0178\n",
      "Epoch: [11]  [ 7/40]  eta: 0:06:24  lr: 1.0000000000000009e-15  img/s: 10.936811627505708  loss: 2.3078 (2.3114)  acc1: 9.3750 (10.5469)  acc5: 49.2188 (50.2930)  time: 11.6553  data: 0.0177\n",
      "Epoch: [11]  [ 8/40]  eta: 0:06:12  lr: 1.0000000000000009e-15  img/s: 11.0228496404722  loss: 2.3078 (2.3108)  acc1: 9.3750 (10.4167)  acc5: 50.0000 (50.2604)  time: 11.6524  data: 0.0177\n",
      "Epoch: [11]  [ 9/40]  eta: 0:06:00  lr: 1.0000000000000009e-15  img/s: 11.093649526156272  loss: 2.3078 (2.3109)  acc1: 9.3750 (10.6250)  acc5: 50.0000 (50.5469)  time: 11.6428  data: 0.0177\n",
      "Epoch: [11]  [10/40]  eta: 0:05:49  lr: 1.0000000000000009e-15  img/s: 10.965658553702621  loss: 2.3118 (2.3114)  acc1: 9.3750 (10.1562)  acc5: 50.0000 (50.3551)  time: 11.6471  data: 0.0177\n",
      "Epoch: [11]  [11/40]  eta: 0:05:37  lr: 1.0000000000000009e-15  img/s: 11.128013978819645  loss: 2.3078 (2.3106)  acc1: 9.3750 (10.0911)  acc5: 50.0000 (50.6510)  time: 11.6366  data: 0.0178\n",
      "Epoch: [11]  [12/40]  eta: 0:05:25  lr: 1.0000000000000009e-15  img/s: 10.99783644919392  loss: 2.3118 (2.3115)  acc1: 9.3750 (9.8558)  acc5: 50.0000 (50.2404)  time: 11.6381  data: 0.0178\n",
      "Epoch: [11]  [13/40]  eta: 0:05:14  lr: 1.0000000000000009e-15  img/s: 10.965198976245485  loss: 2.3092 (2.3114)  acc1: 9.3750 (9.7098)  acc5: 50.0000 (50.2232)  time: 11.6419  data: 0.0178\n",
      "Epoch: [11]  [14/40]  eta: 0:05:02  lr: 1.0000000000000009e-15  img/s: 11.107097314926317  loss: 2.3118 (2.3123)  acc1: 9.3750 (9.4792)  acc5: 50.0000 (49.9479)  time: 11.6353  data: 0.0178\n",
      "Epoch: [11]  [15/40]  eta: 0:04:50  lr: 1.0000000000000009e-15  img/s: 10.975072459027787  loss: 2.3118 (2.3128)  acc1: 9.3750 (9.3262)  acc5: 49.2188 (49.5605)  time: 11.6381  data: 0.0178\n",
      "Epoch: [11]  [16/40]  eta: 0:04:39  lr: 1.0000000000000009e-15  img/s: 10.891433345452036  loss: 2.3136 (2.3133)  acc1: 9.3750 (9.2371)  acc5: 50.0000 (49.6324)  time: 11.6459  data: 0.0178\n",
      "Epoch: [11]  [17/40]  eta: 0:04:27  lr: 1.0000000000000009e-15  img/s: 11.007896343261248  loss: 2.3133 (2.3133)  acc1: 9.3750 (9.3316)  acc5: 50.0000 (49.6528)  time: 11.6459  data: 0.0178\n",
      "Epoch: [11]  [18/40]  eta: 0:04:16  lr: 1.0000000000000009e-15  img/s: 11.037021953515646  loss: 2.3136 (2.3142)  acc1: 9.3750 (9.4161)  acc5: 50.0000 (49.5066)  time: 11.6442  data: 0.0178\n",
      "Epoch: [11]  [19/40]  eta: 0:04:04  lr: 1.0000000000000009e-15  img/s: 11.001519168957305  loss: 2.3136 (2.3147)  acc1: 9.3750 (9.4531)  acc5: 50.0000 (49.5703)  time: 11.6447  data: 0.0178\n",
      "Epoch: [11]  [20/40]  eta: 0:03:52  lr: 1.0000000000000009e-15  img/s: 11.077131486897544  loss: 2.3133 (2.3139)  acc1: 9.3750 (9.5610)  acc5: 50.0000 (49.9256)  time: 11.6584  data: 0.0178\n",
      "Epoch: [11]  [21/40]  eta: 0:03:41  lr: 1.0000000000000009e-15  img/s: 10.978923374087543  loss: 2.3123 (2.3138)  acc1: 9.3750 (9.5526)  acc5: 50.0000 (49.7869)  time: 11.6537  data: 0.0178\n",
      "Epoch: [11]  [22/40]  eta: 0:03:29  lr: 1.0000000000000009e-15  img/s: 10.936583486954245  loss: 2.3133 (2.3144)  acc1: 9.3750 (9.5109)  acc5: 50.0000 (49.4226)  time: 11.6493  data: 0.0178\n",
      "Epoch: [11]  [23/40]  eta: 0:03:17  lr: 1.0000000000000009e-15  img/s: 11.007142770007652  loss: 2.3133 (2.3142)  acc1: 9.3750 (9.5378)  acc5: 50.0000 (49.4141)  time: 11.6536  data: 0.0178\n",
      "Epoch: [11]  [24/40]  eta: 0:03:06  lr: 1.0000000000000009e-15  img/s: 10.876939351527488  loss: 2.3136 (2.3146)  acc1: 9.3750 (9.5625)  acc5: 49.2188 (49.3125)  time: 11.6568  data: 0.0178\n",
      "Epoch: [11]  [25/40]  eta: 0:02:54  lr: 1.0000000000000009e-15  img/s: 10.984838995971012  loss: 2.3168 (2.3147)  acc1: 9.3750 (9.6454)  acc5: 48.4375 (49.2188)  time: 11.6555  data: 0.0178\n",
      "Epoch: [11]  [26/40]  eta: 0:02:43  lr: 1.0000000000000009e-15  img/s: 11.05506223777167  loss: 2.3133 (2.3144)  acc1: 9.3750 (9.7512)  acc5: 49.2188 (49.3924)  time: 11.6521  data: 0.0178\n",
      "Epoch: [11]  [27/40]  eta: 0:02:31  lr: 1.0000000000000009e-15  img/s: 10.858826654469494  loss: 2.3133 (2.3138)  acc1: 9.3750 (9.7098)  acc5: 49.2188 (49.6931)  time: 11.6564  data: 0.0178\n",
      "Epoch: [11]  [28/40]  eta: 0:02:19  lr: 1.0000000000000009e-15  img/s: 11.153124407867157  loss: 2.3133 (2.3134)  acc1: 9.3750 (9.8599)  acc5: 49.2188 (49.7306)  time: 11.6496  data: 0.0178\n",
      "Epoch: [11]  [29/40]  eta: 0:02:08  lr: 1.0000000000000009e-15  img/s: 11.155683409694026  loss: 2.3133 (2.3131)  acc1: 9.3750 (9.8698)  acc5: 49.2188 (49.7917)  time: 11.6464  data: 0.0178\n",
      "Epoch: [11]  [30/40]  eta: 0:01:56  lr: 1.0000000000000009e-15  img/s: 10.938243070179324  loss: 2.3123 (2.3127)  acc1: 10.1562 (9.9294)  acc5: 50.0000 (50.0000)  time: 11.6478  data: 0.0178\n",
      "Epoch: [11]  [31/40]  eta: 0:01:44  lr: 1.0000000000000009e-15  img/s: 10.951135117383293  loss: 2.3133 (2.3133)  acc1: 10.1562 (9.9609)  acc5: 49.2188 (49.6582)  time: 11.6571  data: 0.0178\n",
      "Epoch: [11]  [32/40]  eta: 0:01:33  lr: 1.0000000000000009e-15  img/s: 10.856538569624405  loss: 2.3133 (2.3134)  acc1: 10.1562 (9.8958)  acc5: 49.2188 (49.4792)  time: 11.6647  data: 0.0178\n",
      "Epoch: [11]  [33/40]  eta: 0:01:21  lr: 1.0000000000000009e-15  img/s: 10.933560854957511  loss: 2.3133 (2.3128)  acc1: 10.1562 (10.0414)  acc5: 49.2188 (49.6094)  time: 11.6664  data: 0.0178\n",
      "Epoch: [11]  [34/40]  eta: 0:01:09  lr: 1.0000000000000009e-15  img/s: 10.838724765540574  loss: 2.3133 (2.3131)  acc1: 10.1562 (10.0670)  acc5: 49.2188 (49.5312)  time: 11.6806  data: 0.0178\n",
      "Epoch: [11]  [35/40]  eta: 0:00:58  lr: 1.0000000000000009e-15  img/s: 11.060070582277003  loss: 2.3123 (2.3131)  acc1: 10.9375 (10.0911)  acc5: 49.2188 (49.4792)  time: 11.6762  data: 0.0178\n",
      "Epoch: [11]  [36/40]  eta: 0:00:46  lr: 1.0000000000000009e-15  img/s: 11.07519941802646  loss: 2.3123 (2.3131)  acc1: 10.9375 (10.1351)  acc5: 47.6562 (49.3666)  time: 11.6664  data: 0.0178\n",
      "Epoch: [11]  [37/40]  eta: 0:00:34  lr: 1.0000000000000009e-15  img/s: 11.126881340686717  loss: 2.3104 (2.3127)  acc1: 10.9375 (10.2179)  acc5: 46.8750 (49.3010)  time: 11.6602  data: 0.0178\n",
      "Epoch: [11]  [38/40]  eta: 0:00:23  lr: 1.0000000000000009e-15  img/s: 11.0470436754306  loss: 2.3104 (2.3129)  acc1: 10.9375 (10.2564)  acc5: 47.6562 (49.3790)  time: 11.6597  data: 0.0178\n",
      "Epoch: [11]  [39/40]  eta: 0:00:11  lr: 1.0000000000000009e-15  img/s: 10.358876708911717  loss: 2.3088 (2.3117)  acc1: 10.9375 (10.3000)  acc5: 47.6562 (49.4200)  time: 11.1158  data: 0.0170\n",
      "Epoch: [11] Total time: 0:07:35\n",
      "Epoch 13/15\n",
      "Epoch: [12]  [ 0/40]  eta: 0:07:34  lr: 1.000000000000001e-16  img/s: 11.28366189189952  loss: 2.3059 (2.3059)  acc1: 14.8438 (14.8438)  acc5: 52.3438 (52.3438)  time: 11.3614  data: 0.0175\n",
      "Epoch: [12]  [ 1/40]  eta: 0:07:31  lr: 1.000000000000001e-16  img/s: 10.879923918683879  loss: 2.3059 (2.3164)  acc1: 6.2500 (10.5469)  acc5: 46.0938 (49.2188)  time: 11.5720  data: 0.0177\n",
      "Epoch: [12]  [ 2/40]  eta: 0:07:19  lr: 1.000000000000001e-16  img/s: 11.089039026010298  loss: 2.3241 (2.3189)  acc1: 11.7188 (10.9375)  acc5: 46.0938 (46.6146)  time: 11.5683  data: 0.0177\n",
      "Epoch: [12]  [ 3/40]  eta: 0:07:05  lr: 1.000000000000001e-16  img/s: 11.400404911490034  loss: 2.3059 (2.3122)  acc1: 11.7188 (11.1328)  acc5: 46.0938 (48.2422)  time: 11.4876  data: 0.0177\n",
      "Epoch: [12]  [ 4/40]  eta: 0:06:52  lr: 1.000000000000001e-16  img/s: 11.349075051003316  loss: 2.3092 (2.3116)  acc1: 11.7188 (10.1562)  acc5: 51.5625 (48.9062)  time: 11.4493  data: 0.0177\n",
      "Epoch: [12]  [ 5/40]  eta: 0:06:40  lr: 1.000000000000001e-16  img/s: 11.308207952129292  loss: 2.3059 (2.3102)  acc1: 11.7188 (10.8073)  acc5: 51.5625 (49.6094)  time: 11.4306  data: 0.0177\n",
      "Epoch: [12]  [ 6/40]  eta: 0:06:28  lr: 1.000000000000001e-16  img/s: 11.330784693760352  loss: 2.3092 (2.3103)  acc1: 11.7188 (11.0491)  acc5: 51.5625 (49.3304)  time: 11.4140  data: 0.0177\n",
      "Epoch: [12]  [ 7/40]  eta: 0:06:16  lr: 1.000000000000001e-16  img/s: 11.358074063859615  loss: 2.3092 (2.3107)  acc1: 11.7188 (11.3281)  acc5: 50.0000 (49.4141)  time: 11.3981  data: 0.0177\n",
      "Epoch: [12]  [ 8/40]  eta: 0:06:05  lr: 1.000000000000001e-16  img/s: 11.026795537982837  loss: 2.3104 (2.3111)  acc1: 11.7188 (11.1111)  acc5: 50.0000 (49.1319)  time: 11.4234  data: 0.0177\n",
      "Epoch: [12]  [ 9/40]  eta: 0:05:54  lr: 1.000000000000001e-16  img/s: 11.084741316970826  loss: 2.3104 (2.3120)  acc1: 11.7188 (11.3281)  acc5: 49.2188 (49.1406)  time: 11.4376  data: 0.0178\n",
      "Epoch: [12]  [10/40]  eta: 0:05:43  lr: 1.000000000000001e-16  img/s: 11.078860522960866  loss: 2.3139 (2.3127)  acc1: 11.7188 (10.8665)  acc5: 49.2188 (49.0767)  time: 11.4498  data: 0.0178\n",
      "Epoch: [12]  [11/40]  eta: 0:05:32  lr: 1.000000000000001e-16  img/s: 10.85207102146605  loss: 2.3104 (2.3116)  acc1: 11.7188 (10.8724)  acc5: 49.2188 (49.4141)  time: 11.4801  data: 0.0178\n",
      "Epoch: [12]  [12/40]  eta: 0:05:21  lr: 1.000000000000001e-16  img/s: 10.969900937183525  loss: 2.3104 (2.3102)  acc1: 11.7188 (10.6370)  acc5: 50.0000 (49.7596)  time: 11.4959  data: 0.0178\n",
      "Epoch: [12]  [13/40]  eta: 0:05:11  lr: 1.000000000000001e-16  img/s: 10.840881663235217  loss: 2.3104 (2.3104)  acc1: 10.9375 (10.6027)  acc5: 49.2188 (49.7210)  time: 11.5194  data: 0.0178\n",
      "Epoch: [12]  [14/40]  eta: 0:04:59  lr: 1.000000000000001e-16  img/s: 10.995881267377465  loss: 2.3104 (2.3103)  acc1: 10.9375 (10.6250)  acc5: 50.0000 (49.8438)  time: 11.5287  data: 0.0178\n",
      "Epoch: [12]  [15/40]  eta: 0:04:48  lr: 1.000000000000001e-16  img/s: 11.114113788855756  loss: 2.3092 (2.3102)  acc1: 10.9375 (10.4980)  acc5: 50.0000 (50.0488)  time: 11.5291  data: 0.0178\n",
      "Epoch: [12]  [16/40]  eta: 0:04:36  lr: 1.000000000000001e-16  img/s: 11.124470380523707  loss: 2.3092 (2.3091)  acc1: 10.9375 (10.7537)  acc5: 51.5625 (50.2757)  time: 11.5288  data: 0.0178\n",
      "Epoch: [12]  [17/40]  eta: 0:04:25  lr: 1.000000000000001e-16  img/s: 11.038330412338418  loss: 2.3092 (2.3093)  acc1: 10.9375 (10.7639)  acc5: 51.5625 (50.3906)  time: 11.5335  data: 0.0178\n",
      "Epoch: [12]  [18/40]  eta: 0:04:13  lr: 1.000000000000001e-16  img/s: 11.079705807450738  loss: 2.3104 (2.3103)  acc1: 10.9375 (10.6908)  acc5: 51.5625 (50.0822)  time: 11.5354  data: 0.0178\n",
      "Epoch: [12]  [19/40]  eta: 0:04:02  lr: 1.000000000000001e-16  img/s: 10.947262371382578  loss: 2.3092 (2.3099)  acc1: 10.9375 (10.5078)  acc5: 50.0000 (50.0391)  time: 11.5442  data: 0.0178\n",
      "Epoch: [12]  [20/40]  eta: 0:03:50  lr: 1.000000000000001e-16  img/s: 11.112924860055255  loss: 2.3104 (2.3101)  acc1: 10.1562 (10.3051)  acc5: 50.0000 (50.0372)  time: 11.5532  data: 0.0181\n",
      "Epoch: [12]  [21/40]  eta: 0:03:39  lr: 1.000000000000001e-16  img/s: 11.094629129478237  loss: 2.3092 (2.3099)  acc1: 10.9375 (10.4048)  acc5: 50.0000 (50.1420)  time: 11.5418  data: 0.0181\n",
      "Epoch: [12]  [22/40]  eta: 0:03:27  lr: 1.000000000000001e-16  img/s: 11.082240147382583  loss: 2.3090 (2.3098)  acc1: 10.9375 (10.4959)  acc5: 51.5625 (50.3057)  time: 11.5422  data: 0.0181\n",
      "Epoch: [12]  [23/40]  eta: 0:03:16  lr: 1.000000000000001e-16  img/s: 11.117545348230367  loss: 2.3090 (2.3095)  acc1: 10.9375 (10.5143)  acc5: 51.5625 (50.4232)  time: 11.5565  data: 0.0181\n",
      "Epoch: [12]  [24/40]  eta: 0:03:04  lr: 1.000000000000001e-16  img/s: 11.193686093730026  loss: 2.3087 (2.3091)  acc1: 10.9375 (10.5000)  acc5: 51.5625 (50.5312)  time: 11.5643  data: 0.0181\n",
      "Epoch: [12]  [25/40]  eta: 0:02:53  lr: 1.000000000000001e-16  img/s: 11.087858203537886  loss: 2.3087 (2.3085)  acc1: 10.1562 (10.4868)  acc5: 51.5625 (50.6010)  time: 11.5756  data: 0.0181\n",
      "Epoch: [12]  [26/40]  eta: 0:02:41  lr: 1.000000000000001e-16  img/s: 11.14285410667035  loss: 2.3087 (2.3088)  acc1: 10.1562 (10.5613)  acc5: 51.5625 (50.4919)  time: 11.5851  data: 0.0181\n",
      "Epoch: [12]  [27/40]  eta: 0:02:30  lr: 1.000000000000001e-16  img/s: 10.920467998223664  loss: 2.3071 (2.3088)  acc1: 10.1562 (10.4632)  acc5: 51.5625 (50.4743)  time: 11.6077  data: 0.0181\n",
      "Epoch: [12]  [28/40]  eta: 0:02:18  lr: 1.000000000000001e-16  img/s: 10.945749790334414  loss: 2.3071 (2.3087)  acc1: 10.1562 (10.3987)  acc5: 51.5625 (50.5119)  time: 11.6120  data: 0.0181\n",
      "Epoch: [12]  [29/40]  eta: 0:02:07  lr: 1.000000000000001e-16  img/s: 11.06702637997207  loss: 2.3066 (2.3085)  acc1: 10.1562 (10.5208)  acc5: 52.3438 (50.6250)  time: 11.6129  data: 0.0181\n",
      "Epoch: [12]  [30/40]  eta: 0:01:55  lr: 1.000000000000001e-16  img/s: 10.95521850080578  loss: 2.3066 (2.3085)  acc1: 10.1562 (10.5091)  acc5: 52.3438 (50.4788)  time: 11.6194  data: 0.0181\n",
      "Epoch: [12]  [31/40]  eta: 0:01:44  lr: 1.000000000000001e-16  img/s: 11.00451205340173  loss: 2.3071 (2.3087)  acc1: 10.1562 (10.4980)  acc5: 51.5625 (50.3906)  time: 11.6112  data: 0.0181\n",
      "Epoch: [12]  [32/40]  eta: 0:01:32  lr: 1.000000000000001e-16  img/s: 11.111302456353851  loss: 2.3072 (2.3091)  acc1: 10.1562 (10.4403)  acc5: 51.5625 (50.2367)  time: 11.6038  data: 0.0181\n",
      "Epoch: [12]  [33/40]  eta: 0:01:20  lr: 1.000000000000001e-16  img/s: 10.948576426447989  loss: 2.3072 (2.3094)  acc1: 10.1562 (10.4779)  acc5: 51.5625 (50.1608)  time: 11.5980  data: 0.0181\n",
      "Epoch: [12]  [34/40]  eta: 0:01:09  lr: 1.000000000000001e-16  img/s: 11.116868304811847  loss: 2.3071 (2.3091)  acc1: 10.1562 (10.4464)  acc5: 50.0000 (50.1562)  time: 11.5917  data: 0.0181\n",
      "Epoch: [12]  [35/40]  eta: 0:00:57  lr: 1.000000000000001e-16  img/s: 10.950142943922991  loss: 2.3071 (2.3091)  acc1: 10.1562 (10.3733)  acc5: 50.0000 (50.0651)  time: 11.6003  data: 0.0182\n",
      "Epoch: [12]  [36/40]  eta: 0:00:46  lr: 1.000000000000001e-16  img/s: 11.126804317674397  loss: 2.3071 (2.3086)  acc1: 10.1562 (10.4519)  acc5: 50.0000 (50.3590)  time: 11.6002  data: 0.0182\n",
      "Epoch: [12]  [37/40]  eta: 0:00:34  lr: 1.000000000000001e-16  img/s: 10.901413434980865  loss: 2.3071 (2.3090)  acc1: 10.1562 (10.4646)  acc5: 50.0000 (50.3084)  time: 11.6075  data: 0.0182\n",
      "Epoch: [12]  [38/40]  eta: 0:00:23  lr: 1.000000000000001e-16  img/s: 10.921701640823851  loss: 2.3071 (2.3092)  acc1: 10.1562 (10.3966)  acc5: 50.0000 (50.2404)  time: 11.6158  data: 0.0182\n",
      "Epoch: [12]  [39/40]  eta: 0:00:11  lr: 1.000000000000001e-16  img/s: 11.476163684911299  loss: 2.3071 (2.3091)  acc1: 10.1562 (10.3800)  acc5: 50.0000 (50.2600)  time: 11.0653  data: 0.0174\n",
      "Epoch: [12] Total time: 0:07:32\n",
      "Epoch 14/15\n",
      "Epoch: [13]  [ 0/40]  eta: 0:07:32  lr: 1.000000000000001e-17  img/s: 11.321721174530783  loss: 2.3048 (2.3048)  acc1: 10.9375 (10.9375)  acc5: 52.3438 (52.3438)  time: 11.3239  data: 0.0182\n",
      "Epoch: [13]  [ 1/40]  eta: 0:07:31  lr: 1.000000000000001e-17  img/s: 10.856397846559585  loss: 2.3048 (2.3054)  acc1: 10.9375 (11.7188)  acc5: 52.3438 (52.7344)  time: 11.5660  data: 0.0180\n",
      "Epoch: [13]  [ 2/40]  eta: 0:07:18  lr: 1.000000000000001e-17  img/s: 11.155870015706338  loss: 2.3059 (2.3110)  acc1: 10.9375 (11.4583)  acc5: 52.3438 (51.8229)  time: 11.5412  data: 0.0179\n",
      "Epoch: [13]  [ 3/40]  eta: 0:07:08  lr: 1.000000000000001e-17  img/s: 10.95564683692584  loss: 2.3059 (2.3128)  acc1: 10.9375 (10.7422)  acc5: 51.5625 (51.7578)  time: 11.5812  data: 0.0178\n",
      "Epoch: [13]  [ 4/40]  eta: 0:06:57  lr: 1.000000000000001e-17  img/s: 11.040499833324457  loss: 2.3059 (2.3108)  acc1: 10.9375 (11.2500)  acc5: 51.5625 (51.7188)  time: 11.5873  data: 0.0179\n",
      "Epoch: [13]  [ 5/40]  eta: 0:06:45  lr: 1.000000000000001e-17  img/s: 11.122052631020694  loss: 2.3059 (2.3119)  acc1: 10.9375 (11.0677)  acc5: 51.5625 (51.0417)  time: 11.5771  data: 0.0178\n",
      "Epoch: [13]  [ 6/40]  eta: 0:06:33  lr: 1.000000000000001e-17  img/s: 11.072655736709535  loss: 2.3174 (2.3135)  acc1: 10.9375 (10.4911)  acc5: 51.5625 (50.6696)  time: 11.5772  data: 0.0178\n",
      "Epoch: [13]  [ 7/40]  eta: 0:06:22  lr: 1.000000000000001e-17  img/s: 10.959217682166651  loss: 2.3079 (2.3128)  acc1: 10.1562 (10.3516)  acc5: 51.5625 (50.9766)  time: 11.5922  data: 0.0178\n",
      "Epoch: [13]  [ 8/40]  eta: 0:06:11  lr: 1.000000000000001e-17  img/s: 10.93326961594204  loss: 2.3079 (2.3116)  acc1: 10.1562 (9.8958)  acc5: 51.5625 (51.2153)  time: 11.6070  data: 0.0178\n",
      "Epoch: [13]  [ 9/40]  eta: 0:05:59  lr: 1.000000000000001e-17  img/s: 11.03540983969135  loss: 2.3079 (2.3120)  acc1: 10.1562 (10.0000)  acc5: 51.5625 (51.1719)  time: 11.6080  data: 0.0178\n",
      "Epoch: [13]  [10/40]  eta: 0:05:48  lr: 1.000000000000001e-17  img/s: 11.108386357078263  loss: 2.3157 (2.3126)  acc1: 10.1562 (9.8722)  acc5: 51.5625 (51.2074)  time: 11.6018  data: 0.0178\n",
      "Epoch: [13]  [11/40]  eta: 0:05:36  lr: 1.000000000000001e-17  img/s: 11.116834696508361  loss: 2.3079 (2.3114)  acc1: 10.1562 (10.4818)  acc5: 51.5625 (50.9766)  time: 11.5960  data: 0.0178\n",
      "Epoch: [13]  [12/40]  eta: 0:05:24  lr: 1.000000000000001e-17  img/s: 11.126595622955138  loss: 2.3079 (2.3110)  acc1: 10.9375 (10.6370)  acc5: 51.5625 (50.9014)  time: 11.5903  data: 0.0178\n",
      "Epoch: [13]  [13/40]  eta: 0:05:13  lr: 1.000000000000001e-17  img/s: 10.926242948334815  loss: 2.3079 (2.3119)  acc1: 10.1562 (10.4911)  acc5: 51.5625 (51.0603)  time: 11.6005  data: 0.0178\n",
      "Epoch: [13]  [14/40]  eta: 0:05:01  lr: 1.000000000000001e-17  img/s: 11.055654594153431  loss: 2.3157 (2.3122)  acc1: 10.1562 (10.2604)  acc5: 51.5625 (50.8854)  time: 11.6002  data: 0.0178\n",
      "Epoch: [13]  [15/40]  eta: 0:04:49  lr: 1.000000000000001e-17  img/s: 11.076568820816693  loss: 2.3157 (2.3126)  acc1: 9.3750 (10.0586)  acc5: 51.5625 (51.0254)  time: 11.5985  data: 0.0178\n",
      "Epoch: [13]  [16/40]  eta: 0:04:38  lr: 1.000000000000001e-17  img/s: 11.031041170141897  loss: 2.3170 (2.3135)  acc1: 9.3750 (9.6967)  acc5: 51.5625 (50.9651)  time: 11.5999  data: 0.0178\n",
      "Epoch: [13]  [17/40]  eta: 0:04:26  lr: 1.000000000000001e-17  img/s: 11.02046771231371  loss: 2.3157 (2.3129)  acc1: 9.3750 (9.7222)  acc5: 50.7812 (50.9115)  time: 11.6017  data: 0.0178\n",
      "Epoch: [13]  [18/40]  eta: 0:04:15  lr: 1.000000000000001e-17  img/s: 11.000903069600618  loss: 2.3164 (2.3131)  acc1: 9.3750 (9.4984)  acc5: 51.5625 (50.9868)  time: 11.6044  data: 0.0178\n",
      "Epoch: [13]  [19/40]  eta: 0:04:03  lr: 1.000000000000001e-17  img/s: 10.985570635961496  loss: 2.3164 (2.3141)  acc1: 8.5938 (9.3750)  acc5: 50.7812 (50.6250)  time: 11.6076  data: 0.0178\n",
      "Epoch: [13]  [20/40]  eta: 0:03:52  lr: 1.000000000000001e-17  img/s: 11.013736364787983  loss: 2.3164 (2.3138)  acc1: 8.5938 (9.3006)  acc5: 50.7812 (50.7068)  time: 11.6234  data: 0.0178\n",
      "Epoch: [13]  [21/40]  eta: 0:03:40  lr: 1.000000000000001e-17  img/s: 11.019253499438816  loss: 2.3164 (2.3136)  acc1: 8.5938 (9.4460)  acc5: 50.7812 (50.7812)  time: 11.6147  data: 0.0178\n",
      "Epoch: [13]  [22/40]  eta: 0:03:29  lr: 1.000000000000001e-17  img/s: 10.81035592765098  loss: 2.3157 (2.3136)  acc1: 8.5938 (9.4769)  acc5: 50.7812 (50.6793)  time: 11.6331  data: 0.0178\n",
      "Epoch: [13]  [23/40]  eta: 0:03:17  lr: 1.000000000000001e-17  img/s: 11.017602481670366  loss: 2.3138 (2.3135)  acc1: 8.5938 (9.4076)  acc5: 50.7812 (50.7812)  time: 11.6298  data: 0.0178\n",
      "Epoch: [13]  [24/40]  eta: 0:03:05  lr: 1.000000000000001e-17  img/s: 11.168091568251844  loss: 2.3138 (2.3134)  acc1: 8.5938 (9.4062)  acc5: 50.0000 (50.7500)  time: 11.6232  data: 0.0178\n",
      "Epoch: [13]  [25/40]  eta: 0:02:54  lr: 1.000000000000001e-17  img/s: 11.027096084382128  loss: 2.3138 (2.3135)  acc1: 8.5938 (9.3750)  acc5: 50.0000 (50.5108)  time: 11.6281  data: 0.0178\n",
      "Epoch: [13]  [26/40]  eta: 0:02:42  lr: 1.000000000000001e-17  img/s: 10.975758369602081  loss: 2.3126 (2.3129)  acc1: 8.5938 (9.3750)  acc5: 50.7812 (50.6944)  time: 11.6333  data: 0.0178\n",
      "Epoch: [13]  [27/40]  eta: 0:02:31  lr: 1.000000000000001e-17  img/s: 10.969314596453746  loss: 2.3126 (2.3123)  acc1: 8.5938 (9.5424)  acc5: 50.7812 (50.7254)  time: 11.6327  data: 0.0178\n",
      "Epoch: [13]  [28/40]  eta: 0:02:19  lr: 1.000000000000001e-17  img/s: 10.975338780495498  loss: 2.3138 (2.3124)  acc1: 8.5938 (9.6175)  acc5: 50.0000 (50.6735)  time: 11.6305  data: 0.0178\n",
      "Epoch: [13]  [29/40]  eta: 0:02:07  lr: 1.000000000000001e-17  img/s: 11.09670605711246  loss: 2.3126 (2.3117)  acc1: 8.5938 (9.5312)  acc5: 50.0000 (50.6510)  time: 11.6273  data: 0.0179\n",
      "Epoch: [13]  [30/40]  eta: 0:01:56  lr: 1.000000000000001e-17  img/s: 11.046084502289734  loss: 2.3126 (2.3119)  acc1: 8.5938 (9.4758)  acc5: 50.0000 (50.4788)  time: 11.6305  data: 0.0178\n",
      "Epoch: [13]  [31/40]  eta: 0:01:44  lr: 1.000000000000001e-17  img/s: 10.996463919925183  loss: 2.3129 (2.3119)  acc1: 8.5938 (9.4727)  acc5: 50.0000 (50.4639)  time: 11.6368  data: 0.0178\n",
      "Epoch: [13]  [32/40]  eta: 0:01:32  lr: 1.000000000000001e-17  img/s: 11.109349024747441  loss: 2.3138 (2.3126)  acc1: 7.8125 (9.3987)  acc5: 50.0000 (50.2604)  time: 11.6377  data: 0.0178\n",
      "Epoch: [13]  [33/40]  eta: 0:01:21  lr: 1.000000000000001e-17  img/s: 10.92069480048921  loss: 2.3138 (2.3130)  acc1: 7.8125 (9.3750)  acc5: 50.0000 (50.0460)  time: 11.6380  data: 0.0178\n",
      "Epoch: [13]  [34/40]  eta: 0:01:09  lr: 1.000000000000001e-17  img/s: 11.118310660815496  loss: 2.3129 (2.3122)  acc1: 8.5938 (9.5089)  acc5: 50.0000 (50.2232)  time: 11.6347  data: 0.0178\n",
      "Epoch: [13]  [35/40]  eta: 0:00:58  lr: 1.000000000000001e-17  img/s: 10.990619921949474  loss: 2.3129 (2.3124)  acc1: 8.5938 (9.5052)  acc5: 50.0000 (50.0868)  time: 11.6393  data: 0.0178\n",
      "Epoch: [13]  [36/40]  eta: 0:00:46  lr: 1.000000000000001e-17  img/s: 10.928791654942193  loss: 2.3126 (2.3120)  acc1: 9.3750 (9.5861)  acc5: 50.0000 (50.2323)  time: 11.6447  data: 0.0178\n",
      "Epoch: [13]  [37/40]  eta: 0:00:34  lr: 1.000000000000001e-17  img/s: 11.119590791800633  loss: 2.3129 (2.3121)  acc1: 9.3750 (9.5806)  acc5: 50.0000 (50.1850)  time: 11.6395  data: 0.0178\n",
      "Epoch: [13]  [38/40]  eta: 0:00:23  lr: 1.000000000000001e-17  img/s: 10.976447507739131  loss: 2.3129 (2.3125)  acc1: 9.3750 (9.6354)  acc5: 49.2188 (50.1002)  time: 11.6408  data: 0.0178\n",
      "Epoch: [13]  [39/40]  eta: 0:00:11  lr: 1.000000000000001e-17  img/s: 11.514169651491017  loss: 2.3126 (2.3107)  acc1: 9.3750 (9.6800)  acc5: 50.0000 (50.1200)  time: 11.0922  data: 0.0170\n",
      "Epoch: [13] Total time: 0:07:34\n",
      "Epoch 15/15\n",
      "Epoch: [14]  [ 0/40]  eta: 0:07:28  lr: 1.000000000000001e-18  img/s: 11.423915357074362  loss: 2.3055 (2.3055)  acc1: 10.1562 (10.1562)  acc5: 51.5625 (51.5625)  time: 11.2223  data: 0.0178\n",
      "Epoch: [14]  [ 1/40]  eta: 0:07:26  lr: 1.000000000000001e-18  img/s: 10.990126978156036  loss: 2.3055 (2.3134)  acc1: 7.0312 (8.5938)  acc5: 42.9688 (47.2656)  time: 11.4435  data: 0.0178\n",
      "Epoch: [14]  [ 2/40]  eta: 0:07:15  lr: 1.000000000000001e-18  img/s: 11.144592618056256  loss: 2.3212 (2.3164)  acc1: 7.0312 (7.8125)  acc5: 43.7500 (46.0938)  time: 11.4634  data: 0.0178\n",
      "Epoch: [14]  [ 3/40]  eta: 0:07:04  lr: 1.000000000000001e-18  img/s: 11.15022452854986  loss: 2.3055 (2.3111)  acc1: 7.0312 (9.5703)  acc5: 43.7500 (46.8750)  time: 11.4719  data: 0.0178\n",
      "Epoch: [14]  [ 4/40]  eta: 0:06:53  lr: 1.000000000000001e-18  img/s: 11.14745762630921  loss: 2.3055 (2.3082)  acc1: 10.1562 (10.9375)  acc5: 46.0938 (46.7188)  time: 11.4776  data: 0.0178\n",
      "Epoch: [14]  [ 5/40]  eta: 0:06:41  lr: 1.000000000000001e-18  img/s: 11.12830346009287  loss: 2.2966 (2.3058)  acc1: 10.1562 (10.8073)  acc5: 46.0938 (48.5677)  time: 11.4846  data: 0.0178\n",
      "Epoch: [14]  [ 6/40]  eta: 0:06:30  lr: 1.000000000000001e-18  img/s: 11.149051477683432  loss: 2.2966 (2.3039)  acc1: 10.1562 (11.2723)  acc5: 49.2188 (49.2188)  time: 11.4866  data: 0.0178\n",
      "Epoch: [14]  [ 7/40]  eta: 0:06:19  lr: 1.000000000000001e-18  img/s: 11.091292813826685  loss: 2.2966 (2.3040)  acc1: 10.1562 (11.0352)  acc5: 49.2188 (49.6094)  time: 11.4956  data: 0.0178\n",
      "Epoch: [14]  [ 8/40]  eta: 0:06:07  lr: 1.000000000000001e-18  img/s: 11.133456208224207  loss: 2.3047 (2.3053)  acc1: 10.1562 (10.9375)  acc5: 49.2188 (49.5660)  time: 11.4977  data: 0.0178\n",
      "Epoch: [14]  [ 9/40]  eta: 0:05:56  lr: 1.000000000000001e-18  img/s: 11.134333398171336  loss: 2.3047 (2.3068)  acc1: 10.1562 (10.9375)  acc5: 49.2188 (49.8438)  time: 11.4993  data: 0.0178\n",
      "Epoch: [14]  [10/40]  eta: 0:05:45  lr: 1.000000000000001e-18  img/s: 11.140375191395437  loss: 2.3047 (2.3064)  acc1: 10.1562 (10.5824)  acc5: 51.5625 (50.0000)  time: 11.5001  data: 0.0178\n",
      "Epoch: [14]  [11/40]  eta: 0:05:33  lr: 1.000000000000001e-18  img/s: 11.126985346506236  loss: 2.3027 (2.3056)  acc1: 10.1562 (10.4167)  acc5: 51.5625 (50.3906)  time: 11.5019  data: 0.0178\n",
      "Epoch: [14]  [12/40]  eta: 0:05:22  lr: 1.000000000000001e-18  img/s: 11.153291001575756  loss: 2.3027 (2.3051)  acc1: 10.1562 (10.5168)  acc5: 51.5625 (50.3606)  time: 11.5013  data: 0.0178\n",
      "Epoch: [14]  [13/40]  eta: 0:05:11  lr: 1.000000000000001e-18  img/s: 10.868450452343545  loss: 2.3027 (2.3066)  acc1: 10.1562 (10.4353)  acc5: 50.0000 (50.1674)  time: 11.5223  data: 0.0178\n",
      "Epoch: [14]  [14/40]  eta: 0:04:59  lr: 1.000000000000001e-18  img/s: 11.010001884865153  loss: 2.3027 (2.3062)  acc1: 10.1562 (10.6771)  acc5: 51.5625 (50.5208)  time: 11.5304  data: 0.0178\n",
      "Epoch: [14]  [15/40]  eta: 0:04:48  lr: 1.000000000000001e-18  img/s: 11.004065001740065  loss: 2.3027 (2.3063)  acc1: 10.1562 (10.6445)  acc5: 50.0000 (50.2930)  time: 11.5378  data: 0.0178\n",
      "Epoch: [14]  [16/40]  eta: 0:04:37  lr: 1.000000000000001e-18  img/s: 10.865225012605276  loss: 2.3027 (2.3061)  acc1: 10.1562 (10.5239)  acc5: 51.5625 (50.5515)  time: 11.5532  data: 0.0178\n",
      "Epoch: [14]  [17/40]  eta: 0:04:25  lr: 1.000000000000001e-18  img/s: 11.102490556529618  loss: 2.3027 (2.3071)  acc1: 10.1562 (10.4167)  acc5: 50.0000 (50.2604)  time: 11.5528  data: 0.0178\n",
      "Epoch: [14]  [18/40]  eta: 0:04:14  lr: 1.000000000000001e-18  img/s: 10.895215166587814  loss: 2.3047 (2.3081)  acc1: 10.1562 (10.4030)  acc5: 50.0000 (49.8355)  time: 11.5640  data: 0.0178\n",
      "Epoch: [14]  [19/40]  eta: 0:04:02  lr: 1.000000000000001e-18  img/s: 10.965712755922791  loss: 2.3047 (2.3095)  acc1: 10.1562 (10.2734)  acc5: 49.2188 (49.3359)  time: 11.5704  data: 0.0178\n",
      "Epoch: [14]  [20/40]  eta: 0:03:51  lr: 1.000000000000001e-18  img/s: 11.011468586981593  loss: 2.3037 (2.3092)  acc1: 9.3750 (10.1935)  acc5: 49.2188 (49.7024)  time: 11.5914  data: 0.0178\n",
      "Epoch: [14]  [21/40]  eta: 0:03:40  lr: 1.000000000000001e-18  img/s: 10.880120816618689  loss: 2.3027 (2.3081)  acc1: 10.1562 (10.3338)  acc5: 50.0000 (50.2486)  time: 11.5972  data: 0.0178\n",
      "Epoch: [14]  [22/40]  eta: 0:03:28  lr: 1.000000000000001e-18  img/s: 10.859000605924207  loss: 2.3027 (2.3088)  acc1: 10.1562 (10.2242)  acc5: 50.0000 (50.1019)  time: 11.6124  data: 0.0178\n",
      "Epoch: [14]  [23/40]  eta: 0:03:17  lr: 1.000000000000001e-18  img/s: 11.071318123280001  loss: 2.3037 (2.3091)  acc1: 9.3750 (10.1562)  acc5: 50.0000 (50.0651)  time: 11.6165  data: 0.0178\n",
      "Epoch: [14]  [24/40]  eta: 0:03:05  lr: 1.000000000000001e-18  img/s: 11.025936568410371  loss: 2.3047 (2.3091)  acc1: 9.3750 (10.0625)  acc5: 51.5625 (50.2188)  time: 11.6228  data: 0.0178\n",
      "Epoch: [14]  [25/40]  eta: 0:02:53  lr: 1.000000000000001e-18  img/s: 11.083461417317698  loss: 2.3080 (2.3093)  acc1: 8.5938 (10.0060)  acc5: 50.0000 (49.9099)  time: 11.6251  data: 0.0178\n",
      "Epoch: [14]  [26/40]  eta: 0:02:42  lr: 1.000000000000001e-18  img/s: 11.017656294199224  loss: 2.3089 (2.3095)  acc1: 8.5938 (9.8380)  acc5: 49.2188 (49.7396)  time: 11.6320  data: 0.0178\n",
      "Epoch: [14]  [27/40]  eta: 0:02:30  lr: 1.000000000000001e-18  img/s: 10.84703731439555  loss: 2.3089 (2.3094)  acc1: 8.5938 (9.8493)  acc5: 49.2188 (49.6373)  time: 11.6450  data: 0.0179\n",
      "Epoch: [14]  [28/40]  eta: 0:02:19  lr: 1.000000000000001e-18  img/s: 10.938030246382242  loss: 2.3081 (2.3093)  acc1: 8.5938 (9.8869)  acc5: 48.4375 (49.5959)  time: 11.6552  data: 0.0179\n",
      "Epoch: [14]  [29/40]  eta: 0:02:07  lr: 1.000000000000001e-18  img/s: 10.868962686847874  loss: 2.3080 (2.3091)  acc1: 8.5938 (9.8958)  acc5: 48.4375 (49.5573)  time: 11.6693  data: 0.0179\n",
      "Epoch: [14]  [30/40]  eta: 0:01:56  lr: 1.000000000000001e-18  img/s: 10.94348605219023  loss: 2.3080 (2.3090)  acc1: 8.5938 (9.9294)  acc5: 48.4375 (49.5212)  time: 11.6796  data: 0.0179\n",
      "Epoch: [14]  [31/40]  eta: 0:01:44  lr: 1.000000000000001e-18  img/s: 10.861204693233343  loss: 2.3080 (2.3086)  acc1: 9.3750 (9.9854)  acc5: 48.4375 (49.5850)  time: 11.6937  data: 0.0179\n",
      "Epoch: [14]  [32/40]  eta: 0:01:32  lr: 1.000000000000001e-18  img/s: 11.178011873819663  loss: 2.3081 (2.3090)  acc1: 8.5938 (9.8958)  acc5: 47.6562 (49.5265)  time: 11.6924  data: 0.0178\n",
      "Epoch: [14]  [33/40]  eta: 0:01:21  lr: 1.000000000000001e-18  img/s: 11.071303511329697  loss: 2.3081 (2.3091)  acc1: 8.5938 (9.9265)  acc5: 47.6562 (49.4256)  time: 11.6816  data: 0.0178\n",
      "Epoch: [14]  [34/40]  eta: 0:01:09  lr: 1.000000000000001e-18  img/s: 11.008590426075628  loss: 2.3089 (2.3094)  acc1: 8.5938 (9.8884)  acc5: 47.6562 (49.4196)  time: 11.6817  data: 0.0178\n",
      "Epoch: [14]  [35/40]  eta: 0:00:58  lr: 1.000000000000001e-18  img/s: 11.05979101939573  loss: 2.3100 (2.3094)  acc1: 8.5938 (9.9175)  acc5: 48.4375 (49.3924)  time: 11.6787  data: 0.0178\n",
      "Epoch: [14]  [36/40]  eta: 0:00:46  lr: 1.000000000000001e-18  img/s: 11.094061474928036  loss: 2.3100 (2.3092)  acc1: 8.5938 (9.9240)  acc5: 48.4375 (49.4510)  time: 11.6666  data: 0.0178\n",
      "Epoch: [14]  [37/40]  eta: 0:00:34  lr: 1.000000000000001e-18  img/s: 11.076451586867606  loss: 2.3100 (2.3093)  acc1: 10.1562 (9.9507)  acc5: 48.4375 (49.4449)  time: 11.6680  data: 0.0179\n",
      "Epoch: [14]  [38/40]  eta: 0:00:23  lr: 1.000000000000001e-18  img/s: 11.073291775885304  loss: 2.3100 (2.3095)  acc1: 8.5938 (9.8958)  acc5: 48.4375 (49.5393)  time: 11.6585  data: 0.0179\n",
      "Epoch: [14]  [39/40]  eta: 0:00:11  lr: 1.000000000000001e-18  img/s: 11.303912456276818  loss: 2.3081 (2.3074)  acc1: 10.1562 (9.9200)  acc5: 48.4375 (49.5800)  time: 11.1095  data: 0.0171\n",
      "Epoch: [14] Total time: 0:07:33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [01:49<00:00, 12.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109.72209514304996\n",
      "Accuracy of the network on the 10000 test images: 10.3190 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "axx_mult = 'appro2'\n",
    "model = densenet121(pretrained=True, axx_mult = axx_mult)\n",
    "model.eval() # for evaluation\n",
    "# It is a bit slow since we collect histograms on CPU\n",
    "with torch.no_grad():\n",
    "    stats = collect_stats(model, data_t, num_batches=2)\n",
    "    amax = compute_amax(model, method=\"percentile\", percentile=99.99)\n",
    "    \n",
    "# Inference without retraining\n",
    "import timeit\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "model.eval()\n",
    "start_time = timeit.default_timer()\n",
    "with torch.no_grad():\n",
    "    for iteraction, (images, labels) in tqdm(enumerate(data), total=len(data)):\n",
    "        images, labels = images.to(\"cpu\"), labels.to(\"cpu\")\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(timeit.default_timer() - start_time)\n",
    "print('Accuracy of the network on the 10000 test images: %.4f %%' % (\n",
    "    100 * correct / total))\n",
    "\n",
    "# Retraining for 15 epochs\n",
    "from adapt.references.classification.train import evaluate, train_one_epoch, load_data\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "for epoch in range(15):\n",
    "    print(f\"Epoch {epoch + 1}/15\")\n",
    "    train_one_epoch(model, criterion, optimizer, data_t, \"cpu\", epoch, 1)\n",
    "    lr_scheduler.step()\n",
    "\n",
    "# Inference after retraining\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "model.eval()\n",
    "start_time = timeit.default_timer()\n",
    "with torch.no_grad():\n",
    "    for iteraction, (images, labels) in tqdm(enumerate(data), total=len(data)):\n",
    "        images, labels = images.to(\"cpu\"), labels.to(\"cpu\")\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(timeit.default_timer() - start_time)\n",
    "print('Accuracy of the network on the 10000 test images: %.4f %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6ca82eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/PyInit_conv2d_appro3/build.ninja...\n",
      "Building extension module PyInit_conv2d_appro3...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_appro3, skipping build step...\n",
      "Loading extension module PyInit_conv2d_appro3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [00:06<00:00,  3.01s/it]\n",
      "W0222 06:20:35.301978 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.302585 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.303064 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.303492 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.303942 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.304349 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.304764 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.306354 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.306749 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.307162 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.307581 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.307999 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.308409 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.308805 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.309224 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.309650 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.310054 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.310391 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.310625 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.310848 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.311073 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.311295 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.311534 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.311762 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.311984 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.312207 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.312461 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.312738 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.313041 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.313313 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.313591 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.313858 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.314139 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.314403 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.314672 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.314936 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.315210 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.315468 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.315751 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.316015 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.316293 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.316553 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.316824 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.317085 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.317365 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.317627 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.317899 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.318186 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.318471 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.318737 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.319010 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.319279 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.319671 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.320045 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.320435 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.320813 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.321196 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.321564 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.321941 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.322299 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.322685 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.323057 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.323437 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.323821 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.324202 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.324571 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.324949 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.325311 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.325695 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.326068 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.326452 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.326828 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.327214 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.327585 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.327964 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.328359 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.328744 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.329111 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.329498 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.329864 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.330239 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.330597 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.330981 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.331348 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.331732 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.332101 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.332475 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.332846 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.333211 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.333580 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.333959 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.334325 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.334699 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.335072 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.335417 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.335692 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.335965 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.336225 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.336494 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.336753 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.337020 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.337278 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.337551 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.337809 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.338077 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.338354 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.338628 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.338887 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.339164 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.339423 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.339702 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.339962 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.340227 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.340493 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.340765 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.341022 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.341290 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.341550 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.341816 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.342074 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.342343 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.342599 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.342869 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.343285 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.343569 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.343836 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.344111 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.344390 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.344765 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.345126 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.345508 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.345872 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.346248 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.346614 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.346991 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.347352 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.347731 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.348107 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.348501 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.348865 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.349239 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.349605 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.349976 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.350334 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.350709 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.351072 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.351444 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.351859 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.352229 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.352528 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.352821 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.353098 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.353375 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.353641 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.353917 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.354182 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.354455 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.354717 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.354996 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.355265 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.355543 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.355818 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.356100 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.356364 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.356635 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.356896 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.357174 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.357435 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.357709 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.357971 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.358246 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.358515 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.358784 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.359046 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.359323 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.359595 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.359880 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.360144 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.360455 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.360823 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.361193 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.361563 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.361943 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.362312 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.362693 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.363059 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.363425 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.363799 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.364184 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.364551 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.364922 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.365290 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.365674 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.366043 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.366410 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.366781 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.367158 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.367526 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.367901 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.368269 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.368649 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.369014 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.369376 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.369638 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.369910 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.370167 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.370434 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.370693 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.370965 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.371219 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.371485 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.371748 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.372021 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.372276 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.372540 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.372797 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.373068 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.373327 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.373594 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.373853 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.374123 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.374381 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.374650 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.374909 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.375177 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.375435 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.375706 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.375973 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.376255 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.376519 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.376792 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.377063 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.377339 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.377711 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.378087 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.378459 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.378853 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.379224 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.379606 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.380099 140182088872128 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0222 06:20:35.396946 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.397621 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.398011 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.398563 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.398927 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.399269 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.399921 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.400262 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.400807 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.401151 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.401497 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.401826 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.402539 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.402990 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.403387 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.403725 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.404060 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.404388 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.405265 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.405818 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.406395 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.407148 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.407934 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.408704 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.409502 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.410273 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.411066 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.411844 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.412649 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.413434 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.414210 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.414975 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.415759 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.416528 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.417312 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.418071 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.418848 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.419628 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.420399 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.421159 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.421938 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.422698 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.423463 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.424231 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.425006 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.425762 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.426535 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.427291 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.428079 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.428840 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.429611 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.430375 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.431141 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.431917 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.432694 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.433454 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.434224 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.434982 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.435751 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.436518 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.437286 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.438038 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.438809 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.439578 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.440366 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.441128 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.441910 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.442657 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.443440 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.444218 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.444987 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.445747 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.446526 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.447281 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.448058 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.448813 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.449585 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.450353 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.451145 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.451913 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.452685 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.453436 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.454214 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.454973 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.455755 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.456512 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.457286 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.458042 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.458811 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.459574 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.460380 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.461145 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.461935 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.462690 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.463457 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.464226 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.464996 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.465895 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.466687 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.467446 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.468229 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.468988 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.469754 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.470514 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.471285 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.472053 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.472846 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.473609 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.474392 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.475154 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.475937 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.476701 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.477488 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.478246 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.479026 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.479598 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.480566 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.481329 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.482127 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.482879 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.483661 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.484410 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.485190 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.485945 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.486712 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.487457 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.488241 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.488997 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.489763 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.490525 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.491296 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.492053 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.492830 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.493577 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.494354 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.495109 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.495892 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.496650 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.497433 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.498194 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.498969 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.499734 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.500535 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.501276 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.502066 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.502851 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.503675 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.504457 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.505255 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.506019 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.506805 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.507587 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.508378 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.509155 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.509948 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.510730 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.511540 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.512351 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.513158 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.513955 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.514762 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.515342 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.516344 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.517143 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.517947 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.518731 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.519544 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.520317 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.521110 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.521889 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.522682 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.523461 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.524269 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.524840 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.525818 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.526615 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.527433 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.528217 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.529010 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.529784 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.530576 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.531354 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.532152 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.532922 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.533716 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.534492 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.535283 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.536075 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.536881 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.537666 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.538457 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.539240 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.540046 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.540827 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.541621 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.542409 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.543216 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.544031 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.544842 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.545645 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.546452 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.547250 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.548066 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.548859 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.549657 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.550440 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.551239 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.552025 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.552817 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.553603 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.554408 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.555207 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.556037 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.556843 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.557655 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.558458 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.559278 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.560084 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.560878 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.561658 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.562459 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.563242 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.564058 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.564853 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.565646 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.566425 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.567215 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.568009 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.568800 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.569586 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.570379 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.571176 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.572003 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.572801 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.573605 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.574393 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.575198 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.575993 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.576798 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0222 06:20:35.577580 140182088872128 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.conv0.quantizer                : TensorQuantizer(8bit per-tensor amax=2.1255 calibrator=HistogramCalibrator quant)\n",
      "features.conv0.quantizer_w              : TensorQuantizer(8bit per-tensor amax=0.1544 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer1.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.5012 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer1.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0649 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer1.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.1368 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer1.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0350 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer2.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.4310 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer2.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0556 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer2.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.1235 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer2.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0295 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer3.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.2516 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer3.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0524 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer3.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.1061 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer3.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0293 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer4.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.2366 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer4.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0360 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer4.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.1328 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer4.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0259 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer5.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.1749 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer5.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0332 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer5.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0794 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer5.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0227 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer6.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.1622 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer6.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0406 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer6.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0917 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock1.denselayer6.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0247 calibrator=HistogramCalibrator quant)\n",
      "features.transition1.conv.quantizer     : TensorQuantizer(8bit per-tensor amax=0.2217 calibrator=HistogramCalibrator quant)\n",
      "features.transition1.conv.quantizer_w   : TensorQuantizer(8bit per-tensor amax=0.0519 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer1.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.1803 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer1.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0308 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer1.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0897 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer1.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0197 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer2.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.1544 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer2.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0267 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer2.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0782 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer2.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0230 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer3.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.1490 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer3.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0284 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer3.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0797 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer3.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0236 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer4.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.1165 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer4.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0266 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer4.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0838 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer4.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0222 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer5.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.1087 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer5.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0265 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer5.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0766 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer5.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0243 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer6.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.1123 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer6.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0289 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer6.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0799 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer6.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0233 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer7.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0999 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer7.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0265 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer7.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0787 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer7.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0243 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer8.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0973 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer8.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0293 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer8.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0766 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer8.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0282 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer9.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0978 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer9.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0266 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer9.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0751 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer9.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0246 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer10.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0997 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer10.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0283 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer10.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0801 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer10.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0228 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer11.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0985 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer11.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0305 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer11.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0904 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer11.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0218 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer12.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0954 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer12.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0251 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer12.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0844 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock2.denselayer12.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0244 calibrator=HistogramCalibrator quant)\n",
      "features.transition2.conv.quantizer     : TensorQuantizer(8bit per-tensor amax=0.1699 calibrator=HistogramCalibrator quant)\n",
      "features.transition2.conv.quantizer_w   : TensorQuantizer(8bit per-tensor amax=0.0310 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer1.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0714 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer1.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0260 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer1.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0592 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer1.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0208 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer2.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0936 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer2.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0287 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer2.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0615 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer2.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0174 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer3.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0750 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer3.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0251 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer3.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0609 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer3.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0235 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer4.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0827 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer4.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0288 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer4.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0734 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer4.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0217 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer5.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0812 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer5.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0237 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer5.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0661 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer5.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0202 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer6.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0809 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer6.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0209 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer6.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0442 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer6.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0242 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer7.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0729 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer7.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0236 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer7.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.1022 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer7.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0146 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer8.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0692 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer8.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0249 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer8.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0738 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer8.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0200 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer9.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0724 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer9.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0181 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer9.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0758 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer9.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0135 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer10.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0707 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer10.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0194 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer10.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0746 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer10.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0137 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer11.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0681 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer11.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0220 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer11.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0670 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer11.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0175 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer12.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0722 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer12.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0202 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer12.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0792 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer12.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0133 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer13.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0639 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer13.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0203 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer13.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0876 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer13.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0139 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer14.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0720 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer14.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0191 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer14.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0871 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer14.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0110 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer15.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0658 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer15.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0187 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer15.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0776 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer15.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0125 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer16.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0600 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer16.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0191 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer16.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0864 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer16.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0150 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer17.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0552 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer17.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0214 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer17.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0915 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer17.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0173 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer18.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0650 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer18.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0196 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer18.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0942 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer18.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0235 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer19.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0611 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer19.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0197 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer19.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.1001 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer19.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0163 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer20.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0551 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer20.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0219 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer20.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0920 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer20.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0224 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer21.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0778 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer21.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0192 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer21.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0956 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer21.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0210 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer22.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0794 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer22.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0218 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer22.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.0827 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer22.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0200 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer23.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0831 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer23.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0194 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer23.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.1490 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer23.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0256 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer24.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0773 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer24.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0184 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer24.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.1676 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock3.denselayer24.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0201 calibrator=HistogramCalibrator quant)\n",
      "features.transition3.conv.quantizer     : TensorQuantizer(8bit per-tensor amax=0.8299 calibrator=HistogramCalibrator quant)\n",
      "features.transition3.conv.quantizer_w   : TensorQuantizer(8bit per-tensor amax=0.0380 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer1.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0588 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer1.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0102 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer1.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.2480 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer1.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0348 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer2.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0562 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer2.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0118 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer2.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.2606 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer2.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0350 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer3.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0536 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer3.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0116 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer3.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.3230 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer3.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0352 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer4.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0620 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer4.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0117 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer4.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.2858 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer4.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0349 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer5.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0547 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer5.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0119 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer5.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.3196 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer5.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0382 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer6.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0486 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer6.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0104 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer6.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.2806 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer6.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0333 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer7.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0516 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer7.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0112 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer7.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.3431 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer7.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0326 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer8.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0545 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer8.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0100 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer8.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.2920 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer8.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0319 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer9.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0562 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer9.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0100 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer9.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.3519 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer9.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0332 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer10.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0523 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer10.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0108 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer10.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.4231 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer10.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0354 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer11.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0464 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer11.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0092 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer11.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.3222 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer11.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0376 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer12.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0444 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer12.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0095 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer12.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.2727 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer12.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0363 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer13.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0535 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer13.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0101 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer13.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.2998 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer13.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0405 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer14.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0551 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer14.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0102 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer14.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.3404 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer14.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0362 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer15.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0575 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer15.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0090 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer15.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.3222 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer15.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0319 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer16.conv1.quantizer: TensorQuantizer(8bit per-tensor amax=0.0629 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer16.conv1.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0108 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer16.conv2.quantizer: TensorQuantizer(8bit per-tensor amax=0.3589 calibrator=HistogramCalibrator quant)\n",
      "features.denseblock4.denselayer16.conv2.quantizer_w: TensorQuantizer(8bit per-tensor amax=0.0330 calibrator=HistogramCalibrator quant)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [01:54<00:00, 12.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114.43707771226764\n",
      "Accuracy of the network on the 10000 test images: 10.1128 %\n",
      "Epoch 1/15\n",
      "Epoch: [0]  [ 0/40]  eta: 0:07:47  lr: 0.0001  img/s: 10.97944989219044  loss: 2.2749 (2.2749)  acc1: 14.8438 (14.8438)  acc5: 57.0312 (57.0312)  time: 11.6757  data: 0.0175\n",
      "Epoch: [0]  [ 1/40]  eta: 0:07:39  lr: 0.0001  img/s: 10.763719819928788  loss: 2.2550 (2.2649)  acc1: 14.8438 (17.1875)  acc5: 57.0312 (57.8125)  time: 11.7928  data: 0.0178\n",
      "Epoch: [0]  [ 2/40]  eta: 0:07:27  lr: 0.0001  img/s: 10.93957636013871  loss: 2.2550 (2.2484)  acc1: 19.5312 (18.2292)  acc5: 58.5938 (62.2396)  time: 11.7680  data: 0.0178\n",
      "Epoch: [0]  [ 3/40]  eta: 0:07:14  lr: 0.0001  img/s: 11.013171309571273  loss: 2.2154 (2.2314)  acc1: 19.5312 (19.3359)  acc5: 58.5938 (65.0391)  time: 11.7360  data: 0.0177\n",
      "Epoch: [0]  [ 4/40]  eta: 0:07:03  lr: 0.0001  img/s: 10.852432975823758  loss: 2.2154 (2.2058)  acc1: 20.3125 (23.1250)  acc5: 71.0938 (69.0625)  time: 11.7513  data: 0.0177\n",
      "Epoch: [0]  [ 5/40]  eta: 0:06:51  lr: 0.0001  img/s: 10.81969945764754  loss: 2.1803 (2.1732)  acc1: 20.3125 (25.5208)  acc5: 71.0938 (72.0052)  time: 11.7674  data: 0.0177\n",
      "Epoch: [0]  [ 6/40]  eta: 0:06:40  lr: 0.0001  img/s: 10.880727210010326  loss: 2.1803 (2.1260)  acc1: 22.6562 (27.2321)  acc5: 73.4375 (75.5580)  time: 11.7695  data: 0.0178\n",
      "Epoch: [0]  [ 7/40]  eta: 0:06:27  lr: 0.0001  img/s: 11.009350970466683  loss: 2.1032 (2.0770)  acc1: 22.6562 (29.7852)  acc5: 73.4375 (77.7344)  time: 11.7538  data: 0.0178\n",
      "Epoch: [0]  [ 8/40]  eta: 0:06:16  lr: 0.0001  img/s: 10.902606686639142  loss: 2.1032 (2.0061)  acc1: 37.5000 (33.5938)  acc5: 85.1562 (79.6007)  time: 11.7543  data: 0.0178\n",
      "Epoch: [0]  [ 9/40]  eta: 0:06:04  lr: 0.0001  img/s: 10.841169314456279  loss: 2.0102 (1.9335)  acc1: 37.5000 (36.7188)  acc5: 85.1562 (81.4062)  time: 11.7614  data: 0.0178\n",
      "Epoch: [0]  [10/40]  eta: 0:05:52  lr: 0.0001  img/s: 11.169691790722187  loss: 2.0102 (1.8547)  acc1: 37.5000 (40.4119)  acc5: 86.7188 (82.9545)  time: 11.7356  data: 0.0178\n",
      "Epoch: [0]  [11/40]  eta: 0:05:40  lr: 0.0001  img/s: 11.040541609344526  loss: 1.8429 (1.7797)  acc1: 37.5000 (43.0990)  acc5: 86.7188 (83.8542)  time: 11.7252  data: 0.0178\n",
      "Epoch: [0]  [12/40]  eta: 0:05:27  lr: 0.0001  img/s: 11.070055930008383  loss: 1.8429 (1.7146)  acc1: 38.2812 (45.7933)  acc5: 92.9688 (85.0361)  time: 11.7141  data: 0.0178\n",
      "Epoch: [0]  [13/40]  eta: 0:05:16  lr: 0.0001  img/s: 10.928291116951202  loss: 1.7339 (1.6586)  acc1: 38.2812 (47.6004)  acc5: 92.9688 (85.8817)  time: 11.7153  data: 0.0178\n",
      "Epoch: [0]  [14/40]  eta: 0:05:04  lr: 0.0001  img/s: 10.985698767278064  loss: 1.7339 (1.6063)  acc1: 47.6562 (49.4271)  acc5: 93.7500 (86.7188)  time: 11.7122  data: 0.0178\n",
      "Epoch: [0]  [15/40]  eta: 0:04:52  lr: 0.0001  img/s: 11.14822683130085  loss: 1.4390 (1.5671)  acc1: 47.6562 (50.6348)  acc5: 93.7500 (87.4023)  time: 11.6994  data: 0.0183\n",
      "Epoch: [0]  [16/40]  eta: 0:04:40  lr: 0.0001  img/s: 11.153781775249737  loss: 1.4390 (1.5217)  acc1: 64.0625 (52.2978)  acc5: 94.5312 (87.9596)  time: 11.6873  data: 0.0183\n",
      "Epoch: [0]  [17/40]  eta: 0:04:28  lr: 0.0001  img/s: 11.172411849857706  loss: 1.2798 (1.4826)  acc1: 64.0625 (53.5590)  acc5: 94.5312 (88.4115)  time: 11.6755  data: 0.0182\n",
      "Epoch: [0]  [18/40]  eta: 0:04:16  lr: 0.0001  img/s: 11.065084150927763  loss: 1.2798 (1.4345)  acc1: 64.8438 (55.2220)  acc5: 96.0938 (88.8980)  time: 11.6707  data: 0.0182\n",
      "Epoch: [0]  [19/40]  eta: 0:04:05  lr: 0.0001  img/s: 11.024447219996725  loss: 1.0666 (1.3906)  acc1: 64.8438 (56.7969)  acc5: 96.0938 (89.4531)  time: 11.6686  data: 0.0182\n",
      "Epoch: [0]  [20/40]  eta: 0:03:53  lr: 0.0001  img/s: 10.974550623097569  loss: 0.9790 (1.3547)  acc1: 68.7500 (58.0357)  acc5: 96.8750 (89.8438)  time: 11.6689  data: 0.0182\n",
      "Epoch: [0]  [21/40]  eta: 0:03:41  lr: 0.0001  img/s: 10.886469245021285  loss: 0.9558 (1.3200)  acc1: 71.0938 (59.1974)  acc5: 96.8750 (90.2699)  time: 11.6622  data: 0.0182\n",
      "Epoch: [0]  [22/40]  eta: 0:03:30  lr: 0.0001  img/s: 11.068341963046393  loss: 0.9334 (1.2989)  acc1: 72.6562 (60.0204)  acc5: 96.8750 (90.6250)  time: 11.6554  data: 0.0182\n",
      "Epoch: [0]  [23/40]  eta: 0:03:18  lr: 0.0001  img/s: 11.001899277841616  loss: 0.9297 (1.2697)  acc1: 75.0000 (60.9701)  acc5: 97.6562 (90.9505)  time: 11.6560  data: 0.0182\n",
      "Epoch: [0]  [24/40]  eta: 0:03:06  lr: 0.0001  img/s: 11.024326332811418  loss: 0.8745 (1.2410)  acc1: 75.0000 (61.8125)  acc5: 97.6562 (91.3125)  time: 11.6468  data: 0.0182\n",
      "Epoch: [0]  [25/40]  eta: 0:02:54  lr: 0.0001  img/s: 11.041969676561354  loss: 0.8330 (1.2205)  acc1: 77.3438 (62.6803)  acc5: 97.6562 (91.5865)  time: 11.6349  data: 0.0182\n",
      "Epoch: [0]  [26/40]  eta: 0:02:43  lr: 0.0001  img/s: 10.824844446682969  loss: 0.8178 (1.1968)  acc1: 78.1250 (63.4549)  acc5: 97.6562 (91.8113)  time: 11.6379  data: 0.0182\n",
      "Epoch: [0]  [27/40]  eta: 0:02:31  lr: 0.0001  img/s: 10.991767070505473  loss: 0.7959 (1.1825)  acc1: 78.1250 (63.9788)  acc5: 97.6562 (92.0480)  time: 11.6389  data: 0.0182\n",
      "Epoch: [0]  [28/40]  eta: 0:02:20  lr: 0.0001  img/s: 11.001184848156367  loss: 0.7948 (1.1619)  acc1: 78.1250 (64.6821)  acc5: 98.4375 (92.3222)  time: 11.6336  data: 0.0182\n",
      "Epoch: [0]  [29/40]  eta: 0:02:08  lr: 0.0001  img/s: 10.86096058050721  loss: 0.7948 (1.1503)  acc1: 78.1250 (65.0781)  acc5: 98.4375 (92.4479)  time: 11.6325  data: 0.0182\n",
      "Epoch: [0]  [30/40]  eta: 0:01:56  lr: 0.0001  img/s: 10.88108225714937  loss: 0.7948 (1.1399)  acc1: 78.1250 (65.4486)  acc5: 98.4375 (92.6411)  time: 11.6477  data: 0.0182\n",
      "Epoch: [0]  [31/40]  eta: 0:01:45  lr: 0.0001  img/s: 11.175951164213553  loss: 0.7948 (1.1316)  acc1: 78.1250 (65.7471)  acc5: 98.4375 (92.7979)  time: 11.6407  data: 0.0182\n",
      "Epoch: [0]  [32/40]  eta: 0:01:33  lr: 0.0001  img/s: 11.078315054842694  loss: 0.7084 (1.1160)  acc1: 78.9062 (66.3352)  acc5: 97.6562 (92.9214)  time: 11.6402  data: 0.0182\n",
      "Epoch: [0]  [33/40]  eta: 0:01:21  lr: 0.0001  img/s: 11.177052162479132  loss: 0.7084 (1.1066)  acc1: 78.9062 (66.6590)  acc5: 97.6562 (93.0147)  time: 11.6272  data: 0.0182\n",
      "Epoch: [0]  [34/40]  eta: 0:01:09  lr: 0.0001  img/s: 11.011288813101704  loss: 0.7084 (1.0990)  acc1: 78.9062 (66.8304)  acc5: 97.6562 (93.1473)  time: 11.6259  data: 0.0182\n",
      "Epoch: [0]  [35/40]  eta: 0:00:58  lr: 0.0001  img/s: 11.077134686624417  loss: 0.6358 (1.0861)  acc1: 81.2500 (67.2309)  acc5: 97.6562 (93.2943)  time: 11.6292  data: 0.0178\n",
      "Epoch: [0]  [36/40]  eta: 0:00:46  lr: 0.0001  img/s: 11.072646830396035  loss: 0.6343 (1.0698)  acc1: 82.0312 (67.7154)  acc5: 98.4375 (93.4755)  time: 11.6334  data: 0.0178\n",
      "Epoch: [0]  [37/40]  eta: 0:00:34  lr: 0.0001  img/s: 10.956994436243317  loss: 0.6166 (1.0569)  acc1: 82.8125 (68.1743)  acc5: 98.4375 (93.6061)  time: 11.6447  data: 0.0178\n",
      "Epoch: [0]  [38/40]  eta: 0:00:23  lr: 0.0001  img/s: 11.138451503305618  loss: 0.6166 (1.0419)  acc1: 82.8125 (68.6699)  acc5: 98.4375 (93.7099)  time: 11.6408  data: 0.0178\n",
      "Epoch: [0]  [39/40]  eta: 0:00:11  lr: 0.0001  img/s: 11.339013693556574  loss: 0.6343 (1.0617)  acc1: 82.0312 (68.6400)  acc5: 98.4375 (93.6800)  time: 11.0948  data: 0.0170\n",
      "Epoch: [0] Total time: 0:07:35\n",
      "Epoch 2/15\n",
      "Epoch: [1]  [ 0/40]  eta: 0:07:31  lr: 1e-05  img/s: 11.361396352961355  loss: 2.1946 (2.1946)  acc1: 32.0312 (32.0312)  acc5: 75.7812 (75.7812)  time: 11.2879  data: 0.0217\n",
      "Epoch: [1]  [ 1/40]  eta: 0:07:28  lr: 1e-05  img/s: 10.935374990943556  loss: 2.1946 (2.2121)  acc1: 32.0312 (32.8125)  acc5: 75.7812 (78.1250)  time: 11.5054  data: 0.0197\n",
      "Epoch: [1]  [ 2/40]  eta: 0:07:17  lr: 1e-05  img/s: 11.100406872940452  loss: 2.1946 (2.1810)  acc1: 33.5938 (34.1146)  acc5: 80.4688 (79.6875)  time: 11.5200  data: 0.0191\n",
      "Epoch: [1]  [ 3/40]  eta: 0:07:08  lr: 1e-05  img/s: 10.87463856173462  loss: 2.1187 (2.0837)  acc1: 33.5938 (36.3281)  acc5: 80.4688 (81.4453)  time: 11.5871  data: 0.0188\n",
      "Epoch: [1]  [ 4/40]  eta: 0:06:57  lr: 1e-05  img/s: 10.953287603685197  loss: 2.1187 (2.0401)  acc1: 36.7188 (37.9688)  acc5: 82.8125 (81.8750)  time: 11.6105  data: 0.0187\n",
      "Epoch: [1]  [ 5/40]  eta: 0:06:46  lr: 1e-05  img/s: 11.027104691076454  loss: 1.8658 (2.0045)  acc1: 36.7188 (38.2812)  acc5: 82.8125 (83.2031)  time: 11.6130  data: 0.0185\n",
      "Epoch: [1]  [ 6/40]  eta: 0:06:35  lr: 1e-05  img/s: 10.933559964294668  loss: 1.8658 (1.9600)  acc1: 39.8438 (39.9554)  acc5: 83.5938 (84.4866)  time: 11.6290  data: 0.0184\n",
      "Epoch: [1]  [ 7/40]  eta: 0:06:24  lr: 1e-05  img/s: 10.88184734119912  loss: 1.8264 (1.8986)  acc1: 39.8438 (41.2109)  acc5: 83.5938 (85.3516)  time: 11.6479  data: 0.0184\n",
      "Epoch: [1]  [ 8/40]  eta: 0:06:12  lr: 1e-05  img/s: 11.096280151341702  loss: 1.8264 (1.8584)  acc1: 42.9688 (42.6215)  acc5: 86.7188 (85.8507)  time: 11.6374  data: 0.0183\n",
      "Epoch: [1]  [ 9/40]  eta: 0:06:00  lr: 1e-05  img/s: 10.952676224062564  loss: 1.7919 (1.8169)  acc1: 42.9688 (44.3750)  acc5: 86.7188 (86.7188)  time: 11.6441  data: 0.0182\n",
      "Epoch: [1]  [10/40]  eta: 0:05:48  lr: 1e-05  img/s: 11.12308104284172  loss: 1.7919 (1.7589)  acc1: 44.5312 (46.0227)  acc5: 89.8438 (87.5000)  time: 11.6333  data: 0.0182\n",
      "Epoch: [1]  [11/40]  eta: 0:05:37  lr: 1e-05  img/s: 11.015529290800103  loss: 1.6931 (1.7237)  acc1: 44.5312 (47.1354)  acc5: 89.8438 (87.9557)  time: 11.6337  data: 0.0182\n",
      "Epoch: [1]  [12/40]  eta: 0:05:25  lr: 1e-05  img/s: 10.991691231598821  loss: 1.6931 (1.6833)  acc1: 50.0000 (48.2572)  acc5: 89.8438 (88.4615)  time: 11.6359  data: 0.0181\n",
      "Epoch: [1]  [13/40]  eta: 0:05:14  lr: 1e-05  img/s: 10.88658779002116  loss: 1.5366 (1.6590)  acc1: 50.0000 (48.8839)  acc5: 89.8438 (88.9509)  time: 11.6459  data: 0.0181\n",
      "Epoch: [1]  [14/40]  eta: 0:05:02  lr: 1e-05  img/s: 10.96427747378346  loss: 1.5366 (1.6357)  acc1: 50.0000 (49.5312)  acc5: 91.4062 (89.2188)  time: 11.6490  data: 0.0181\n",
      "Epoch: [1]  [15/40]  eta: 0:04:51  lr: 1e-05  img/s: 10.892828843387532  loss: 1.4687 (1.6114)  acc1: 50.0000 (50.5371)  acc5: 91.4062 (89.6484)  time: 11.6565  data: 0.0181\n",
      "Epoch: [1]  [16/40]  eta: 0:04:39  lr: 1e-05  img/s: 10.880526982463325  loss: 1.4687 (1.5822)  acc1: 53.9062 (51.3327)  acc5: 92.1875 (90.0735)  time: 11.6638  data: 0.0181\n",
      "Epoch: [1]  [17/40]  eta: 0:04:28  lr: 1e-05  img/s: 11.011355663004643  loss: 1.4437 (1.5543)  acc1: 53.9062 (52.0399)  acc5: 92.1875 (90.4948)  time: 11.6626  data: 0.0180\n",
      "Epoch: [1]  [18/40]  eta: 0:04:16  lr: 1e-05  img/s: 11.002952041499423  loss: 1.4437 (1.5359)  acc1: 57.0312 (52.5905)  acc5: 92.9688 (90.7484)  time: 11.6620  data: 0.0180\n",
      "Epoch: [1]  [19/40]  eta: 0:04:04  lr: 1e-05  img/s: 11.030130323458788  loss: 1.3429 (1.5139)  acc1: 57.0312 (53.3984)  acc5: 92.9688 (91.0156)  time: 11.6600  data: 0.0180\n",
      "Epoch: [1]  [20/40]  eta: 0:03:53  lr: 1e-05  img/s: 10.881780951724819  loss: 1.3368 (1.4899)  acc1: 58.5938 (54.2039)  acc5: 92.9688 (91.3690)  time: 11.6847  data: 0.0178\n",
      "Epoch: [1]  [21/40]  eta: 0:03:41  lr: 1e-05  img/s: 11.040084360767308  loss: 1.3103 (1.4676)  acc1: 59.3750 (55.0426)  acc5: 94.5312 (91.5483)  time: 11.6791  data: 0.0178\n",
      "Epoch: [1]  [22/40]  eta: 0:03:29  lr: 1e-05  img/s: 11.024234650478022  loss: 1.2471 (1.4453)  acc1: 60.1562 (55.7745)  acc5: 94.5312 (91.7799)  time: 11.6831  data: 0.0178\n",
      "Epoch: [1]  [23/40]  eta: 0:03:18  lr: 1e-05  img/s: 11.036558643759788  loss: 1.2055 (1.4281)  acc1: 61.7188 (56.4453)  acc5: 95.3125 (92.0247)  time: 11.6745  data: 0.0178\n",
      "Epoch: [1]  [24/40]  eta: 0:03:06  lr: 1e-05  img/s: 11.01878602644537  loss: 1.1984 (1.4065)  acc1: 62.5000 (57.0938)  acc5: 95.3125 (92.2500)  time: 11.6710  data: 0.0178\n",
      "Epoch: [1]  [25/40]  eta: 0:02:54  lr: 1e-05  img/s: 11.037645962993038  loss: 1.1785 (1.3877)  acc1: 62.5000 (57.7524)  acc5: 95.3125 (92.4579)  time: 11.6704  data: 0.0178\n",
      "Epoch: [1]  [26/40]  eta: 0:02:43  lr: 1e-05  img/s: 10.92496114855828  loss: 1.1136 (1.3683)  acc1: 64.0625 (58.3912)  acc5: 95.3125 (92.5926)  time: 11.6709  data: 0.0178\n",
      "Epoch: [1]  [27/40]  eta: 0:02:31  lr: 1e-05  img/s: 10.803993528522106  loss: 1.0965 (1.3508)  acc1: 64.0625 (58.9286)  acc5: 96.0938 (92.7455)  time: 11.6751  data: 0.0178\n",
      "Epoch: [1]  [28/40]  eta: 0:02:19  lr: 1e-05  img/s: 11.120902546698307  loss: 1.0803 (1.3319)  acc1: 65.6250 (59.5097)  acc5: 96.0938 (92.8879)  time: 11.6738  data: 0.0178\n",
      "Epoch: [1]  [29/40]  eta: 0:02:08  lr: 1e-05  img/s: 11.017986868409295  loss: 1.0516 (1.3226)  acc1: 68.7500 (59.8177)  acc5: 96.0938 (92.9427)  time: 11.6704  data: 0.0178\n",
      "Epoch: [1]  [30/40]  eta: 0:01:56  lr: 1e-05  img/s: 11.152223868522842  loss: 1.0332 (1.3027)  acc1: 68.7500 (60.3075)  acc5: 96.0938 (93.1704)  time: 11.6689  data: 0.0178\n",
      "Epoch: [1]  [31/40]  eta: 0:01:44  lr: 1e-05  img/s: 11.11516512608011  loss: 1.0299 (1.2942)  acc1: 70.3125 (60.6445)  acc5: 96.0938 (93.1152)  time: 11.6640  data: 0.0182\n",
      "Epoch: [1]  [32/40]  eta: 0:01:33  lr: 1e-05  img/s: 10.912298517800297  loss: 1.0096 (1.2780)  acc1: 71.0938 (61.1979)  acc5: 96.0938 (93.2055)  time: 11.6683  data: 0.0182\n",
      "Epoch: [1]  [33/40]  eta: 0:01:21  lr: 1e-05  img/s: 11.069399949728304  loss: 0.9988 (1.2668)  acc1: 71.8750 (61.5349)  acc5: 96.0938 (93.2675)  time: 11.6586  data: 0.0182\n",
      "Epoch: [1]  [34/40]  eta: 0:01:09  lr: 1e-05  img/s: 11.037729245230883  loss: 0.9547 (1.2562)  acc1: 71.8750 (61.8304)  acc5: 96.8750 (93.3929)  time: 11.6547  data: 0.0182\n",
      "Epoch: [1]  [35/40]  eta: 0:00:58  lr: 1e-05  img/s: 11.073667724239638  loss: 0.9164 (1.2465)  acc1: 71.8750 (62.1962)  acc5: 96.8750 (93.4896)  time: 11.6451  data: 0.0182\n",
      "Epoch: [1]  [36/40]  eta: 0:00:46  lr: 1e-05  img/s: 11.058509813388607  loss: 0.9061 (1.2324)  acc1: 72.6562 (62.7323)  acc5: 96.8750 (93.6233)  time: 11.6356  data: 0.0182\n",
      "Epoch: [1]  [37/40]  eta: 0:00:34  lr: 1e-05  img/s: 10.995751997601571  loss: 0.8980 (1.2184)  acc1: 72.6562 (63.1579)  acc5: 96.8750 (93.7706)  time: 11.6364  data: 0.0181\n",
      "Epoch: [1]  [38/40]  eta: 0:00:23  lr: 1e-05  img/s: 11.01961266867217  loss: 0.8943 (1.2032)  acc1: 72.6562 (63.6018)  acc5: 96.8750 (93.9103)  time: 11.6356  data: 0.0182\n",
      "Epoch: [1]  [39/40]  eta: 0:00:11  lr: 1e-05  img/s: 11.368995980887702  loss: 0.8943 (1.2084)  acc1: 72.6562 (63.5800)  acc5: 96.8750 (93.9200)  time: 11.0897  data: 0.0174\n",
      "Epoch: [1] Total time: 0:07:35\n",
      "Epoch 3/15\n",
      "Epoch: [2]  [ 0/40]  eta: 0:07:32  lr: 1.0000000000000002e-06  img/s: 11.321188295310966  loss: 0.9624 (0.9624)  acc1: 67.9688 (67.9688)  acc5: 97.6562 (97.6562)  time: 11.3245  data: 0.0183\n",
      "Epoch: [2]  [ 1/40]  eta: 0:07:30  lr: 1.0000000000000002e-06  img/s: 10.901380231341893  loss: 0.8161 (0.8892)  acc1: 67.9688 (71.8750)  acc5: 97.6562 (97.6562)  time: 11.5420  data: 0.0181\n",
      "Epoch: [2]  [ 2/40]  eta: 0:07:20  lr: 1.0000000000000002e-06  img/s: 10.961639899541918  loss: 0.9326 (0.9037)  acc1: 75.0000 (72.9167)  acc5: 97.6562 (97.6562)  time: 11.5929  data: 0.0180\n",
      "Epoch: [2]  [ 3/40]  eta: 0:07:09  lr: 1.0000000000000002e-06  img/s: 10.955860793971741  loss: 0.9326 (0.9309)  acc1: 67.9688 (71.6797)  acc5: 97.6562 (97.2656)  time: 11.6200  data: 0.0180\n",
      "Epoch: [2]  [ 4/40]  eta: 0:06:57  lr: 1.0000000000000002e-06  img/s: 11.149208688297941  loss: 0.9326 (0.9096)  acc1: 75.0000 (73.1250)  acc5: 97.6562 (97.0312)  time: 11.5957  data: 0.0180\n",
      "Epoch: [2]  [ 5/40]  eta: 0:06:45  lr: 1.0000000000000002e-06  img/s: 11.103997392973557  loss: 0.8580 (0.9010)  acc1: 75.0000 (73.6979)  acc5: 97.6562 (97.2656)  time: 11.5873  data: 0.0180\n",
      "Epoch: [2]  [ 6/40]  eta: 0:06:32  lr: 1.0000000000000002e-06  img/s: 11.362578443129816  loss: 0.8580 (0.8803)  acc1: 75.7812 (74.3304)  acc5: 97.6562 (97.3214)  time: 11.5438  data: 0.0179\n",
      "Epoch: [2]  [ 7/40]  eta: 0:06:19  lr: 1.0000000000000002e-06  img/s: 11.345170156025567  loss: 0.8580 (0.8845)  acc1: 75.0000 (74.2188)  acc5: 97.6562 (97.4609)  time: 11.5134  data: 0.0179\n",
      "Epoch: [2]  [ 8/40]  eta: 0:06:07  lr: 1.0000000000000002e-06  img/s: 11.351966481106547  loss: 0.9141 (0.8893)  acc1: 75.0000 (73.7847)  acc5: 97.6562 (97.3958)  time: 11.4889  data: 0.0179\n",
      "Epoch: [2]  [ 9/40]  eta: 0:05:55  lr: 1.0000000000000002e-06  img/s: 11.374651707896168  loss: 0.9141 (0.9067)  acc1: 73.4375 (73.2031)  acc5: 97.6562 (97.4219)  time: 11.4671  data: 0.0179\n",
      "Epoch: [2]  [10/40]  eta: 0:05:43  lr: 1.0000000000000002e-06  img/s: 11.35943572220039  loss: 0.9141 (0.8874)  acc1: 75.0000 (73.7216)  acc5: 97.6562 (97.5852)  time: 11.4506  data: 0.0178\n",
      "Epoch: [2]  [11/40]  eta: 0:05:32  lr: 1.0000000000000002e-06  img/s: 11.136990979627246  loss: 0.8580 (0.8807)  acc1: 75.0000 (73.8932)  acc5: 97.6562 (97.5911)  time: 11.4557  data: 0.0178\n",
      "Epoch: [2]  [12/40]  eta: 0:05:20  lr: 1.0000000000000002e-06  img/s: 11.18542120424073  loss: 0.8580 (0.8742)  acc1: 75.0000 (73.8582)  acc5: 97.6562 (97.6562)  time: 11.4561  data: 0.0178\n",
      "Epoch: [2]  [13/40]  eta: 0:05:09  lr: 1.0000000000000002e-06  img/s: 11.034022927594842  loss: 0.8242 (0.8673)  acc1: 75.0000 (74.3304)  acc5: 97.6562 (97.7121)  time: 11.4677  data: 0.0178\n",
      "Epoch: [2]  [14/40]  eta: 0:04:58  lr: 1.0000000000000002e-06  img/s: 10.908459835857492  loss: 0.8580 (0.8729)  acc1: 75.0000 (74.3229)  acc5: 97.6562 (97.7604)  time: 11.4866  data: 0.0178\n",
      "Epoch: [2]  [15/40]  eta: 0:04:47  lr: 1.0000000000000002e-06  img/s: 11.020001493497395  loss: 0.8242 (0.8689)  acc1: 75.0000 (74.4141)  acc5: 97.6562 (97.8027)  time: 11.4958  data: 0.0178\n",
      "Epoch: [2]  [16/40]  eta: 0:04:36  lr: 1.0000000000000002e-06  img/s: 10.983291297310728  loss: 0.8242 (0.8604)  acc1: 75.7812 (74.7702)  acc5: 97.6562 (97.8401)  time: 11.5062  data: 0.0179\n",
      "Epoch: [2]  [17/40]  eta: 0:04:24  lr: 1.0000000000000002e-06  img/s: 11.104235098059908  loss: 0.8161 (0.8544)  acc1: 75.7812 (75.0000)  acc5: 97.6562 (97.9167)  time: 11.5083  data: 0.0178\n",
      "Epoch: [2]  [18/40]  eta: 0:04:13  lr: 1.0000000000000002e-06  img/s: 10.845492269001529  loss: 0.8161 (0.8493)  acc1: 75.7812 (75.2467)  acc5: 97.6562 (97.8618)  time: 11.5247  data: 0.0179\n",
      "Epoch: [2]  [19/40]  eta: 0:04:02  lr: 1.0000000000000002e-06  img/s: 10.968907377565017  loss: 0.8096 (0.8459)  acc1: 75.7812 (75.4297)  acc5: 97.6562 (97.8516)  time: 11.5329  data: 0.0179\n",
      "Epoch: [2]  [20/40]  eta: 0:03:50  lr: 1.0000000000000002e-06  img/s: 11.022360137734204  loss: 0.8096 (0.8483)  acc1: 75.7812 (75.1860)  acc5: 97.6562 (97.8423)  time: 11.5482  data: 0.0178\n",
      "Epoch: [2]  [21/40]  eta: 0:03:39  lr: 1.0000000000000002e-06  img/s: 11.023403919587826  loss: 0.8082 (0.8465)  acc1: 75.7812 (75.2486)  acc5: 97.6562 (97.9048)  time: 11.5417  data: 0.0178\n",
      "Epoch: [2]  [22/40]  eta: 0:03:27  lr: 1.0000000000000002e-06  img/s: 11.086424424082324  loss: 0.8068 (0.8436)  acc1: 76.5625 (75.3057)  acc5: 97.6562 (97.7921)  time: 11.5351  data: 0.0179\n",
      "Epoch: [2]  [23/40]  eta: 0:03:16  lr: 1.0000000000000002e-06  img/s: 10.89114147415986  loss: 0.7964 (0.8415)  acc1: 76.5625 (75.3581)  acc5: 97.6562 (97.7214)  time: 11.5386  data: 0.0178\n",
      "Epoch: [2]  [24/40]  eta: 0:03:04  lr: 1.0000000000000002e-06  img/s: 10.999634793977807  loss: 0.7936 (0.8390)  acc1: 76.5625 (75.5312)  acc5: 97.6562 (97.6562)  time: 11.5464  data: 0.0178\n",
      "Epoch: [2]  [25/40]  eta: 0:02:53  lr: 1.0000000000000002e-06  img/s: 11.001035391012714  loss: 0.7800 (0.8353)  acc1: 76.5625 (75.6911)  acc5: 97.6562 (97.5661)  time: 11.5518  data: 0.0178\n",
      "Epoch: [2]  [26/40]  eta: 0:02:41  lr: 1.0000000000000002e-06  img/s: 10.950777270927016  loss: 0.7800 (0.8292)  acc1: 76.5625 (75.7812)  acc5: 97.6562 (97.6273)  time: 11.5729  data: 0.0178\n",
      "Epoch: [2]  [27/40]  eta: 0:02:30  lr: 1.0000000000000002e-06  img/s: 11.065236265693253  loss: 0.7797 (0.8259)  acc1: 76.5625 (75.8092)  acc5: 97.6562 (97.6842)  time: 11.5872  data: 0.0178\n",
      "Epoch: [2]  [28/40]  eta: 0:02:18  lr: 1.0000000000000002e-06  img/s: 11.087671575989424  loss: 0.7785 (0.8215)  acc1: 76.5625 (75.8890)  acc5: 98.4375 (97.7101)  time: 11.6006  data: 0.0178\n",
      "Epoch: [2]  [29/40]  eta: 0:02:07  lr: 1.0000000000000002e-06  img/s: 11.080928577223943  loss: 0.7785 (0.8257)  acc1: 76.5625 (75.7812)  acc5: 98.4375 (97.7083)  time: 11.6156  data: 0.0178\n",
      "Epoch: [2]  [30/40]  eta: 0:01:55  lr: 1.0000000000000002e-06  img/s: 11.008836705437888  loss: 0.7785 (0.8210)  acc1: 76.5625 (75.8821)  acc5: 97.6562 (97.6815)  time: 11.6335  data: 0.0178\n",
      "Epoch: [2]  [31/40]  eta: 0:01:44  lr: 1.0000000000000002e-06  img/s: 11.089556459393851  loss: 0.7785 (0.8238)  acc1: 76.5625 (75.8789)  acc5: 97.6562 (97.6562)  time: 11.6360  data: 0.0178\n",
      "Epoch: [2]  [32/40]  eta: 0:01:32  lr: 1.0000000000000002e-06  img/s: 10.8786392960213  loss: 0.7783 (0.8206)  acc1: 78.1250 (75.9706)  acc5: 97.6562 (97.6326)  time: 11.6521  data: 0.0178\n",
      "Epoch: [2]  [33/40]  eta: 0:01:21  lr: 1.0000000000000002e-06  img/s: 10.951820720017292  loss: 0.7581 (0.8180)  acc1: 77.3438 (76.0110)  acc5: 97.6562 (97.6103)  time: 11.6564  data: 0.0178\n",
      "Epoch: [2]  [34/40]  eta: 0:01:09  lr: 1.0000000000000002e-06  img/s: 11.030172021041425  loss: 0.7527 (0.8150)  acc1: 78.1250 (76.0714)  acc5: 97.6562 (97.6562)  time: 11.6500  data: 0.0178\n",
      "Epoch: [2]  [35/40]  eta: 0:00:57  lr: 1.0000000000000002e-06  img/s: 11.08513658262092  loss: 0.7527 (0.8170)  acc1: 78.1250 (75.9983)  acc5: 97.6562 (97.6780)  time: 11.6465  data: 0.0178\n",
      "Epoch: [2]  [36/40]  eta: 0:00:46  lr: 1.0000000000000002e-06  img/s: 11.026536904536544  loss: 0.7527 (0.8127)  acc1: 78.1250 (76.1613)  acc5: 97.6562 (97.6985)  time: 11.6442  data: 0.0178\n",
      "Epoch: [2]  [37/40]  eta: 0:00:34  lr: 1.0000000000000002e-06  img/s: 11.083472400359108  loss: 0.7443 (0.8088)  acc1: 77.3438 (76.1719)  acc5: 97.6562 (97.6974)  time: 11.6453  data: 0.0178\n",
      "Epoch: [2]  [38/40]  eta: 0:00:23  lr: 1.0000000000000002e-06  img/s: 11.070337153435494  loss: 0.7363 (0.8061)  acc1: 77.3438 (76.2220)  acc5: 97.6562 (97.7364)  time: 11.6333  data: 0.0178\n",
      "Epoch: [2]  [39/40]  eta: 0:00:11  lr: 1.0000000000000002e-06  img/s: 10.828561811625983  loss: 0.7363 (0.8050)  acc1: 76.5625 (76.2200)  acc5: 97.6562 (97.7400)  time: 11.0860  data: 0.0170\n",
      "Epoch: [2] Total time: 0:07:32\n",
      "Epoch 4/15\n",
      "Epoch: [3]  [ 0/40]  eta: 0:07:31  lr: 1.0000000000000002e-07  img/s: 11.347377930980024  loss: 0.9672 (0.9672)  acc1: 71.8750 (71.8750)  acc5: 96.0938 (96.0938)  time: 11.2984  data: 0.0182\n",
      "Epoch: [3]  [ 1/40]  eta: 0:07:28  lr: 1.0000000000000002e-07  img/s: 10.932699651757646  loss: 0.7620 (0.8646)  acc1: 71.8750 (75.0000)  acc5: 96.0938 (97.2656)  time: 11.5120  data: 0.0180\n",
      "Epoch: [3]  [ 2/40]  eta: 0:07:18  lr: 1.0000000000000002e-07  img/s: 11.079901999571474  loss: 0.9244 (0.8845)  acc1: 71.8750 (73.6979)  acc5: 97.6562 (97.3958)  time: 11.5315  data: 0.0179\n",
      "Epoch: [3]  [ 3/40]  eta: 0:07:06  lr: 1.0000000000000002e-07  img/s: 11.099533643405238  loss: 0.9229 (0.8941)  acc1: 71.8750 (74.0234)  acc5: 96.8750 (97.2656)  time: 11.5361  data: 0.0179\n",
      "Epoch: [3]  [ 4/40]  eta: 0:06:55  lr: 1.0000000000000002e-07  img/s: 11.052240196781295  loss: 0.9229 (0.8582)  acc1: 75.0000 (75.1562)  acc5: 97.6562 (97.5000)  time: 11.5487  data: 0.0179\n",
      "Epoch: [3]  [ 5/40]  eta: 0:06:44  lr: 1.0000000000000002e-07  img/s: 11.041636072061099  loss: 0.7786 (0.8449)  acc1: 75.0000 (75.1302)  acc5: 97.6562 (97.6562)  time: 11.5590  data: 0.0179\n",
      "Epoch: [3]  [ 6/40]  eta: 0:06:33  lr: 1.0000000000000002e-07  img/s: 10.952976989280293  loss: 0.7786 (0.8200)  acc1: 75.0000 (76.0045)  acc5: 98.4375 (97.9911)  time: 11.5798  data: 0.0179\n",
      "Epoch: [3]  [ 7/40]  eta: 0:06:22  lr: 1.0000000000000002e-07  img/s: 11.023034092217307  loss: 0.7620 (0.8127)  acc1: 75.0000 (76.2695)  acc5: 98.4375 (98.0469)  time: 11.5860  data: 0.0179\n",
      "Epoch: [3]  [ 8/40]  eta: 0:06:11  lr: 1.0000000000000002e-07  img/s: 10.916956542241527  loss: 0.7786 (0.8171)  acc1: 75.7812 (76.2153)  acc5: 98.4375 (98.0903)  time: 11.6034  data: 0.0179\n",
      "Epoch: [3]  [ 9/40]  eta: 0:05:59  lr: 1.0000000000000002e-07  img/s: 10.999026116532319  loss: 0.7786 (0.8390)  acc1: 75.0000 (75.3906)  acc5: 98.4375 (98.0469)  time: 11.6086  data: 0.0179\n",
      "Epoch: [3]  [10/40]  eta: 0:05:48  lr: 1.0000000000000002e-07  img/s: 10.914147538564768  loss: 0.7786 (0.8261)  acc1: 75.7812 (75.7812)  acc5: 98.4375 (98.0824)  time: 11.6211  data: 0.0179\n",
      "Epoch: [3]  [11/40]  eta: 0:05:37  lr: 1.0000000000000002e-07  img/s: 11.018800047796558  loss: 0.7620 (0.8178)  acc1: 75.7812 (75.9766)  acc5: 98.4375 (97.9167)  time: 11.6222  data: 0.0179\n",
      "Epoch: [3]  [12/40]  eta: 0:05:25  lr: 1.0000000000000002e-07  img/s: 11.002686182187098  loss: 0.7786 (0.8151)  acc1: 78.1250 (76.1418)  acc5: 98.4375 (97.8966)  time: 11.6245  data: 0.0179\n",
      "Epoch: [3]  [13/40]  eta: 0:05:13  lr: 1.0000000000000002e-07  img/s: 11.008466274754422  loss: 0.7786 (0.8147)  acc1: 75.7812 (76.0603)  acc5: 98.4375 (97.9353)  time: 11.6259  data: 0.0179\n",
      "Epoch: [3]  [14/40]  eta: 0:05:02  lr: 1.0000000000000002e-07  img/s: 10.941260933599965  loss: 0.7833 (0.8138)  acc1: 75.7812 (76.0417)  acc5: 98.4375 (98.0208)  time: 11.6320  data: 0.0179\n",
      "Epoch: [3]  [15/40]  eta: 0:04:50  lr: 1.0000000000000002e-07  img/s: 11.025997482223184  loss: 0.7786 (0.8084)  acc1: 75.7812 (76.1230)  acc5: 98.4375 (98.0957)  time: 11.6317  data: 0.0179\n",
      "Epoch: [3]  [16/40]  eta: 0:04:39  lr: 1.0000000000000002e-07  img/s: 10.957583262551143  loss: 0.7786 (0.8007)  acc1: 77.3438 (76.4706)  acc5: 98.4375 (98.1158)  time: 11.6356  data: 0.0179\n",
      "Epoch: [3]  [17/40]  eta: 0:04:27  lr: 1.0000000000000002e-07  img/s: 11.104701580810648  loss: 0.7620 (0.7985)  acc1: 75.7812 (76.3889)  acc5: 98.4375 (98.1337)  time: 11.6306  data: 0.0179\n",
      "Epoch: [3]  [18/40]  eta: 0:04:15  lr: 1.0000000000000002e-07  img/s: 11.115597315805308  loss: 0.7786 (0.7975)  acc1: 77.3438 (76.5214)  acc5: 98.4375 (98.0263)  time: 11.6255  data: 0.0179\n",
      "Epoch: [3]  [19/40]  eta: 0:04:04  lr: 1.0000000000000002e-07  img/s: 10.97849388946947  loss: 0.7620 (0.7916)  acc1: 77.3438 (76.7188)  acc5: 98.4375 (97.9688)  time: 11.6280  data: 0.0179\n",
      "Epoch: [3]  [20/40]  eta: 0:03:52  lr: 1.0000000000000002e-07  img/s: 11.073050140905957  loss: 0.7620 (0.7956)  acc1: 77.3438 (76.4509)  acc5: 98.4375 (97.8795)  time: 11.6420  data: 0.0179\n",
      "Epoch: [3]  [21/40]  eta: 0:03:40  lr: 1.0000000000000002e-07  img/s: 10.994422992314504  loss: 0.7614 (0.7918)  acc1: 77.3438 (76.7401)  acc5: 98.4375 (97.8693)  time: 11.6387  data: 0.0179\n",
      "Epoch: [3]  [22/40]  eta: 0:03:29  lr: 1.0000000000000002e-07  img/s: 11.08883541008793  loss: 0.7608 (0.7864)  acc1: 77.3438 (76.7663)  acc5: 98.4375 (97.8261)  time: 11.6382  data: 0.0179\n",
      "Epoch: [3]  [23/40]  eta: 0:03:17  lr: 1.0000000000000002e-07  img/s: 10.97472134727574  loss: 0.7276 (0.7817)  acc1: 78.1250 (76.9531)  acc5: 98.4375 (97.8516)  time: 11.6448  data: 0.0179\n",
      "Epoch: [3]  [24/40]  eta: 0:03:05  lr: 1.0000000000000002e-07  img/s: 11.094558971933088  loss: 0.7608 (0.7872)  acc1: 77.3438 (76.7188)  acc5: 98.4375 (97.7500)  time: 11.6426  data: 0.0179\n",
      "Epoch: [3]  [25/40]  eta: 0:02:54  lr: 1.0000000000000002e-07  img/s: 10.995202522369354  loss: 0.7276 (0.7815)  acc1: 78.1250 (76.8930)  acc5: 97.6562 (97.7163)  time: 11.6450  data: 0.0179\n",
      "Epoch: [3]  [26/40]  eta: 0:02:42  lr: 1.0000000000000002e-07  img/s: 11.029588736760155  loss: 0.7276 (0.7780)  acc1: 78.1250 (77.0544)  acc5: 97.6562 (97.6273)  time: 11.6410  data: 0.0178\n",
      "Epoch: [3]  [27/40]  eta: 0:02:31  lr: 1.0000000000000002e-07  img/s: 11.1243708013145  loss: 0.7276 (0.7818)  acc1: 77.3438 (76.8973)  acc5: 97.6562 (97.6562)  time: 11.6357  data: 0.0178\n",
      "Epoch: [3]  [28/40]  eta: 0:02:19  lr: 1.0000000000000002e-07  img/s: 11.321083014625506  loss: 0.7258 (0.7794)  acc1: 78.1250 (76.9666)  acc5: 97.6562 (97.6832)  time: 11.6147  data: 0.0178\n",
      "Epoch: [3]  [29/40]  eta: 0:02:07  lr: 1.0000000000000002e-07  img/s: 11.114650132371704  loss: 0.7258 (0.7825)  acc1: 78.1250 (76.9792)  acc5: 97.6562 (97.7083)  time: 11.6087  data: 0.0178\n",
      "Epoch: [3]  [30/40]  eta: 0:01:56  lr: 1.0000000000000002e-07  img/s: 10.961182225409601  loss: 0.7258 (0.7769)  acc1: 78.1250 (77.0917)  acc5: 97.6562 (97.7067)  time: 11.6061  data: 0.0178\n",
      "Epoch: [3]  [31/40]  eta: 0:01:44  lr: 1.0000000000000002e-07  img/s: 11.06941387195386  loss: 0.7276 (0.7826)  acc1: 77.3438 (76.9043)  acc5: 97.6562 (97.6562)  time: 11.6035  data: 0.0178\n",
      "Epoch: [3]  [32/40]  eta: 0:01:32  lr: 1.0000000000000002e-07  img/s: 10.840387175238204  loss: 0.7276 (0.7821)  acc1: 77.3438 (76.9413)  acc5: 97.6562 (97.6089)  time: 11.6122  data: 0.0178\n",
      "Epoch: [3]  [33/40]  eta: 0:01:21  lr: 1.0000000000000002e-07  img/s: 11.058440795247344  loss: 0.7276 (0.7811)  acc1: 77.3438 (76.9072)  acc5: 97.6562 (97.6103)  time: 11.6096  data: 0.0178\n",
      "Epoch: [3]  [34/40]  eta: 0:01:09  lr: 1.0000000000000002e-07  img/s: 10.966383384740999  loss: 0.7276 (0.7816)  acc1: 77.3438 (76.8973)  acc5: 97.6562 (97.6562)  time: 11.6083  data: 0.0178\n",
      "Epoch: [3]  [35/40]  eta: 0:00:58  lr: 1.0000000000000002e-07  img/s: 10.997066006536572  loss: 0.7436 (0.7806)  acc1: 78.1250 (76.9314)  acc5: 96.8750 (97.6128)  time: 11.6098  data: 0.0178\n",
      "Epoch: [3]  [36/40]  eta: 0:00:46  lr: 1.0000000000000002e-07  img/s: 10.915077276997504  loss: 0.7489 (0.7804)  acc1: 77.3438 (76.8792)  acc5: 96.8750 (97.5929)  time: 11.6121  data: 0.0178\n",
      "Epoch: [3]  [37/40]  eta: 0:00:34  lr: 1.0000000000000002e-07  img/s: 11.133660311682233  loss: 0.7436 (0.7742)  acc1: 78.1250 (77.1382)  acc5: 96.8750 (97.5946)  time: 11.6105  data: 0.0178\n",
      "Epoch: [3]  [38/40]  eta: 0:00:23  lr: 1.0000000000000002e-07  img/s: 10.95990563967406  loss: 0.7120 (0.7710)  acc1: 78.1250 (77.1635)  acc5: 96.8750 (97.6362)  time: 11.6187  data: 0.0178\n",
      "Epoch: [3]  [39/40]  eta: 0:00:11  lr: 1.0000000000000002e-07  img/s: 10.22119708982621  loss: 0.7436 (0.7713)  acc1: 77.3438 (77.1400)  acc5: 97.6562 (97.6400)  time: 11.0741  data: 0.0170\n",
      "Epoch: [3] Total time: 0:07:34\n",
      "Epoch 5/15\n",
      "Epoch: [4]  [ 0/40]  eta: 0:07:30  lr: 1.0000000000000004e-08  img/s: 11.393964208025542  loss: 0.9204 (0.9204)  acc1: 70.3125 (70.3125)  acc5: 98.4375 (98.4375)  time: 11.2515  data: 0.0175\n",
      "Epoch: [4]  [ 1/40]  eta: 0:07:27  lr: 1.0000000000000004e-08  img/s: 10.970375255589978  loss: 0.8776 (0.8990)  acc1: 70.3125 (73.8281)  acc5: 97.6562 (98.0469)  time: 11.4686  data: 0.0177\n",
      "Epoch: [4]  [ 2/40]  eta: 0:07:19  lr: 1.0000000000000004e-08  img/s: 10.928637484354754  loss: 0.9132 (0.9037)  acc1: 70.3125 (72.6562)  acc5: 97.6562 (97.9167)  time: 11.5558  data: 0.0177\n",
      "Epoch: [4]  [ 3/40]  eta: 0:07:07  lr: 1.0000000000000004e-08  img/s: 11.090495933984073  loss: 0.8776 (0.8819)  acc1: 70.3125 (73.8281)  acc5: 97.6562 (97.8516)  time: 11.5565  data: 0.0176\n",
      "Epoch: [4]  [ 4/40]  eta: 0:06:56  lr: 1.0000000000000004e-08  img/s: 11.05455894408808  loss: 0.8776 (0.8559)  acc1: 77.3438 (75.3125)  acc5: 97.6562 (98.1250)  time: 11.5646  data: 0.0177\n",
      "Epoch: [4]  [ 5/40]  eta: 0:06:45  lr: 1.0000000000000004e-08  img/s: 11.006847372167064  loss: 0.8163 (0.8434)  acc1: 75.7812 (75.3906)  acc5: 97.6562 (98.3073)  time: 11.5783  data: 0.0177\n",
      "Epoch: [4]  [ 6/40]  eta: 0:06:33  lr: 1.0000000000000004e-08  img/s: 11.034787443865005  loss: 0.8163 (0.8190)  acc1: 77.3438 (76.1161)  acc5: 97.6562 (98.2143)  time: 11.5839  data: 0.0177\n",
      "Epoch: [4]  [ 7/40]  eta: 0:06:22  lr: 1.0000000000000004e-08  img/s: 11.06294905612003  loss: 0.7807 (0.8107)  acc1: 75.7812 (76.0742)  acc5: 97.6562 (98.3398)  time: 11.5844  data: 0.0177\n",
      "Epoch: [4]  [ 8/40]  eta: 0:06:11  lr: 1.0000000000000004e-08  img/s: 10.932800949643303  loss: 0.8163 (0.8232)  acc1: 75.7812 (75.6944)  acc5: 97.6562 (98.1771)  time: 11.6001  data: 0.0177\n",
      "Epoch: [4]  [ 9/40]  eta: 0:05:59  lr: 1.0000000000000004e-08  img/s: 10.98656541807446  loss: 0.8163 (0.8337)  acc1: 75.7812 (75.3906)  acc5: 97.6562 (98.2031)  time: 11.6069  data: 0.0177\n",
      "Epoch: [4]  [10/40]  eta: 0:05:48  lr: 1.0000000000000004e-08  img/s: 11.079933326928678  loss: 0.8163 (0.8271)  acc1: 75.7812 (75.6392)  acc5: 98.4375 (98.2244)  time: 11.6036  data: 0.0177\n",
      "Epoch: [4]  [11/40]  eta: 0:05:36  lr: 1.0000000000000004e-08  img/s: 11.065532981341416  loss: 0.7807 (0.8214)  acc1: 75.7812 (75.7812)  acc5: 97.6562 (98.1120)  time: 11.6021  data: 0.0177\n",
      "Epoch: [4]  [12/40]  eta: 0:05:25  lr: 1.0000000000000004e-08  img/s: 10.88035145864901  loss: 0.7807 (0.8171)  acc1: 77.3438 (75.9014)  acc5: 98.4375 (98.1370)  time: 11.6159  data: 0.0178\n",
      "Epoch: [4]  [13/40]  eta: 0:05:13  lr: 1.0000000000000004e-08  img/s: 11.09145939863803  loss: 0.7736 (0.8140)  acc1: 77.3438 (76.0045)  acc5: 97.6562 (98.0469)  time: 11.6118  data: 0.0178\n",
      "Epoch: [4]  [14/40]  eta: 0:05:02  lr: 1.0000000000000004e-08  img/s: 10.784281984420216  loss: 0.7807 (0.8157)  acc1: 77.3438 (75.7292)  acc5: 97.6562 (97.9688)  time: 11.6301  data: 0.0178\n",
      "Epoch: [4]  [15/40]  eta: 0:04:50  lr: 1.0000000000000004e-08  img/s: 10.988000244864141  loss: 0.7736 (0.8063)  acc1: 77.3438 (76.0254)  acc5: 97.6562 (98.0469)  time: 11.6324  data: 0.0177\n",
      "Epoch: [4]  [16/40]  eta: 0:04:39  lr: 1.0000000000000004e-08  img/s: 10.849011388118413  loss: 0.7736 (0.8003)  acc1: 77.3438 (76.3787)  acc5: 97.6562 (98.0239)  time: 11.6432  data: 0.0177\n",
      "Epoch: [4]  [17/40]  eta: 0:04:27  lr: 1.0000000000000004e-08  img/s: 11.187073019120763  loss: 0.7658 (0.7892)  acc1: 77.3438 (76.6927)  acc5: 97.6562 (98.1337)  time: 11.6330  data: 0.0177\n",
      "Epoch: [4]  [18/40]  eta: 0:04:15  lr: 1.0000000000000004e-08  img/s: 10.973035426429762  loss: 0.7666 (0.7880)  acc1: 77.3438 (76.8092)  acc5: 98.4375 (98.1908)  time: 11.6356  data: 0.0177\n",
      "Epoch: [4]  [19/40]  eta: 0:04:04  lr: 1.0000000000000004e-08  img/s: 11.032175689426586  loss: 0.7658 (0.7809)  acc1: 77.3438 (76.9922)  acc5: 98.4375 (98.2422)  time: 11.6349  data: 0.0177\n",
      "Epoch: [4]  [20/40]  eta: 0:03:52  lr: 1.0000000000000004e-08  img/s: 11.031710746310907  loss: 0.7610 (0.7792)  acc1: 77.3438 (77.0461)  acc5: 97.6562 (98.2143)  time: 11.6533  data: 0.0178\n",
      "Epoch: [4]  [21/40]  eta: 0:03:40  lr: 1.0000000000000004e-08  img/s: 11.122593888821775  loss: 0.7588 (0.7754)  acc1: 77.3438 (77.2372)  acc5: 98.4375 (98.2244)  time: 11.6453  data: 0.0178\n",
      "Epoch: [4]  [22/40]  eta: 0:03:29  lr: 1.0000000000000004e-08  img/s: 11.007137353861776  loss: 0.7528 (0.7731)  acc1: 77.3438 (77.2418)  acc5: 98.4375 (98.2337)  time: 11.6412  data: 0.0178\n",
      "Epoch: [4]  [23/40]  eta: 0:03:17  lr: 1.0000000000000004e-08  img/s: 10.883669728713533  loss: 0.7522 (0.7698)  acc1: 78.1250 (77.4089)  acc5: 98.4375 (98.2096)  time: 11.6522  data: 0.0178\n",
      "Epoch: [4]  [24/40]  eta: 0:03:06  lr: 1.0000000000000004e-08  img/s: 11.06914136787197  loss: 0.7528 (0.7712)  acc1: 78.1250 (77.4375)  acc5: 98.4375 (98.1875)  time: 11.6514  data: 0.0178\n",
      "Epoch: [4]  [25/40]  eta: 0:02:54  lr: 1.0000000000000004e-08  img/s: 10.933818930628828  loss: 0.7528 (0.7716)  acc1: 78.1250 (77.4339)  acc5: 97.6562 (98.1671)  time: 11.6553  data: 0.0178\n",
      "Epoch: [4]  [26/40]  eta: 0:02:42  lr: 1.0000000000000004e-08  img/s: 11.145287387847972  loss: 0.7528 (0.7660)  acc1: 78.1250 (77.5463)  acc5: 98.4375 (98.1771)  time: 11.6496  data: 0.0178\n",
      "Epoch: [4]  [27/40]  eta: 0:02:31  lr: 1.0000000000000004e-08  img/s: 11.053552266606443  loss: 0.7557 (0.7657)  acc1: 78.1250 (77.5391)  acc5: 97.6562 (98.1306)  time: 11.6501  data: 0.0178\n",
      "Epoch: [4]  [28/40]  eta: 0:02:19  lr: 1.0000000000000004e-08  img/s: 11.15229105064179  loss: 0.7557 (0.7667)  acc1: 78.1250 (77.5323)  acc5: 97.6562 (98.1142)  time: 11.6386  data: 0.0178\n",
      "Epoch: [4]  [29/40]  eta: 0:02:07  lr: 1.0000000000000004e-08  img/s: 11.074123647257778  loss: 0.7557 (0.7755)  acc1: 78.1250 (77.2135)  acc5: 97.6562 (98.0729)  time: 11.6339  data: 0.0178\n",
      "Epoch: [4]  [30/40]  eta: 0:01:56  lr: 1.0000000000000004e-08  img/s: 10.928420807505255  loss: 0.7441 (0.7719)  acc1: 78.1250 (77.2429)  acc5: 97.6562 (98.1099)  time: 11.6420  data: 0.0178\n",
      "Epoch: [4]  [31/40]  eta: 0:01:44  lr: 1.0000000000000004e-08  img/s: 11.034130647444659  loss: 0.7441 (0.7733)  acc1: 78.1250 (77.1484)  acc5: 97.6562 (98.0713)  time: 11.6436  data: 0.0179\n",
      "Epoch: [4]  [32/40]  eta: 0:01:33  lr: 1.0000000000000004e-08  img/s: 11.016613374408985  loss: 0.7229 (0.7707)  acc1: 78.1250 (77.1780)  acc5: 97.6562 (98.1061)  time: 11.6364  data: 0.0179\n",
      "Epoch: [4]  [33/40]  eta: 0:01:21  lr: 1.0000000000000004e-08  img/s: 10.863116446716603  loss: 0.7229 (0.7705)  acc1: 78.1250 (77.2518)  acc5: 97.6562 (98.0469)  time: 11.6485  data: 0.0179\n",
      "Epoch: [4]  [34/40]  eta: 0:01:09  lr: 1.0000000000000004e-08  img/s: 11.16059032556181  loss: 0.7229 (0.7700)  acc1: 78.1250 (77.2545)  acc5: 97.6562 (98.0580)  time: 11.6285  data: 0.0179\n",
      "Epoch: [4]  [35/40]  eta: 0:00:58  lr: 1.0000000000000004e-08  img/s: 11.07751021026662  loss: 0.7441 (0.7718)  acc1: 78.1250 (77.1484)  acc5: 97.6562 (98.0686)  time: 11.6238  data: 0.0179\n",
      "Epoch: [4]  [36/40]  eta: 0:00:46  lr: 1.0000000000000004e-08  img/s: 10.985440484578913  loss: 0.7491 (0.7712)  acc1: 78.1250 (77.2171)  acc5: 98.4375 (98.0785)  time: 11.6165  data: 0.0179\n",
      "Epoch: [4]  [37/40]  eta: 0:00:34  lr: 1.0000000000000004e-08  img/s: 10.948660602804448  loss: 0.7491 (0.7698)  acc1: 78.1250 (77.2615)  acc5: 97.6562 (98.0469)  time: 11.6289  data: 0.0179\n",
      "Epoch: [4]  [38/40]  eta: 0:00:23  lr: 1.0000000000000004e-08  img/s: 11.00406364846021  loss: 0.7441 (0.7642)  acc1: 78.1250 (77.4639)  acc5: 97.6562 (98.0569)  time: 11.6273  data: 0.0179\n",
      "Epoch: [4]  [39/40]  eta: 0:00:11  lr: 1.0000000000000004e-08  img/s: 11.446633326283184  loss: 0.7491 (0.7655)  acc1: 78.1250 (77.4600)  acc5: 97.6562 (98.0600)  time: 11.0813  data: 0.0171\n",
      "Epoch: [4] Total time: 0:07:34\n",
      "Epoch 6/15\n",
      "Epoch: [5]  [ 0/40]  eta: 0:07:30  lr: 1.0000000000000005e-09  img/s: 11.393460050017644  loss: 0.9858 (0.9858)  acc1: 74.2188 (74.2188)  acc5: 97.6562 (97.6562)  time: 11.2527  data: 0.0182\n",
      "Epoch: [5]  [ 1/40]  eta: 0:07:27  lr: 1.0000000000000005e-09  img/s: 10.955703846607264  loss: 0.8531 (0.9194)  acc1: 74.2188 (75.0000)  acc5: 97.6562 (98.0469)  time: 11.4771  data: 0.0181\n",
      "Epoch: [5]  [ 2/40]  eta: 0:07:18  lr: 1.0000000000000005e-09  img/s: 10.962418816736243  loss: 0.8878 (0.9089)  acc1: 74.2188 (74.7396)  acc5: 98.4375 (98.1771)  time: 11.5494  data: 0.0180\n",
      "Epoch: [5]  [ 3/40]  eta: 0:07:08  lr: 1.0000000000000005e-09  img/s: 10.95513556485354  loss: 0.8878 (0.9164)  acc1: 74.2188 (74.4141)  acc5: 97.6562 (97.4609)  time: 11.5875  data: 0.0180\n",
      "Epoch: [5]  [ 4/40]  eta: 0:06:56  lr: 1.0000000000000005e-09  img/s: 11.137582444410048  loss: 0.8878 (0.8779)  acc1: 74.2188 (75.9375)  acc5: 98.4375 (97.8125)  time: 11.5721  data: 0.0180\n",
      "Epoch: [5]  [ 5/40]  eta: 0:06:44  lr: 1.0000000000000005e-09  img/s: 11.149694934754967  loss: 0.8531 (0.8674)  acc1: 74.2188 (76.1719)  acc5: 98.4375 (98.0469)  time: 11.5597  data: 0.0179\n",
      "Epoch: [5]  [ 6/40]  eta: 0:06:32  lr: 1.0000000000000005e-09  img/s: 11.154549767466252  loss: 0.8531 (0.8422)  acc1: 75.7812 (76.6741)  acc5: 98.4375 (98.1027)  time: 11.5502  data: 0.0179\n",
      "Epoch: [5]  [ 7/40]  eta: 0:06:21  lr: 1.0000000000000005e-09  img/s: 11.127351342532029  loss: 0.8147 (0.8298)  acc1: 75.7812 (76.7578)  acc5: 98.4375 (98.3398)  time: 11.5465  data: 0.0179\n",
      "Epoch: [5]  [ 8/40]  eta: 0:06:09  lr: 1.0000000000000005e-09  img/s: 11.072711915324907  loss: 0.8299 (0.8298)  acc1: 76.5625 (76.7361)  acc5: 98.4375 (98.0903)  time: 11.5500  data: 0.0178\n",
      "Epoch: [5]  [ 9/40]  eta: 0:05:57  lr: 1.0000000000000005e-09  img/s: 11.153109115787242  loss: 0.8299 (0.8397)  acc1: 75.7812 (76.3281)  acc5: 98.4375 (98.0469)  time: 11.5444  data: 0.0178\n",
      "Epoch: [5]  [10/40]  eta: 0:05:46  lr: 1.0000000000000005e-09  img/s: 11.135709611088176  loss: 0.8299 (0.8264)  acc1: 76.5625 (76.4915)  acc5: 98.4375 (98.0114)  time: 11.5415  data: 0.0178\n",
      "Epoch: [5]  [11/40]  eta: 0:05:34  lr: 1.0000000000000005e-09  img/s: 11.105871520135484  loss: 0.8147 (0.8168)  acc1: 76.5625 (76.8229)  acc5: 97.6562 (97.9818)  time: 11.5417  data: 0.0178\n",
      "Epoch: [5]  [12/40]  eta: 0:05:23  lr: 1.0000000000000005e-09  img/s: 10.999966316028267  loss: 0.8147 (0.8120)  acc1: 76.5625 (76.6827)  acc5: 98.4375 (98.0769)  time: 11.5503  data: 0.0178\n",
      "Epoch: [5]  [13/40]  eta: 0:05:11  lr: 1.0000000000000005e-09  img/s: 11.086158178499007  loss: 0.8147 (0.8148)  acc1: 75.7812 (76.6183)  acc5: 97.6562 (97.9911)  time: 11.5513  data: 0.0178\n",
      "Epoch: [5]  [14/40]  eta: 0:05:00  lr: 1.0000000000000005e-09  img/s: 11.059099122170178  loss: 0.8147 (0.8113)  acc1: 76.5625 (76.8750)  acc5: 97.6562 (97.9688)  time: 11.5540  data: 0.0178\n",
      "Epoch: [5]  [15/40]  eta: 0:04:49  lr: 1.0000000000000005e-09  img/s: 11.001110908066424  loss: 0.7624 (0.8063)  acc1: 76.5625 (76.9531)  acc5: 97.6562 (97.9980)  time: 11.5602  data: 0.0178\n",
      "Epoch: [5]  [16/40]  eta: 0:04:37  lr: 1.0000000000000005e-09  img/s: 11.141314048179735  loss: 0.7624 (0.7974)  acc1: 77.3438 (77.1140)  acc5: 97.6562 (97.9779)  time: 11.5570  data: 0.0178\n",
      "Epoch: [5]  [17/40]  eta: 0:04:25  lr: 1.0000000000000005e-09  img/s: 10.967356099072072  loss: 0.7547 (0.7904)  acc1: 77.3438 (77.3872)  acc5: 97.6562 (98.0469)  time: 11.5644  data: 0.0178\n",
      "Epoch: [5]  [18/40]  eta: 0:04:14  lr: 1.0000000000000005e-09  img/s: 11.05767482005486  loss: 0.7624 (0.7959)  acc1: 77.3438 (77.1382)  acc5: 97.6562 (97.9852)  time: 11.5659  data: 0.0178\n",
      "Epoch: [5]  [19/40]  eta: 0:04:02  lr: 1.0000000000000005e-09  img/s: 11.070843026344775  loss: 0.7547 (0.7913)  acc1: 77.3438 (77.2656)  acc5: 97.6562 (97.9688)  time: 11.5666  data: 0.0178\n",
      "Epoch: [5]  [20/40]  eta: 0:03:51  lr: 1.0000000000000005e-09  img/s: 11.027498121947568  loss: 0.7429 (0.7877)  acc1: 77.3438 (77.2693)  acc5: 97.6562 (98.0283)  time: 11.5852  data: 0.0178\n",
      "Epoch: [5]  [21/40]  eta: 0:03:39  lr: 1.0000000000000005e-09  img/s: 11.114553720175815  loss: 0.7302 (0.7825)  acc1: 77.3438 (77.5213)  acc5: 97.6562 (98.0469)  time: 11.5768  data: 0.0178\n",
      "Epoch: [5]  [22/40]  eta: 0:03:28  lr: 1.0000000000000005e-09  img/s: 10.949747420163776  loss: 0.7240 (0.7796)  acc1: 78.1250 (77.6495)  acc5: 97.6562 (97.9959)  time: 11.5775  data: 0.0178\n",
      "Epoch: [5]  [23/40]  eta: 0:03:16  lr: 1.0000000000000005e-09  img/s: 11.061428497403082  loss: 0.7162 (0.7764)  acc1: 78.1250 (77.8320)  acc5: 97.6562 (97.9492)  time: 11.5719  data: 0.0178\n",
      "Epoch: [5]  [24/40]  eta: 0:03:05  lr: 1.0000000000000005e-09  img/s: 10.916943222861534  loss: 0.7162 (0.7789)  acc1: 78.1250 (77.7812)  acc5: 97.6562 (97.9375)  time: 11.5835  data: 0.0178\n",
      "Epoch: [5]  [25/40]  eta: 0:02:53  lr: 1.0000000000000005e-09  img/s: 11.104368998256154  loss: 0.7161 (0.7746)  acc1: 78.1250 (77.9748)  acc5: 97.6562 (97.8365)  time: 11.5858  data: 0.0178\n",
      "Epoch: [5]  [26/40]  eta: 0:02:42  lr: 1.0000000000000005e-09  img/s: 11.150418131142668  loss: 0.7161 (0.7693)  acc1: 78.1250 (78.0382)  acc5: 97.6562 (97.8588)  time: 11.5860  data: 0.0178\n",
      "Epoch: [5]  [27/40]  eta: 0:02:30  lr: 1.0000000000000005e-09  img/s: 11.00750950061663  loss: 0.7161 (0.7694)  acc1: 78.1250 (77.9576)  acc5: 97.6562 (97.8516)  time: 11.5923  data: 0.0178\n",
      "Epoch: [5]  [28/40]  eta: 0:02:18  lr: 1.0000000000000005e-09  img/s: 10.989387757842424  loss: 0.7107 (0.7639)  acc1: 79.6875 (78.1519)  acc5: 97.6562 (97.8718)  time: 11.5967  data: 0.0178\n",
      "Epoch: [5]  [29/40]  eta: 0:02:07  lr: 1.0000000000000005e-09  img/s: 11.045456811374379  loss: 0.7107 (0.7681)  acc1: 79.6875 (77.9948)  acc5: 97.6562 (97.7604)  time: 11.6023  data: 0.0178\n",
      "Epoch: [5]  [30/40]  eta: 0:01:55  lr: 1.0000000000000005e-09  img/s: 10.89752400888713  loss: 0.7107 (0.7637)  acc1: 79.6875 (78.2006)  acc5: 97.6562 (97.7823)  time: 11.6148  data: 0.0178\n",
      "Epoch: [5]  [31/40]  eta: 0:01:44  lr: 1.0000000000000005e-09  img/s: 11.008679365289105  loss: 0.7161 (0.7686)  acc1: 79.6875 (78.0518)  acc5: 97.6562 (97.7051)  time: 11.6199  data: 0.0178\n",
      "Epoch: [5]  [32/40]  eta: 0:01:32  lr: 1.0000000000000005e-09  img/s: 10.824050039861033  loss: 0.7040 (0.7650)  acc1: 79.6875 (78.2434)  acc5: 97.6562 (97.6799)  time: 11.6294  data: 0.0177\n",
      "Epoch: [5]  [33/40]  eta: 0:01:21  lr: 1.0000000000000005e-09  img/s: 11.026067681132176  loss: 0.7040 (0.7655)  acc1: 79.6875 (78.1710)  acc5: 97.6562 (97.6562)  time: 11.6325  data: 0.0177\n",
      "Epoch: [5]  [34/40]  eta: 0:01:09  lr: 1.0000000000000005e-09  img/s: 10.820975867876083  loss: 0.7040 (0.7642)  acc1: 79.6875 (78.1920)  acc5: 97.6562 (97.6786)  time: 11.6453  data: 0.0177\n",
      "Epoch: [5]  [35/40]  eta: 0:00:58  lr: 1.0000000000000005e-09  img/s: 10.849760347557124  loss: 0.7040 (0.7645)  acc1: 79.6875 (78.1467)  acc5: 97.6562 (97.6997)  time: 11.6534  data: 0.0177\n",
      "Epoch: [5]  [36/40]  eta: 0:00:46  lr: 1.0000000000000005e-09  img/s: 11.020555938653581  loss: 0.7073 (0.7629)  acc1: 79.6875 (78.1883)  acc5: 97.6562 (97.6774)  time: 11.6597  data: 0.0177\n",
      "Epoch: [5]  [37/40]  eta: 0:00:34  lr: 1.0000000000000005e-09  img/s: 10.994588931537354  loss: 0.7073 (0.7607)  acc1: 79.6875 (78.3717)  acc5: 96.8750 (97.6562)  time: 11.6582  data: 0.0177\n",
      "Epoch: [5]  [38/40]  eta: 0:00:23  lr: 1.0000000000000005e-09  img/s: 11.024349649796225  loss: 0.7040 (0.7560)  acc1: 79.6875 (78.5857)  acc5: 97.6562 (97.6963)  time: 11.6600  data: 0.0178\n",
      "Epoch: [5]  [39/40]  eta: 0:00:11  lr: 1.0000000000000005e-09  img/s: 11.376689918837785  loss: 0.7024 (0.7517)  acc1: 79.6875 (78.6200)  acc5: 97.6562 (97.7000)  time: 11.1162  data: 0.0170\n",
      "Epoch: [5] Total time: 0:07:33\n",
      "Epoch 7/15\n",
      "Epoch: [6]  [ 0/40]  eta: 0:07:33  lr: 1.0000000000000006e-10  img/s: 11.313408293167047  loss: 0.8994 (0.8994)  acc1: 71.8750 (71.8750)  acc5: 97.6562 (97.6562)  time: 11.3320  data: 0.0180\n",
      "Epoch: [6]  [ 1/40]  eta: 0:07:31  lr: 1.0000000000000006e-10  img/s: 10.833223367501773  loss: 0.7579 (0.8287)  acc1: 71.8750 (76.9531)  acc5: 97.6562 (98.0469)  time: 11.5827  data: 0.0179\n",
      "Epoch: [6]  [ 2/40]  eta: 0:07:21  lr: 1.0000000000000006e-10  img/s: 10.972927326242027  loss: 0.8994 (0.8534)  acc1: 73.4375 (75.7812)  acc5: 97.6562 (97.9167)  time: 11.6161  data: 0.0179\n",
      "Epoch: [6]  [ 3/40]  eta: 0:07:09  lr: 1.0000000000000006e-10  img/s: 11.026618660382818  loss: 0.8276 (0.8469)  acc1: 73.4375 (75.9766)  acc5: 97.6562 (98.0469)  time: 11.6186  data: 0.0179\n",
      "Epoch: [6]  [ 4/40]  eta: 0:06:58  lr: 1.0000000000000006e-10  img/s: 11.058454462137602  loss: 0.8276 (0.8334)  acc1: 76.5625 (77.0312)  acc5: 98.4375 (98.1250)  time: 11.6135  data: 0.0179\n",
      "Epoch: [6]  [ 5/40]  eta: 0:06:46  lr: 1.0000000000000006e-10  img/s: 11.076705711160972  loss: 0.7829 (0.8250)  acc1: 76.5625 (77.0833)  acc5: 98.4375 (98.3073)  time: 11.6068  data: 0.0179\n",
      "Epoch: [6]  [ 6/40]  eta: 0:06:35  lr: 1.0000000000000006e-10  img/s: 10.960833344494159  loss: 0.7829 (0.8048)  acc1: 77.3438 (77.3438)  acc5: 98.4375 (98.4375)  time: 11.6195  data: 0.0179\n",
      "Epoch: [6]  [ 7/40]  eta: 0:06:23  lr: 1.0000000000000006e-10  img/s: 11.11196870233136  loss: 0.7829 (0.8133)  acc1: 76.5625 (76.8555)  acc5: 98.4375 (98.5352)  time: 11.6100  data: 0.0186\n",
      "Epoch: [6]  [ 8/40]  eta: 0:06:11  lr: 1.0000000000000006e-10  img/s: 10.951352025714165  loss: 0.8213 (0.8142)  acc1: 76.5625 (76.8229)  acc5: 98.4375 (98.5243)  time: 11.6206  data: 0.0185\n",
      "Epoch: [6]  [ 9/40]  eta: 0:06:00  lr: 1.0000000000000006e-10  img/s: 10.969181244066126  loss: 0.8213 (0.8479)  acc1: 76.5625 (75.5469)  acc5: 98.4375 (98.4375)  time: 11.6273  data: 0.0185\n",
      "Epoch: [6]  [10/40]  eta: 0:05:48  lr: 1.0000000000000006e-10  img/s: 10.979856996026504  loss: 0.8213 (0.8377)  acc1: 76.5625 (75.9233)  acc5: 98.4375 (98.4375)  time: 11.6316  data: 0.0184\n",
      "Epoch: [6]  [11/40]  eta: 0:05:37  lr: 1.0000000000000006e-10  img/s: 11.100424086467168  loss: 0.7829 (0.8302)  acc1: 76.5625 (76.0417)  acc5: 98.4375 (98.3073)  time: 11.6248  data: 0.0184\n",
      "Epoch: [6]  [12/40]  eta: 0:05:25  lr: 1.0000000000000006e-10  img/s: 10.875111066240773  loss: 0.7829 (0.8256)  acc1: 77.3438 (76.2019)  acc5: 98.4375 (98.3774)  time: 11.6373  data: 0.0184\n",
      "Epoch: [6]  [13/40]  eta: 0:05:14  lr: 1.0000000000000006e-10  img/s: 10.88834860860701  loss: 0.7829 (0.8313)  acc1: 76.5625 (76.1161)  acc5: 98.4375 (98.3259)  time: 11.6471  data: 0.0183\n",
      "Epoch: [6]  [14/40]  eta: 0:05:02  lr: 1.0000000000000006e-10  img/s: 11.134431539362877  loss: 0.7829 (0.8251)  acc1: 77.3438 (76.4062)  acc5: 98.4375 (98.3333)  time: 11.6382  data: 0.0183\n",
      "Epoch: [6]  [15/40]  eta: 0:04:50  lr: 1.0000000000000006e-10  img/s: 11.079547806959262  loss: 0.7794 (0.8206)  acc1: 77.3438 (76.4648)  acc5: 98.4375 (98.3398)  time: 11.6340  data: 0.0183\n",
      "Epoch: [6]  [16/40]  eta: 0:04:39  lr: 1.0000000000000006e-10  img/s: 10.987327644226568  loss: 0.7794 (0.8136)  acc1: 77.3438 (76.6085)  acc5: 98.4375 (98.2996)  time: 11.6359  data: 0.0182\n",
      "Epoch: [6]  [17/40]  eta: 0:04:27  lr: 1.0000000000000006e-10  img/s: 11.014567898929657  loss: 0.7794 (0.8156)  acc1: 77.3438 (76.5625)  acc5: 98.4375 (98.3073)  time: 11.6361  data: 0.0182\n",
      "Epoch: [6]  [18/40]  eta: 0:04:16  lr: 1.0000000000000006e-10  img/s: 10.870479210413402  loss: 0.7794 (0.8118)  acc1: 77.3438 (76.3980)  acc5: 98.4375 (98.3553)  time: 11.6443  data: 0.0182\n",
      "Epoch: [6]  [19/40]  eta: 0:04:04  lr: 1.0000000000000006e-10  img/s: 11.074666646643585  loss: 0.7706 (0.8065)  acc1: 77.3438 (76.4844)  acc5: 98.4375 (98.2812)  time: 11.6409  data: 0.0182\n",
      "Epoch: [6]  [20/40]  eta: 0:03:52  lr: 1.0000000000000006e-10  img/s: 11.124032429730752  loss: 0.7579 (0.8005)  acc1: 77.3438 (76.6369)  acc5: 98.4375 (98.2515)  time: 11.6505  data: 0.0182\n",
      "Epoch: [6]  [21/40]  eta: 0:03:41  lr: 1.0000000000000006e-10  img/s: 11.015585795308576  loss: 0.7706 (0.8006)  acc1: 77.3438 (76.5980)  acc5: 98.4375 (98.2599)  time: 11.6407  data: 0.0182\n",
      "Epoch: [6]  [22/40]  eta: 0:03:29  lr: 1.0000000000000006e-10  img/s: 11.14977296968331  loss: 0.7535 (0.7961)  acc1: 77.3438 (76.6984)  acc5: 98.4375 (98.1997)  time: 11.6315  data: 0.0181\n",
      "Epoch: [6]  [23/40]  eta: 0:03:17  lr: 1.0000000000000006e-10  img/s: 11.156322995896645  loss: 0.7476 (0.7928)  acc1: 77.3438 (76.8880)  acc5: 98.4375 (98.1445)  time: 11.6247  data: 0.0182\n",
      "Epoch: [6]  [24/40]  eta: 0:03:06  lr: 1.0000000000000006e-10  img/s: 10.967125338236892  loss: 0.7476 (0.7916)  acc1: 77.3438 (76.9688)  acc5: 98.4375 (98.1250)  time: 11.6296  data: 0.0182\n",
      "Epoch: [6]  [25/40]  eta: 0:02:54  lr: 1.0000000000000006e-10  img/s: 11.00553282969035  loss: 0.7476 (0.7925)  acc1: 77.3438 (76.9832)  acc5: 97.6562 (98.0769)  time: 11.6333  data: 0.0182\n",
      "Epoch: [6]  [26/40]  eta: 0:02:42  lr: 1.0000000000000006e-10  img/s: 11.08575482821208  loss: 0.7476 (0.7870)  acc1: 77.3438 (77.1701)  acc5: 97.6562 (98.0903)  time: 11.6267  data: 0.0182\n",
      "Epoch: [6]  [27/40]  eta: 0:02:31  lr: 1.0000000000000006e-10  img/s: 11.128566428248034  loss: 0.7437 (0.7847)  acc1: 78.1250 (77.2042)  acc5: 97.6562 (98.0748)  time: 11.6256  data: 0.0179\n",
      "Epoch: [6]  [28/40]  eta: 0:02:19  lr: 1.0000000000000006e-10  img/s: 10.991028529600563  loss: 0.7391 (0.7798)  acc1: 78.1250 (77.2360)  acc5: 97.6562 (98.1142)  time: 11.6235  data: 0.0179\n",
      "Epoch: [6]  [29/40]  eta: 0:02:07  lr: 1.0000000000000006e-10  img/s: 11.131895202820061  loss: 0.7391 (0.7837)  acc1: 78.1250 (77.1354)  acc5: 97.6562 (98.1250)  time: 11.6149  data: 0.0179\n",
      "Epoch: [6]  [30/40]  eta: 0:01:56  lr: 1.0000000000000006e-10  img/s: 11.141153823302684  loss: 0.7391 (0.7807)  acc1: 78.1250 (77.2681)  acc5: 97.6562 (98.1603)  time: 11.6065  data: 0.0179\n",
      "Epoch: [6]  [31/40]  eta: 0:01:44  lr: 1.0000000000000006e-10  img/s: 11.161061091274965  loss: 0.7391 (0.7850)  acc1: 78.1250 (77.1973)  acc5: 97.6562 (98.0225)  time: 11.6033  data: 0.0178\n",
      "Epoch: [6]  [32/40]  eta: 0:01:32  lr: 1.0000000000000006e-10  img/s: 11.186407992507931  loss: 0.7242 (0.7827)  acc1: 78.1250 (77.2017)  acc5: 97.6562 (97.9640)  time: 11.5870  data: 0.0178\n",
      "Epoch: [6]  [33/40]  eta: 0:01:21  lr: 1.0000000000000006e-10  img/s: 11.16506594535892  loss: 0.7167 (0.7808)  acc1: 78.1250 (77.1599)  acc5: 97.6562 (97.9550)  time: 11.5724  data: 0.0178\n",
      "Epoch: [6]  [34/40]  eta: 0:01:09  lr: 1.0000000000000006e-10  img/s: 10.990306961636367  loss: 0.7167 (0.7808)  acc1: 77.3438 (77.0759)  acc5: 97.6562 (97.9688)  time: 11.5799  data: 0.0178\n",
      "Epoch: [6]  [35/40]  eta: 0:00:58  lr: 1.0000000000000006e-10  img/s: 11.123652823443376  loss: 0.7167 (0.7858)  acc1: 77.3438 (77.0182)  acc5: 97.6562 (97.9601)  time: 11.5776  data: 0.0178\n",
      "Epoch: [6]  [36/40]  eta: 0:00:46  lr: 1.0000000000000006e-10  img/s: 10.95618968259164  loss: 0.7242 (0.7861)  acc1: 77.3438 (76.9637)  acc5: 97.6562 (97.9730)  time: 11.5793  data: 0.0179\n",
      "Epoch: [6]  [37/40]  eta: 0:00:34  lr: 1.0000000000000006e-10  img/s: 11.00584304784685  loss: 0.7167 (0.7831)  acc1: 77.3438 (77.0765)  acc5: 97.6562 (97.9646)  time: 11.5798  data: 0.0179\n",
      "Epoch: [6]  [38/40]  eta: 0:00:23  lr: 1.0000000000000006e-10  img/s: 11.097692852963169  loss: 0.7162 (0.7774)  acc1: 78.1250 (77.3237)  acc5: 97.6562 (97.9968)  time: 11.5677  data: 0.0179\n",
      "Epoch: [6]  [39/40]  eta: 0:00:11  lr: 1.0000000000000006e-10  img/s: 11.523533407651165  loss: 0.7167 (0.7830)  acc1: 77.3438 (77.3200)  acc5: 97.6562 (97.9800)  time: 11.0237  data: 0.0171\n",
      "Epoch: [6] Total time: 0:07:33\n",
      "Epoch 8/15\n",
      "Epoch: [7]  [ 0/40]  eta: 0:07:28  lr: 1.0000000000000006e-11  img/s: 11.436700906696835  loss: 1.0032 (1.0032)  acc1: 67.1875 (67.1875)  acc5: 94.5312 (94.5312)  time: 11.2096  data: 0.0176\n",
      "Epoch: [7]  [ 1/40]  eta: 0:07:27  lr: 1.0000000000000006e-11  img/s: 10.94138290472914  loss: 0.7899 (0.8965)  acc1: 67.1875 (70.7031)  acc5: 94.5312 (95.7031)  time: 11.4631  data: 0.0178\n",
      "Epoch: [7]  [ 2/40]  eta: 0:07:15  lr: 1.0000000000000006e-11  img/s: 11.190765552786443  loss: 0.9244 (0.9058)  acc1: 72.6562 (71.3542)  acc5: 96.8750 (96.3542)  time: 11.4607  data: 0.0178\n",
      "Epoch: [7]  [ 3/40]  eta: 0:07:05  lr: 1.0000000000000006e-11  img/s: 10.99521040379799  loss: 0.9244 (0.9219)  acc1: 72.6562 (71.6797)  acc5: 95.3125 (96.0938)  time: 11.5103  data: 0.0178\n",
      "Epoch: [7]  [ 4/40]  eta: 0:06:54  lr: 1.0000000000000006e-11  img/s: 11.127328971573698  loss: 0.9244 (0.8580)  acc1: 72.6562 (75.0000)  acc5: 96.8750 (96.7188)  time: 11.5125  data: 0.0178\n",
      "Epoch: [7]  [ 5/40]  eta: 0:06:43  lr: 1.0000000000000006e-11  img/s: 10.9777575800268  loss: 0.7899 (0.8390)  acc1: 72.6562 (75.3906)  acc5: 96.8750 (97.1354)  time: 11.5400  data: 0.0178\n",
      "Epoch: [7]  [ 6/40]  eta: 0:06:33  lr: 1.0000000000000006e-11  img/s: 10.884609728902982  loss: 0.7899 (0.8108)  acc1: 74.2188 (76.1161)  acc5: 97.6562 (97.4330)  time: 11.5740  data: 0.0178\n",
      "Epoch: [7]  [ 7/40]  eta: 0:06:22  lr: 1.0000000000000006e-11  img/s: 10.962379420565306  loss: 0.7439 (0.7996)  acc1: 74.2188 (76.3672)  acc5: 97.6562 (97.5586)  time: 11.5890  data: 0.0178\n",
      "Epoch: [7]  [ 8/40]  eta: 0:06:11  lr: 1.0000000000000006e-11  img/s: 10.940412339562663  loss: 0.7899 (0.8054)  acc1: 76.5625 (76.3889)  acc5: 97.6562 (97.3958)  time: 11.6033  data: 0.0178\n",
      "Epoch: [7]  [ 9/40]  eta: 0:05:59  lr: 1.0000000000000006e-11  img/s: 10.986486503355398  loss: 0.7899 (0.8212)  acc1: 74.2188 (75.5469)  acc5: 97.6562 (97.5000)  time: 11.6098  data: 0.0178\n",
      "Epoch: [7]  [10/40]  eta: 0:05:48  lr: 1.0000000000000006e-11  img/s: 11.062221433371509  loss: 0.7899 (0.8166)  acc1: 76.5625 (75.8523)  acc5: 98.4375 (97.5852)  time: 11.6079  data: 0.0178\n",
      "Epoch: [7]  [11/40]  eta: 0:05:37  lr: 1.0000000000000006e-11  img/s: 10.865285702877168  loss: 0.7706 (0.8095)  acc1: 76.5625 (76.1719)  acc5: 97.6562 (97.5911)  time: 11.6238  data: 0.0178\n",
      "Epoch: [7]  [12/40]  eta: 0:05:25  lr: 1.0000000000000006e-11  img/s: 11.011538826863614  loss: 0.7706 (0.8011)  acc1: 77.3438 (76.3221)  acc5: 98.4375 (97.6562)  time: 11.6252  data: 0.0178\n",
      "Epoch: [7]  [13/40]  eta: 0:05:13  lr: 1.0000000000000006e-11  img/s: 11.069252056772896  loss: 0.7471 (0.7972)  acc1: 77.3438 (76.5625)  acc5: 98.4375 (97.7121)  time: 11.6220  data: 0.0178\n",
      "Epoch: [7]  [14/40]  eta: 0:05:02  lr: 1.0000000000000006e-11  img/s: 11.06516260239849  loss: 0.7706 (0.7994)  acc1: 78.1250 (76.7708)  acc5: 98.4375 (97.7083)  time: 11.6196  data: 0.0178\n",
      "Epoch: [7]  [15/40]  eta: 0:04:50  lr: 1.0000000000000006e-11  img/s: 10.862506520082665  loss: 0.7471 (0.7919)  acc1: 78.1250 (77.1484)  acc5: 98.4375 (97.8516)  time: 11.6310  data: 0.0178\n",
      "Epoch: [7]  [16/40]  eta: 0:04:39  lr: 1.0000000000000006e-11  img/s: 10.93607421533643  loss: 0.7471 (0.7857)  acc1: 78.1250 (77.3897)  acc5: 98.4375 (97.8860)  time: 11.6364  data: 0.0178\n",
      "Epoch: [7]  [17/40]  eta: 0:04:27  lr: 1.0000000000000006e-11  img/s: 10.85668368724521  loss: 0.7439 (0.7777)  acc1: 78.1250 (77.5174)  acc5: 98.4375 (97.9601)  time: 11.6459  data: 0.0178\n",
      "Epoch: [7]  [18/40]  eta: 0:04:16  lr: 1.0000000000000006e-11  img/s: 11.044810105275355  loss: 0.7439 (0.7747)  acc1: 78.9062 (77.7138)  acc5: 98.4375 (97.9441)  time: 11.6438  data: 0.0178\n",
      "Epoch: [7]  [19/40]  eta: 0:04:04  lr: 1.0000000000000006e-11  img/s: 11.011151050658961  loss: 0.7375 (0.7729)  acc1: 78.1250 (77.6953)  acc5: 98.4375 (98.0078)  time: 11.6438  data: 0.0178\n",
      "Epoch: [7]  [20/40]  eta: 0:03:52  lr: 1.0000000000000006e-11  img/s: 10.935983994995533  loss: 0.7315 (0.7664)  acc1: 78.9062 (77.7902)  acc5: 98.4375 (97.9911)  time: 11.6694  data: 0.0179\n",
      "Epoch: [7]  [21/40]  eta: 0:03:41  lr: 1.0000000000000006e-11  img/s: 10.968589153800536  loss: 0.7213 (0.7629)  acc1: 79.6875 (77.9119)  acc5: 98.4375 (98.0114)  time: 11.6680  data: 0.0179\n",
      "Epoch: [7]  [22/40]  eta: 0:03:29  lr: 1.0000000000000006e-11  img/s: 10.870944309603773  loss: 0.7207 (0.7608)  acc1: 79.6875 (77.8872)  acc5: 98.4375 (97.9959)  time: 11.6848  data: 0.0179\n",
      "Epoch: [7]  [23/40]  eta: 0:03:18  lr: 1.0000000000000006e-11  img/s: 10.962371809974007  loss: 0.7207 (0.7639)  acc1: 79.6875 (77.6693)  acc5: 98.4375 (97.9492)  time: 11.6866  data: 0.0179\n",
      "Epoch: [7]  [24/40]  eta: 0:03:06  lr: 1.0000000000000006e-11  img/s: 10.91161763262546  loss: 0.7213 (0.7630)  acc1: 79.6875 (77.7812)  acc5: 98.4375 (97.9062)  time: 11.6979  data: 0.0179\n",
      "Epoch: [7]  [25/40]  eta: 0:02:54  lr: 1.0000000000000006e-11  img/s: 10.869137403301142  loss: 0.7213 (0.7628)  acc1: 79.6875 (77.7043)  acc5: 98.4375 (97.8666)  time: 11.7038  data: 0.0179\n",
      "Epoch: [7]  [26/40]  eta: 0:02:43  lr: 1.0000000000000006e-11  img/s: 11.007830212457712  loss: 0.7213 (0.7565)  acc1: 79.6875 (77.8935)  acc5: 97.6562 (97.8299)  time: 11.6972  data: 0.0179\n",
      "Epoch: [7]  [27/40]  eta: 0:02:31  lr: 1.0000000000000006e-11  img/s: 11.134604733921416  loss: 0.7315 (0.7585)  acc1: 79.6875 (77.7623)  acc5: 97.6562 (97.8237)  time: 11.6882  data: 0.0179\n",
      "Epoch: [7]  [28/40]  eta: 0:02:19  lr: 1.0000000000000006e-11  img/s: 11.088839761756104  loss: 0.7315 (0.7578)  acc1: 79.6875 (77.6940)  acc5: 97.6562 (97.8448)  time: 11.6803  data: 0.0179\n",
      "Epoch: [7]  [29/40]  eta: 0:02:08  lr: 1.0000000000000006e-11  img/s: 10.93272102434211  loss: 0.7315 (0.7607)  acc1: 79.6875 (77.5260)  acc5: 97.6562 (97.8646)  time: 11.6832  data: 0.0179\n",
      "Epoch: [7]  [30/40]  eta: 0:01:56  lr: 1.0000000000000006e-11  img/s: 10.928598775539783  loss: 0.7207 (0.7556)  acc1: 79.6875 (77.6966)  acc5: 97.6562 (97.8579)  time: 11.6903  data: 0.0179\n",
      "Epoch: [7]  [31/40]  eta: 0:01:44  lr: 1.0000000000000006e-11  img/s: 11.096140024711692  loss: 0.7207 (0.7589)  acc1: 79.6875 (77.6123)  acc5: 97.6562 (97.7051)  time: 11.6780  data: 0.0179\n",
      "Epoch: [7]  [32/40]  eta: 0:01:33  lr: 1.0000000000000006e-11  img/s: 10.896828378218087  loss: 0.7282 (0.7580)  acc1: 79.6875 (77.6989)  acc5: 97.6562 (97.6562)  time: 11.6841  data: 0.0179\n",
      "Epoch: [7]  [33/40]  eta: 0:01:21  lr: 1.0000000000000006e-11  img/s: 10.986496395736665  loss: 0.7282 (0.7587)  acc1: 79.6875 (77.6654)  acc5: 97.6562 (97.6103)  time: 11.6885  data: 0.0179\n",
      "Epoch: [7]  [34/40]  eta: 0:01:09  lr: 1.0000000000000006e-11  img/s: 11.102760801726872  loss: 0.7282 (0.7597)  acc1: 77.3438 (77.6339)  acc5: 97.6562 (97.6786)  time: 11.6865  data: 0.0179\n",
      "Epoch: [7]  [35/40]  eta: 0:00:58  lr: 1.0000000000000006e-11  img/s: 11.0002951533565  loss: 0.7367 (0.7634)  acc1: 77.3438 (77.5391)  acc5: 97.6562 (97.6562)  time: 11.6792  data: 0.0179\n",
      "Epoch: [7]  [36/40]  eta: 0:00:46  lr: 1.0000000000000006e-11  img/s: 11.038554646962076  loss: 0.7375 (0.7628)  acc1: 77.3438 (77.6182)  acc5: 97.6562 (97.6562)  time: 11.6737  data: 0.0179\n",
      "Epoch: [7]  [37/40]  eta: 0:00:34  lr: 1.0000000000000006e-11  img/s: 11.061308621029646  loss: 0.7375 (0.7597)  acc1: 77.3438 (77.7138)  acc5: 97.6562 (97.6562)  time: 11.6628  data: 0.0179\n",
      "Epoch: [7]  [38/40]  eta: 0:00:23  lr: 1.0000000000000006e-11  img/s: 11.096303315004391  loss: 0.7375 (0.7579)  acc1: 77.3438 (77.7244)  acc5: 97.6562 (97.6763)  time: 11.6601  data: 0.0179\n",
      "Epoch: [7]  [39/40]  eta: 0:00:11  lr: 1.0000000000000006e-11  img/s: 10.79184019508298  loss: 0.7367 (0.7561)  acc1: 76.5625 (77.7200)  acc5: 97.6562 (97.6800)  time: 11.1152  data: 0.0171\n",
      "Epoch: [7] Total time: 0:07:35\n",
      "Epoch 9/15\n",
      "Epoch: [8]  [ 0/40]  eta: 0:07:38  lr: 1.0000000000000006e-12  img/s: 11.189520289133362  loss: 0.9983 (0.9983)  acc1: 73.4375 (73.4375)  acc5: 96.8750 (96.8750)  time: 11.4574  data: 0.0181\n",
      "Epoch: [8]  [ 1/40]  eta: 0:07:30  lr: 1.0000000000000006e-12  img/s: 10.986069915379964  loss: 0.7976 (0.8979)  acc1: 73.4375 (74.2188)  acc5: 96.8750 (97.2656)  time: 11.5631  data: 0.0179\n",
      "Epoch: [8]  [ 2/40]  eta: 0:07:21  lr: 1.0000000000000006e-12  img/s: 10.955842908005176  loss: 0.9682 (0.9213)  acc1: 73.4375 (73.4375)  acc5: 96.8750 (97.1354)  time: 11.6091  data: 0.0178\n",
      "Epoch: [8]  [ 3/40]  eta: 0:07:09  lr: 1.0000000000000006e-12  img/s: 11.042894063905846  loss: 0.9352 (0.9248)  acc1: 71.8750 (72.6562)  acc5: 96.8750 (97.2656)  time: 11.6090  data: 0.0178\n",
      "Epoch: [8]  [ 4/40]  eta: 0:06:57  lr: 1.0000000000000006e-12  img/s: 11.04968476231363  loss: 0.9352 (0.8855)  acc1: 73.4375 (74.0625)  acc5: 97.6562 (97.8125)  time: 11.6076  data: 0.0179\n",
      "Epoch: [8]  [ 5/40]  eta: 0:06:46  lr: 1.0000000000000006e-12  img/s: 11.046814777039783  loss: 0.7976 (0.8629)  acc1: 73.4375 (74.8698)  acc5: 97.6562 (97.7865)  time: 11.6072  data: 0.0179\n",
      "Epoch: [8]  [ 6/40]  eta: 0:06:34  lr: 1.0000000000000006e-12  img/s: 11.104442954808922  loss: 0.7976 (0.8299)  acc1: 75.0000 (75.7812)  acc5: 97.6562 (97.7679)  time: 11.5983  data: 0.0179\n",
      "Epoch: [8]  [ 7/40]  eta: 0:06:22  lr: 1.0000000000000006e-12  img/s: 11.106208787764759  loss: 0.7495 (0.8161)  acc1: 75.0000 (76.0742)  acc5: 97.6562 (98.0469)  time: 11.5914  data: 0.0179\n",
      "Epoch: [8]  [ 8/40]  eta: 0:06:10  lr: 1.0000000000000006e-12  img/s: 11.053422547597197  loss: 0.7976 (0.8278)  acc1: 75.0000 (75.6944)  acc5: 97.6562 (97.8299)  time: 11.5921  data: 0.0178\n",
      "Epoch: [8]  [ 9/40]  eta: 0:05:59  lr: 1.0000000000000006e-12  img/s: 11.035620798813802  loss: 0.7976 (0.8466)  acc1: 73.4375 (75.0000)  acc5: 97.6562 (97.6562)  time: 11.5945  data: 0.0178\n",
      "Epoch: [8]  [10/40]  eta: 0:05:47  lr: 1.0000000000000006e-12  img/s: 11.05186661195535  loss: 0.7976 (0.8405)  acc1: 73.4375 (74.7869)  acc5: 97.6562 (97.7273)  time: 11.5950  data: 0.0178\n",
      "Epoch: [8]  [11/40]  eta: 0:05:36  lr: 1.0000000000000006e-12  img/s: 11.07078618182148  loss: 0.7797 (0.8338)  acc1: 73.4375 (75.0651)  acc5: 97.6562 (97.4609)  time: 11.5938  data: 0.0179\n",
      "Epoch: [8]  [12/40]  eta: 0:05:25  lr: 1.0000000000000006e-12  img/s: 10.792472151390344  loss: 0.7797 (0.8202)  acc1: 75.0000 (75.4808)  acc5: 97.6562 (97.6562)  time: 11.6156  data: 0.0179\n",
      "Epoch: [8]  [13/40]  eta: 0:05:13  lr: 1.0000000000000006e-12  img/s: 11.069976496108792  loss: 0.7603 (0.8146)  acc1: 75.0000 (75.6138)  acc5: 97.6562 (97.6562)  time: 11.6131  data: 0.0178\n",
      "Epoch: [8]  [14/40]  eta: 0:05:01  lr: 1.0000000000000006e-12  img/s: 11.086847741970603  loss: 0.7797 (0.8155)  acc1: 77.3438 (75.7292)  acc5: 97.6562 (97.7083)  time: 11.6098  data: 0.0179\n",
      "Epoch: [8]  [15/40]  eta: 0:04:50  lr: 1.0000000000000006e-12  img/s: 11.145522236386643  loss: 0.7749 (0.8130)  acc1: 76.5625 (75.7812)  acc5: 97.6562 (97.8516)  time: 11.6031  data: 0.0178\n",
      "Epoch: [8]  [16/40]  eta: 0:04:38  lr: 1.0000000000000006e-12  img/s: 11.132296146899197  loss: 0.7749 (0.8040)  acc1: 77.3438 (76.0110)  acc5: 97.6562 (97.8401)  time: 11.5979  data: 0.0179\n",
      "Epoch: [8]  [17/40]  eta: 0:04:26  lr: 1.0000000000000006e-12  img/s: 11.068740395893949  loss: 0.7603 (0.7947)  acc1: 77.3438 (76.2153)  acc5: 97.6562 (97.9167)  time: 11.5970  data: 0.0179\n",
      "Epoch: [8]  [18/40]  eta: 0:04:15  lr: 1.0000000000000006e-12  img/s: 11.143934714135218  loss: 0.7749 (0.7950)  acc1: 77.3438 (76.3158)  acc5: 97.6562 (97.8207)  time: 11.5921  data: 0.0179\n",
      "Epoch: [8]  [19/40]  eta: 0:04:03  lr: 1.0000000000000006e-12  img/s: 11.170510318987677  loss: 0.7603 (0.7900)  acc1: 77.3438 (76.4062)  acc5: 97.6562 (97.8516)  time: 11.5864  data: 0.0179\n",
      "Epoch: [8]  [20/40]  eta: 0:03:51  lr: 1.0000000000000006e-12  img/s: 11.165667130371645  loss: 0.7603 (0.7964)  acc1: 77.3438 (76.1533)  acc5: 97.6562 (97.7679)  time: 11.5876  data: 0.0178\n",
      "Epoch: [8]  [21/40]  eta: 0:03:39  lr: 1.0000000000000006e-12  img/s: 11.185500439193751  loss: 0.7495 (0.7908)  acc1: 78.1250 (76.2784)  acc5: 97.6562 (97.7983)  time: 11.5772  data: 0.0179\n",
      "Epoch: [8]  [22/40]  eta: 0:03:28  lr: 1.0000000000000006e-12  img/s: 11.161085222278018  loss: 0.7427 (0.7885)  acc1: 78.1250 (76.4266)  acc5: 97.6562 (97.7921)  time: 11.5665  data: 0.0179\n",
      "Epoch: [8]  [23/40]  eta: 0:03:16  lr: 1.0000000000000006e-12  img/s: 11.004844320674064  loss: 0.7369 (0.7844)  acc1: 78.1250 (76.6602)  acc5: 97.6562 (97.7865)  time: 11.5685  data: 0.0179\n",
      "Epoch: [8]  [24/40]  eta: 0:03:05  lr: 1.0000000000000006e-12  img/s: 11.083888170247176  loss: 0.7427 (0.7863)  acc1: 78.1250 (76.6875)  acc5: 97.6562 (97.8125)  time: 11.5667  data: 0.0179\n",
      "Epoch: [8]  [25/40]  eta: 0:02:53  lr: 1.0000000000000006e-12  img/s: 10.916805813155161  loss: 0.7369 (0.7836)  acc1: 78.1250 (76.8630)  acc5: 97.6562 (97.6863)  time: 11.5736  data: 0.0178\n",
      "Epoch: [8]  [26/40]  eta: 0:02:42  lr: 1.0000000000000006e-12  img/s: 11.045260019281802  loss: 0.7369 (0.7769)  acc1: 78.1250 (77.1123)  acc5: 97.6562 (97.6852)  time: 11.5766  data: 0.0178\n",
      "Epoch: [8]  [27/40]  eta: 0:02:30  lr: 1.0000000000000006e-12  img/s: 10.98851683827134  loss: 0.7369 (0.7750)  acc1: 78.1250 (77.1484)  acc5: 97.6562 (97.7121)  time: 11.5828  data: 0.0178\n",
      "Epoch: [8]  [28/40]  eta: 0:02:19  lr: 1.0000000000000006e-12  img/s: 10.98732809394873  loss: 0.7224 (0.7724)  acc1: 78.1250 (77.2091)  acc5: 97.6562 (97.7640)  time: 11.5863  data: 0.0179\n",
      "Epoch: [8]  [29/40]  eta: 0:02:07  lr: 1.0000000000000006e-12  img/s: 10.991152062894496  loss: 0.7224 (0.7786)  acc1: 78.1250 (76.9792)  acc5: 98.4375 (97.8125)  time: 11.5887  data: 0.0179\n",
      "Epoch: [8]  [30/40]  eta: 0:01:55  lr: 1.0000000000000006e-12  img/s: 11.046897288474561  loss: 0.7162 (0.7735)  acc1: 78.1250 (77.0161)  acc5: 98.4375 (97.8579)  time: 11.5890  data: 0.0179\n",
      "Epoch: [8]  [31/40]  eta: 0:01:44  lr: 1.0000000000000006e-12  img/s: 11.080699415976905  loss: 0.7162 (0.7763)  acc1: 78.1250 (76.8799)  acc5: 98.4375 (97.8027)  time: 11.5884  data: 0.0179\n",
      "Epoch: [8]  [32/40]  eta: 0:01:32  lr: 1.0000000000000006e-12  img/s: 10.99343781726356  loss: 0.7162 (0.7742)  acc1: 78.1250 (76.9650)  acc5: 97.6562 (97.7746)  time: 11.5776  data: 0.0179\n",
      "Epoch: [8]  [33/40]  eta: 0:01:21  lr: 1.0000000000000006e-12  img/s: 10.958512587387851  loss: 0.7162 (0.7734)  acc1: 78.1250 (76.8842)  acc5: 97.6562 (97.7482)  time: 11.5835  data: 0.0179\n",
      "Epoch: [8]  [34/40]  eta: 0:01:09  lr: 1.0000000000000006e-12  img/s: 10.985476000539707  loss: 0.7162 (0.7733)  acc1: 78.1250 (76.9866)  acc5: 97.6562 (97.7902)  time: 11.5888  data: 0.0179\n",
      "Epoch: [8]  [35/40]  eta: 0:00:58  lr: 1.0000000000000006e-12  img/s: 10.877064740924123  loss: 0.7077 (0.7715)  acc1: 78.9062 (77.0616)  acc5: 97.6562 (97.8299)  time: 11.6030  data: 0.0179\n",
      "Epoch: [8]  [36/40]  eta: 0:00:46  lr: 1.0000000000000006e-12  img/s: 10.93692213650711  loss: 0.7077 (0.7687)  acc1: 78.9062 (77.1537)  acc5: 98.4375 (97.8463)  time: 11.6135  data: 0.0181\n",
      "Epoch: [8]  [37/40]  eta: 0:00:34  lr: 1.0000000000000006e-12  img/s: 10.997740475889348  loss: 0.7077 (0.7652)  acc1: 78.9062 (77.3232)  acc5: 97.6562 (97.8002)  time: 11.6172  data: 0.0181\n",
      "Epoch: [8]  [38/40]  eta: 0:00:23  lr: 1.0000000000000006e-12  img/s: 10.868714484718295  loss: 0.7072 (0.7629)  acc1: 78.9062 (77.3438)  acc5: 98.4375 (97.8365)  time: 11.6318  data: 0.0181\n",
      "Epoch: [8]  [39/40]  eta: 0:00:11  lr: 1.0000000000000006e-12  img/s: 11.465474004830924  loss: 0.7072 (0.7592)  acc1: 78.9062 (77.3600)  acc5: 98.4375 (97.8400)  time: 11.0929  data: 0.0173\n",
      "Epoch: [8] Total time: 0:07:33\n",
      "Epoch 10/15\n",
      "Epoch: [9]  [ 0/40]  eta: 0:07:29  lr: 1.0000000000000007e-13  img/s: 11.400845768162824  loss: 0.9806 (0.9806)  acc1: 67.9688 (67.9688)  acc5: 98.4375 (98.4375)  time: 11.2447  data: 0.0175\n",
      "Epoch: [9]  [ 1/40]  eta: 0:07:27  lr: 1.0000000000000007e-13  img/s: 10.961343805750788  loss: 0.7776 (0.8791)  acc1: 67.9688 (73.4375)  acc5: 97.6562 (98.0469)  time: 11.4699  data: 0.0176\n",
      "Epoch: [9]  [ 2/40]  eta: 0:07:18  lr: 1.0000000000000007e-13  img/s: 11.01110385087717  loss: 0.7963 (0.8515)  acc1: 78.1250 (75.0000)  acc5: 98.4375 (98.1771)  time: 11.5273  data: 0.0176\n",
      "Epoch: [9]  [ 3/40]  eta: 0:07:07  lr: 1.0000000000000007e-13  img/s: 11.031951033812902  loss: 0.7963 (0.8716)  acc1: 71.8750 (74.2188)  acc5: 97.6562 (97.8516)  time: 11.5507  data: 0.0177\n",
      "Epoch: [9]  [ 4/40]  eta: 0:06:56  lr: 1.0000000000000007e-13  img/s: 11.058320528069697  loss: 0.7963 (0.8364)  acc1: 78.1250 (75.3125)  acc5: 98.4375 (98.1250)  time: 11.5591  data: 0.0177\n",
      "Epoch: [9]  [ 5/40]  eta: 0:06:44  lr: 1.0000000000000007e-13  img/s: 11.027550898632851  loss: 0.7963 (0.8379)  acc1: 73.4375 (75.0000)  acc5: 98.4375 (98.1771)  time: 11.5701  data: 0.0177\n",
      "Epoch: [9]  [ 6/40]  eta: 0:06:32  lr: 1.0000000000000007e-13  img/s: 11.159078760967107  loss: 0.7963 (0.8116)  acc1: 78.1250 (76.1161)  acc5: 98.4375 (98.3259)  time: 11.5584  data: 0.0177\n",
      "Epoch: [9]  [ 7/40]  eta: 0:06:22  lr: 1.0000000000000007e-13  img/s: 10.887194242165004  loss: 0.7776 (0.8042)  acc1: 74.2188 (75.8789)  acc5: 98.4375 (98.4375)  time: 11.5855  data: 0.0178\n",
      "Epoch: [9]  [ 8/40]  eta: 0:06:10  lr: 1.0000000000000007e-13  img/s: 11.146131500460413  loss: 0.7963 (0.8066)  acc1: 74.2188 (75.6076)  acc5: 98.4375 (98.2639)  time: 11.5762  data: 0.0178\n",
      "Epoch: [9]  [ 9/40]  eta: 0:05:58  lr: 1.0000000000000007e-13  img/s: 11.157706969272704  loss: 0.7963 (0.8367)  acc1: 73.4375 (75.2344)  acc5: 98.4375 (98.2031)  time: 11.5675  data: 0.0178\n",
      "Epoch: [9]  [10/40]  eta: 0:05:47  lr: 1.0000000000000007e-13  img/s: 10.990323835389889  loss: 0.7963 (0.8284)  acc1: 74.2188 (75.3551)  acc5: 98.4375 (98.2244)  time: 11.5764  data: 0.0178\n",
      "Epoch: [9]  [11/40]  eta: 0:05:35  lr: 1.0000000000000007e-13  img/s: 11.122556559006266  loss: 0.7776 (0.8146)  acc1: 74.2188 (75.7812)  acc5: 98.4375 (97.9818)  time: 11.5722  data: 0.0178\n",
      "Epoch: [9]  [12/40]  eta: 0:05:24  lr: 1.0000000000000007e-13  img/s: 11.008422483865553  loss: 0.7776 (0.8033)  acc1: 76.5625 (76.3221)  acc5: 98.4375 (98.0168)  time: 11.5778  data: 0.0178\n",
      "Epoch: [9]  [13/40]  eta: 0:05:12  lr: 1.0000000000000007e-13  img/s: 11.016161722322568  loss: 0.7529 (0.7906)  acc1: 76.5625 (76.8415)  acc5: 98.4375 (97.9911)  time: 11.5820  data: 0.0178\n",
      "Epoch: [9]  [14/40]  eta: 0:05:01  lr: 1.0000000000000007e-13  img/s: 10.992591914329388  loss: 0.7776 (0.7947)  acc1: 76.5625 (76.5625)  acc5: 98.4375 (98.0208)  time: 11.5874  data: 0.0178\n",
      "Epoch: [9]  [15/40]  eta: 0:04:49  lr: 1.0000000000000007e-13  img/s: 11.063892691943128  loss: 0.7529 (0.7849)  acc1: 76.5625 (76.9043)  acc5: 98.4375 (98.0957)  time: 11.5873  data: 0.0178\n",
      "Epoch: [9]  [16/40]  eta: 0:04:38  lr: 1.0000000000000007e-13  img/s: 11.091336808228702  loss: 0.7658 (0.7838)  acc1: 76.5625 (76.7923)  acc5: 98.4375 (98.1618)  time: 11.5856  data: 0.0178\n",
      "Epoch: [9]  [17/40]  eta: 0:04:26  lr: 1.0000000000000007e-13  img/s: 11.11858927549095  loss: 0.7658 (0.7842)  acc1: 76.5625 (76.7795)  acc5: 98.4375 (98.1337)  time: 11.5825  data: 0.0178\n",
      "Epoch: [9]  [18/40]  eta: 0:04:14  lr: 1.0000000000000007e-13  img/s: 11.092794090883771  loss: 0.7658 (0.7816)  acc1: 76.5625 (76.9737)  acc5: 98.4375 (98.0674)  time: 11.5812  data: 0.0178\n",
      "Epoch: [9]  [19/40]  eta: 0:04:03  lr: 1.0000000000000007e-13  img/s: 11.14539382044215  loss: 0.7529 (0.7737)  acc1: 76.5625 (77.2656)  acc5: 98.4375 (98.0859)  time: 11.5772  data: 0.0178\n",
      "Epoch: [9]  [20/40]  eta: 0:03:51  lr: 1.0000000000000007e-13  img/s: 11.019309815988697  loss: 0.7529 (0.7751)  acc1: 78.1250 (77.3438)  acc5: 98.4375 (97.9911)  time: 11.5967  data: 0.0178\n",
      "Epoch: [9]  [21/40]  eta: 0:03:39  lr: 1.0000000000000007e-13  img/s: 11.136766886334994  loss: 0.7529 (0.7788)  acc1: 76.5625 (77.2372)  acc5: 98.4375 (98.0114)  time: 11.5876  data: 0.0179\n",
      "Epoch: [9]  [22/40]  eta: 0:03:28  lr: 1.0000000000000007e-13  img/s: 10.935735841044897  loss: 0.7529 (0.7789)  acc1: 76.5625 (77.2418)  acc5: 98.4375 (97.9620)  time: 11.5916  data: 0.0179\n",
      "Epoch: [9]  [23/40]  eta: 0:03:17  lr: 1.0000000000000007e-13  img/s: 10.9513238785249  loss: 0.7458 (0.7773)  acc1: 77.3438 (77.2461)  acc5: 98.4375 (98.0143)  time: 11.5959  data: 0.0179\n",
      "Epoch: [9]  [24/40]  eta: 0:03:05  lr: 1.0000000000000007e-13  img/s: 11.038296596368912  loss: 0.7529 (0.7797)  acc1: 76.5625 (77.1250)  acc5: 98.4375 (98.0312)  time: 11.5969  data: 0.0179\n",
      "Epoch: [9]  [25/40]  eta: 0:02:53  lr: 1.0000000000000007e-13  img/s: 11.051941690921767  loss: 0.7458 (0.7747)  acc1: 77.3438 (77.2536)  acc5: 98.4375 (98.0168)  time: 11.5956  data: 0.0179\n",
      "Epoch: [9]  [26/40]  eta: 0:02:42  lr: 1.0000000000000007e-13  img/s: 11.035827455838984  loss: 0.7458 (0.7699)  acc1: 77.3438 (77.3727)  acc5: 97.6562 (97.9745)  time: 11.6020  data: 0.0179\n",
      "Epoch: [9]  [27/40]  eta: 0:02:30  lr: 1.0000000000000007e-13  img/s: 11.005515909202957  loss: 0.7458 (0.7718)  acc1: 77.3438 (77.3158)  acc5: 97.6562 (97.9911)  time: 11.5957  data: 0.0179\n",
      "Epoch: [9]  [28/40]  eta: 0:02:19  lr: 1.0000000000000007e-13  img/s: 11.015059421761142  loss: 0.7395 (0.7695)  acc1: 77.3438 (77.4784)  acc5: 98.4375 (98.0065)  time: 11.6025  data: 0.0179\n",
      "Epoch: [9]  [29/40]  eta: 0:02:07  lr: 1.0000000000000007e-13  img/s: 10.947957089024174  loss: 0.7395 (0.7788)  acc1: 77.3438 (77.1615)  acc5: 98.4375 (97.9948)  time: 11.6135  data: 0.0179\n",
      "Epoch: [9]  [30/40]  eta: 0:01:56  lr: 1.0000000000000007e-13  img/s: 10.909400354642795  loss: 0.7343 (0.7724)  acc1: 78.9062 (77.3942)  acc5: 98.4375 (98.0091)  time: 11.6179  data: 0.0179\n",
      "Epoch: [9]  [31/40]  eta: 0:01:44  lr: 1.0000000000000007e-13  img/s: 10.927889384959759  loss: 0.7395 (0.7774)  acc1: 77.3438 (77.3438)  acc5: 98.4375 (97.9248)  time: 11.6281  data: 0.0179\n",
      "Epoch: [9]  [32/40]  eta: 0:01:32  lr: 1.0000000000000007e-13  img/s: 11.034414584369001  loss: 0.7395 (0.7758)  acc1: 77.3438 (77.3674)  acc5: 97.6562 (97.7983)  time: 11.6267  data: 0.0179\n",
      "Epoch: [9]  [33/40]  eta: 0:01:21  lr: 1.0000000000000007e-13  img/s: 10.99471929978623  loss: 0.7658 (0.7761)  acc1: 77.3438 (77.2978)  acc5: 97.6562 (97.7482)  time: 11.6279  data: 0.0179\n",
      "Epoch: [9]  [34/40]  eta: 0:01:09  lr: 1.0000000000000007e-13  img/s: 10.948141721941736  loss: 0.7658 (0.7775)  acc1: 77.3438 (77.1652)  acc5: 97.6562 (97.7902)  time: 11.6305  data: 0.0181\n",
      "Epoch: [9]  [35/40]  eta: 0:00:58  lr: 1.0000000000000007e-13  img/s: 11.016934164824153  loss: 0.7805 (0.7790)  acc1: 77.3438 (77.1918)  acc5: 97.6562 (97.7865)  time: 11.6329  data: 0.0181\n",
      "Epoch: [9]  [36/40]  eta: 0:00:46  lr: 1.0000000000000007e-13  img/s: 10.979657369909933  loss: 0.7805 (0.7761)  acc1: 77.3438 (77.2171)  acc5: 97.6562 (97.8041)  time: 11.6388  data: 0.0181\n",
      "Epoch: [9]  [37/40]  eta: 0:00:34  lr: 1.0000000000000007e-13  img/s: 10.887899904581216  loss: 0.7395 (0.7710)  acc1: 78.1250 (77.4465)  acc5: 97.6562 (97.8002)  time: 11.6510  data: 0.0181\n",
      "Epoch: [9]  [38/40]  eta: 0:00:23  lr: 1.0000000000000007e-13  img/s: 11.026202420252146  loss: 0.7395 (0.7674)  acc1: 78.1250 (77.5240)  acc5: 97.6562 (97.8365)  time: 11.6545  data: 0.0181\n",
      "Epoch: [9]  [39/40]  eta: 0:00:11  lr: 1.0000000000000007e-13  img/s: 11.445442467386503  loss: 0.7395 (0.7624)  acc1: 78.1250 (77.5400)  acc5: 97.6562 (97.8400)  time: 11.1144  data: 0.0174\n",
      "Epoch: [9] Total time: 0:07:33\n",
      "Epoch 11/15\n",
      "Epoch: [10]  [ 0/40]  eta: 0:07:32  lr: 1.0000000000000008e-14  img/s: 11.329119336613209  loss: 0.8028 (0.8028)  acc1: 75.7812 (75.7812)  acc5: 97.6562 (97.6562)  time: 11.3162  data: 0.0178\n",
      "Epoch: [10]  [ 1/40]  eta: 0:07:31  lr: 1.0000000000000008e-14  img/s: 10.809379739930232  loss: 0.7621 (0.7825)  acc1: 75.7812 (76.1719)  acc5: 97.6562 (98.0469)  time: 11.5877  data: 0.0178\n",
      "Epoch: [10]  [ 2/40]  eta: 0:07:20  lr: 1.0000000000000008e-14  img/s: 11.08053361160347  loss: 0.8028 (0.8047)  acc1: 75.7812 (75.7812)  acc5: 98.4375 (98.1771)  time: 11.5817  data: 0.0178\n",
      "Epoch: [10]  [ 3/40]  eta: 0:07:08  lr: 1.0000000000000008e-14  img/s: 11.109517071952867  loss: 0.8028 (0.8382)  acc1: 75.0000 (75.1953)  acc5: 97.6562 (97.8516)  time: 11.5712  data: 0.0178\n",
      "Epoch: [10]  [ 4/40]  eta: 0:06:57  lr: 1.0000000000000008e-14  img/s: 10.947540515741311  loss: 0.8028 (0.8152)  acc1: 75.7812 (75.4688)  acc5: 97.6562 (97.8125)  time: 11.5989  data: 0.0178\n",
      "Epoch: [10]  [ 5/40]  eta: 0:06:47  lr: 1.0000000000000008e-14  img/s: 10.879066955480441  loss: 0.7888 (0.8108)  acc1: 75.7812 (75.9115)  acc5: 97.6562 (97.9167)  time: 11.6297  data: 0.0178\n",
      "Epoch: [10]  [ 6/40]  eta: 0:06:35  lr: 1.0000000000000008e-14  img/s: 11.053971257046902  loss: 0.7888 (0.7811)  acc1: 76.5625 (77.0089)  acc5: 98.4375 (98.1027)  time: 11.6251  data: 0.0178\n",
      "Epoch: [10]  [ 7/40]  eta: 0:06:23  lr: 1.0000000000000008e-14  img/s: 11.050139849782406  loss: 0.7888 (0.7918)  acc1: 75.7812 (75.9766)  acc5: 98.4375 (98.2422)  time: 11.6221  data: 0.0178\n",
      "Epoch: [10]  [ 8/40]  eta: 0:06:12  lr: 1.0000000000000008e-14  img/s: 10.897973948959926  loss: 0.8028 (0.7996)  acc1: 75.7812 (75.7812)  acc5: 98.4375 (98.1771)  time: 11.6377  data: 0.0178\n",
      "Epoch: [10]  [ 9/40]  eta: 0:06:00  lr: 1.0000000000000008e-14  img/s: 10.957098868347314  loss: 0.8028 (0.8149)  acc1: 75.0000 (75.5469)  acc5: 97.6562 (98.0469)  time: 11.6439  data: 0.0178\n",
      "Epoch: [10]  [10/40]  eta: 0:05:49  lr: 1.0000000000000008e-14  img/s: 11.04771746914502  loss: 0.8028 (0.8123)  acc1: 75.7812 (75.6392)  acc5: 98.4375 (98.0824)  time: 11.6403  data: 0.0178\n",
      "Epoch: [10]  [11/40]  eta: 0:05:38  lr: 1.0000000000000008e-14  img/s: 10.841950690745811  loss: 0.7888 (0.8052)  acc1: 75.7812 (75.9766)  acc5: 97.6562 (97.9167)  time: 11.6556  data: 0.0178\n",
      "Epoch: [10]  [12/40]  eta: 0:05:26  lr: 1.0000000000000008e-14  img/s: 11.100287297781723  loss: 0.7888 (0.7959)  acc1: 76.5625 (76.1418)  acc5: 98.4375 (98.0168)  time: 11.6474  data: 0.0178\n",
      "Epoch: [10]  [13/40]  eta: 0:05:14  lr: 1.0000000000000008e-14  img/s: 10.978362109760859  loss: 0.7862 (0.7906)  acc1: 76.5625 (76.5067)  acc5: 97.6562 (97.9911)  time: 11.6495  data: 0.0178\n",
      "Epoch: [10]  [14/40]  eta: 0:05:02  lr: 1.0000000000000008e-14  img/s: 11.063032036724085  loss: 0.7862 (0.7901)  acc1: 76.5625 (76.7188)  acc5: 97.6562 (97.9688)  time: 11.6454  data: 0.0178\n",
      "Epoch: [10]  [15/40]  eta: 0:04:51  lr: 1.0000000000000008e-14  img/s: 11.05610311526959  loss: 0.7841 (0.7819)  acc1: 76.5625 (76.7578)  acc5: 97.6562 (97.9980)  time: 11.6422  data: 0.0178\n",
      "Epoch: [10]  [16/40]  eta: 0:04:39  lr: 1.0000000000000008e-14  img/s: 11.133039481754958  loss: 0.7841 (0.7788)  acc1: 76.5625 (76.9761)  acc5: 98.4375 (98.0699)  time: 11.6348  data: 0.0178\n",
      "Epoch: [10]  [17/40]  eta: 0:04:27  lr: 1.0000000000000008e-14  img/s: 10.966156248761031  loss: 0.7621 (0.7772)  acc1: 76.5625 (76.9531)  acc5: 98.4375 (98.1337)  time: 11.6378  data: 0.0178\n",
      "Epoch: [10]  [18/40]  eta: 0:04:16  lr: 1.0000000000000008e-14  img/s: 11.010412838201185  loss: 0.7621 (0.7687)  acc1: 76.5625 (77.2615)  acc5: 98.4375 (98.1497)  time: 11.6381  data: 0.0178\n",
      "Epoch: [10]  [19/40]  eta: 0:04:04  lr: 1.0000000000000008e-14  img/s: 11.008583428381234  loss: 0.7485 (0.7670)  acc1: 76.5625 (77.2656)  acc5: 98.4375 (98.1641)  time: 11.6385  data: 0.0178\n",
      "Epoch: [10]  [20/40]  eta: 0:03:52  lr: 1.0000000000000008e-14  img/s: 10.99401165575817  loss: 0.7434 (0.7658)  acc1: 76.5625 (77.2321)  acc5: 98.4375 (98.1399)  time: 11.6557  data: 0.0178\n",
      "Epoch: [10]  [21/40]  eta: 0:03:41  lr: 1.0000000000000008e-14  img/s: 11.050333858985036  loss: 0.7434 (0.7672)  acc1: 76.5625 (77.1662)  acc5: 98.4375 (98.1534)  time: 11.6428  data: 0.0178\n",
      "Epoch: [10]  [22/40]  eta: 0:03:29  lr: 1.0000000000000008e-14  img/s: 10.910977855428966  loss: 0.7434 (0.7662)  acc1: 77.3438 (77.2418)  acc5: 98.4375 (98.1318)  time: 11.6518  data: 0.0178\n",
      "Epoch: [10]  [23/40]  eta: 0:03:17  lr: 1.0000000000000008e-14  img/s: 10.93690097029563  loss: 0.7339 (0.7622)  acc1: 77.3438 (77.4414)  acc5: 98.4375 (98.1120)  time: 11.6609  data: 0.0178\n",
      "Epoch: [10]  [24/40]  eta: 0:03:06  lr: 1.0000000000000008e-14  img/s: 11.02263124853471  loss: 0.7434 (0.7629)  acc1: 77.3438 (77.4062)  acc5: 98.4375 (98.0938)  time: 11.6569  data: 0.0178\n",
      "Epoch: [10]  [25/40]  eta: 0:02:54  lr: 1.0000000000000008e-14  img/s: 10.812275733568272  loss: 0.7434 (0.7627)  acc1: 77.3438 (77.3438)  acc5: 97.6562 (98.0469)  time: 11.6605  data: 0.0177\n",
      "Epoch: [10]  [26/40]  eta: 0:02:43  lr: 1.0000000000000008e-14  img/s: 11.052314370707874  loss: 0.7434 (0.7591)  acc1: 77.3438 (77.6042)  acc5: 97.6562 (98.0613)  time: 11.6606  data: 0.0177\n",
      "Epoch: [10]  [27/40]  eta: 0:02:31  lr: 1.0000000000000008e-14  img/s: 10.922925561691882  loss: 0.7434 (0.7587)  acc1: 77.3438 (77.4554)  acc5: 97.6562 (98.0190)  time: 11.6673  data: 0.0178\n",
      "Epoch: [10]  [28/40]  eta: 0:02:19  lr: 1.0000000000000008e-14  img/s: 11.061724325122537  loss: 0.7339 (0.7542)  acc1: 77.3438 (77.6131)  acc5: 97.6562 (98.0334)  time: 11.6586  data: 0.0178\n",
      "Epoch: [10]  [29/40]  eta: 0:02:08  lr: 1.0000000000000008e-14  img/s: 11.038970458348945  loss: 0.7339 (0.7612)  acc1: 77.3438 (77.5000)  acc5: 97.6562 (98.0208)  time: 11.6543  data: 0.0178\n",
      "Epoch: [10]  [30/40]  eta: 0:01:56  lr: 1.0000000000000008e-14  img/s: 10.853629351906385  loss: 0.7302 (0.7564)  acc1: 78.1250 (77.6966)  acc5: 97.6562 (97.9839)  time: 11.6647  data: 0.0178\n",
      "Epoch: [10]  [31/40]  eta: 0:01:44  lr: 1.0000000000000008e-14  img/s: 11.104830438936567  loss: 0.7339 (0.7608)  acc1: 77.3438 (77.5879)  acc5: 97.6562 (97.9492)  time: 11.6507  data: 0.0178\n",
      "Epoch: [10]  [32/40]  eta: 0:01:33  lr: 1.0000000000000008e-14  img/s: 11.022257852316285  loss: 0.7434 (0.7604)  acc1: 77.3438 (77.5805)  acc5: 97.6562 (97.9167)  time: 11.6548  data: 0.0178\n",
      "Epoch: [10]  [33/40]  eta: 0:01:21  lr: 1.0000000000000008e-14  img/s: 11.01299780565053  loss: 0.7434 (0.7587)  acc1: 77.3438 (77.6654)  acc5: 97.6562 (97.8631)  time: 11.6529  data: 0.0178\n",
      "Epoch: [10]  [34/40]  eta: 0:01:09  lr: 1.0000000000000008e-14  img/s: 10.928273098438522  loss: 0.7339 (0.7567)  acc1: 77.3438 (77.7679)  acc5: 97.6562 (97.9018)  time: 11.6601  data: 0.0178\n",
      "Epoch: [10]  [35/40]  eta: 0:00:58  lr: 1.0000000000000008e-14  img/s: 10.920051070937944  loss: 0.7434 (0.7587)  acc1: 77.3438 (77.6910)  acc5: 97.6562 (97.9167)  time: 11.6673  data: 0.0178\n",
      "Epoch: [10]  [36/40]  eta: 0:00:46  lr: 1.0000000000000008e-14  img/s: 11.03756268201195  loss: 0.7434 (0.7554)  acc1: 77.3438 (77.7872)  acc5: 97.6562 (97.9519)  time: 11.6723  data: 0.0178\n",
      "Epoch: [10]  [37/40]  eta: 0:00:34  lr: 1.0000000000000008e-14  img/s: 10.847143825019241  loss: 0.7339 (0.7537)  acc1: 77.3438 (77.7961)  acc5: 97.6562 (97.9441)  time: 11.6787  data: 0.0178\n",
      "Epoch: [10]  [38/40]  eta: 0:00:23  lr: 1.0000000000000008e-14  img/s: 11.023387170443385  loss: 0.7339 (0.7518)  acc1: 77.3438 (77.7644)  acc5: 97.6562 (97.9968)  time: 11.6780  data: 0.0178\n",
      "Epoch: [10]  [39/40]  eta: 0:00:11  lr: 1.0000000000000008e-14  img/s: 10.395665316802846  loss: 0.7434 (0.7545)  acc1: 76.5625 (77.7600)  acc5: 97.6562 (98.0000)  time: 11.1343  data: 0.0170\n",
      "Epoch: [10] Total time: 0:07:35\n",
      "Epoch 12/15\n",
      "Epoch: [11]  [ 0/40]  eta: 0:07:33  lr: 1.0000000000000009e-15  img/s: 11.313383499005026  loss: 0.9111 (0.9111)  acc1: 70.3125 (70.3125)  acc5: 97.6562 (97.6562)  time: 11.3319  data: 0.0179\n",
      "Epoch: [11]  [ 1/40]  eta: 0:07:32  lr: 1.0000000000000009e-15  img/s: 10.809022392659726  loss: 0.7825 (0.8468)  acc1: 70.3125 (74.6094)  acc5: 96.0938 (96.8750)  time: 11.5958  data: 0.0178\n",
      "Epoch: [11]  [ 2/40]  eta: 0:07:20  lr: 1.0000000000000009e-15  img/s: 11.058350822396374  loss: 0.7999 (0.8312)  acc1: 72.6562 (73.9583)  acc5: 97.6562 (97.3958)  time: 11.5948  data: 0.0178\n",
      "Epoch: [11]  [ 3/40]  eta: 0:07:08  lr: 1.0000000000000009e-15  img/s: 11.057245756328062  loss: 0.7999 (0.8676)  acc1: 71.8750 (73.4375)  acc5: 97.6562 (97.6562)  time: 11.5945  data: 0.0177\n",
      "Epoch: [11]  [ 4/40]  eta: 0:06:56  lr: 1.0000000000000009e-15  img/s: 11.110801847038582  loss: 0.7999 (0.8425)  acc1: 72.6562 (74.0625)  acc5: 98.4375 (97.9688)  time: 11.5832  data: 0.0177\n",
      "Epoch: [11]  [ 5/40]  eta: 0:06:45  lr: 1.0000000000000009e-15  img/s: 11.082670924316346  loss: 0.7825 (0.8296)  acc1: 72.6562 (74.0885)  acc5: 97.6562 (97.9167)  time: 11.5806  data: 0.0178\n",
      "Epoch: [11]  [ 6/40]  eta: 0:06:33  lr: 1.0000000000000009e-15  img/s: 11.033107507150998  loss: 0.7825 (0.7925)  acc1: 74.2188 (75.3348)  acc5: 98.4375 (97.9911)  time: 11.5861  data: 0.0178\n",
      "Epoch: [11]  [ 7/40]  eta: 0:06:21  lr: 1.0000000000000009e-15  img/s: 11.206396053197802  loss: 0.7825 (0.7953)  acc1: 74.2188 (75.1953)  acc5: 98.4375 (98.2422)  time: 11.5678  data: 0.0177\n",
      "Epoch: [11]  [ 8/40]  eta: 0:06:09  lr: 1.0000000000000009e-15  img/s: 11.184485850556756  loss: 0.7999 (0.8027)  acc1: 74.2188 (75.4340)  acc5: 98.4375 (98.0903)  time: 11.5561  data: 0.0178\n",
      "Epoch: [11]  [ 9/40]  eta: 0:05:57  lr: 1.0000000000000009e-15  img/s: 11.203841792431335  loss: 0.7999 (0.8185)  acc1: 74.2188 (75.1562)  acc5: 97.6562 (98.0469)  time: 11.5447  data: 0.0178\n",
      "Epoch: [11]  [10/40]  eta: 0:05:45  lr: 1.0000000000000009e-15  img/s: 11.230444407679313  loss: 0.7999 (0.8090)  acc1: 74.2188 (75.4261)  acc5: 97.6562 (98.0114)  time: 11.5330  data: 0.0178\n",
      "Epoch: [11]  [11/40]  eta: 0:05:33  lr: 1.0000000000000009e-15  img/s: 11.405847435582682  loss: 0.7825 (0.7977)  acc1: 74.2188 (76.1719)  acc5: 97.6562 (97.8516)  time: 11.5086  data: 0.0178\n",
      "Epoch: [11]  [12/40]  eta: 0:05:22  lr: 1.0000000000000009e-15  img/s: 11.05979101939573  loss: 0.7825 (0.7874)  acc1: 76.5625 (76.5024)  acc5: 97.6562 (97.9567)  time: 11.5149  data: 0.0178\n",
      "Epoch: [11]  [13/40]  eta: 0:05:10  lr: 1.0000000000000009e-15  img/s: 11.103370910839072  loss: 0.7685 (0.7860)  acc1: 76.5625 (76.6183)  acc5: 97.6562 (97.9353)  time: 11.5171  data: 0.0178\n",
      "Epoch: [11]  [14/40]  eta: 0:04:59  lr: 1.0000000000000009e-15  img/s: 11.223572912491004  loss: 0.7825 (0.7897)  acc1: 76.5625 (76.5625)  acc5: 97.6562 (97.8646)  time: 11.5108  data: 0.0178\n",
      "Epoch: [11]  [15/40]  eta: 0:04:47  lr: 1.0000000000000009e-15  img/s: 11.1810750229137  loss: 0.7825 (0.7918)  acc1: 76.5625 (76.5625)  acc5: 97.6562 (97.9980)  time: 11.5080  data: 0.0178\n",
      "Epoch: [11]  [16/40]  eta: 0:04:36  lr: 1.0000000000000009e-15  img/s: 10.962697284109538  loss: 0.7825 (0.7892)  acc1: 76.5625 (76.6085)  acc5: 97.6562 (97.9779)  time: 11.5189  data: 0.0178\n",
      "Epoch: [11]  [17/40]  eta: 0:04:25  lr: 1.0000000000000009e-15  img/s: 11.028662044402374  loss: 0.7685 (0.7831)  acc1: 76.5625 (76.7361)  acc5: 97.6562 (98.0035)  time: 11.5248  data: 0.0178\n",
      "Epoch: [11]  [18/40]  eta: 0:04:13  lr: 1.0000000000000009e-15  img/s: 11.054198403653498  loss: 0.7685 (0.7795)  acc1: 77.3438 (76.9326)  acc5: 97.6562 (97.9852)  time: 11.5286  data: 0.0178\n",
      "Epoch: [11]  [19/40]  eta: 0:04:02  lr: 1.0000000000000009e-15  img/s: 11.011177699518317  loss: 0.7649 (0.7747)  acc1: 77.3438 (77.1875)  acc5: 97.6562 (97.9688)  time: 11.5343  data: 0.0178\n",
      "Epoch: [11]  [20/40]  eta: 0:03:50  lr: 1.0000000000000009e-15  img/s: 11.097991541255738  loss: 0.7478 (0.7702)  acc1: 77.3438 (77.4182)  acc5: 97.6562 (97.9167)  time: 11.5453  data: 0.0178\n",
      "Epoch: [11]  [21/40]  eta: 0:03:39  lr: 1.0000000000000009e-15  img/s: 10.895773489416726  loss: 0.7478 (0.7715)  acc1: 77.3438 (77.5213)  acc5: 97.6562 (97.8338)  time: 11.5406  data: 0.0178\n",
      "Epoch: [11]  [22/40]  eta: 0:03:27  lr: 1.0000000000000009e-15  img/s: 11.036137796184443  loss: 0.7478 (0.7725)  acc1: 77.3438 (77.5136)  acc5: 97.6562 (97.7582)  time: 11.5417  data: 0.0178\n",
      "Epoch: [11]  [23/40]  eta: 0:03:16  lr: 1.0000000000000009e-15  img/s: 10.997576243768002  loss: 0.7421 (0.7699)  acc1: 78.1250 (77.5716)  acc5: 97.6562 (97.7539)  time: 11.5449  data: 0.0178\n",
      "Epoch: [11]  [24/40]  eta: 0:03:04  lr: 1.0000000000000009e-15  img/s: 11.134673320455901  loss: 0.7478 (0.7702)  acc1: 78.1250 (77.5000)  acc5: 97.6562 (97.7188)  time: 11.5437  data: 0.0178\n",
      "Epoch: [11]  [25/40]  eta: 0:02:53  lr: 1.0000000000000009e-15  img/s: 11.124137304280517  loss: 0.7478 (0.7696)  acc1: 78.1250 (77.6142)  acc5: 97.6562 (97.6562)  time: 11.5415  data: 0.0178\n",
      "Epoch: [11]  [26/40]  eta: 0:02:41  lr: 1.0000000000000009e-15  img/s: 11.104911981719857  loss: 0.7485 (0.7689)  acc1: 78.1250 (77.6331)  acc5: 97.6562 (97.6852)  time: 11.5378  data: 0.0179\n",
      "Epoch: [11]  [27/40]  eta: 0:02:30  lr: 1.0000000000000009e-15  img/s: 11.126814003144277  loss: 0.7485 (0.7682)  acc1: 78.1250 (77.5670)  acc5: 97.6562 (97.7121)  time: 11.5419  data: 0.0179\n",
      "Epoch: [11]  [28/40]  eta: 0:02:18  lr: 1.0000000000000009e-15  img/s: 11.116789809022709  loss: 0.7478 (0.7669)  acc1: 78.1250 (77.6401)  acc5: 97.6562 (97.7640)  time: 11.5453  data: 0.0179\n",
      "Epoch: [11]  [29/40]  eta: 0:02:07  lr: 1.0000000000000009e-15  img/s: 11.070938453403228  loss: 0.7478 (0.7715)  acc1: 78.1250 (77.4740)  acc5: 97.6562 (97.8125)  time: 11.5522  data: 0.0179\n",
      "Epoch: [11]  [30/40]  eta: 0:01:55  lr: 1.0000000000000009e-15  img/s: 10.987997995978573  loss: 0.7478 (0.7655)  acc1: 78.9062 (77.6462)  acc5: 97.6562 (97.8831)  time: 11.5648  data: 0.0179\n",
      "Epoch: [11]  [31/40]  eta: 0:01:44  lr: 1.0000000000000009e-15  img/s: 10.742047417914277  loss: 0.7485 (0.7672)  acc1: 78.1250 (77.6611)  acc5: 97.6562 (97.8760)  time: 11.5994  data: 0.0179\n",
      "Epoch: [11]  [32/40]  eta: 0:01:32  lr: 1.0000000000000009e-15  img/s: 10.868158491478601  loss: 0.7509 (0.7670)  acc1: 78.1250 (77.6752)  acc5: 97.6562 (97.8930)  time: 11.6096  data: 0.0179\n",
      "Epoch: [11]  [33/40]  eta: 0:01:21  lr: 1.0000000000000009e-15  img/s: 10.957749657146113  loss: 0.7509 (0.7671)  acc1: 78.1250 (77.6425)  acc5: 97.6562 (97.8860)  time: 11.6173  data: 0.0179\n",
      "Epoch: [11]  [34/40]  eta: 0:01:09  lr: 1.0000000000000009e-15  img/s: 10.846897714132373  loss: 0.7485 (0.7665)  acc1: 78.1250 (77.7232)  acc5: 97.6562 (97.9241)  time: 11.6371  data: 0.0179\n",
      "Epoch: [11]  [35/40]  eta: 0:00:57  lr: 1.0000000000000009e-15  img/s: 10.861788982385002  loss: 0.7485 (0.7706)  acc1: 78.1250 (77.5391)  acc5: 97.6562 (97.8733)  time: 11.6539  data: 0.0179\n",
      "Epoch: [11]  [36/40]  eta: 0:00:46  lr: 1.0000000000000009e-15  img/s: 10.82398631777228  loss: 0.7485 (0.7697)  acc1: 78.1250 (77.5549)  acc5: 97.6562 (97.8463)  time: 11.6614  data: 0.0178\n",
      "Epoch: [11]  [37/40]  eta: 0:00:34  lr: 1.0000000000000009e-15  img/s: 10.872462266977944  loss: 0.7485 (0.7668)  acc1: 78.1250 (77.6933)  acc5: 97.6562 (97.8207)  time: 11.6697  data: 0.0178\n",
      "Epoch: [11]  [38/40]  eta: 0:00:23  lr: 1.0000000000000009e-15  img/s: 11.034645463981626  loss: 0.7485 (0.7632)  acc1: 78.1250 (77.8045)  acc5: 97.6562 (97.8566)  time: 11.6707  data: 0.0178\n",
      "Epoch: [11]  [39/40]  eta: 0:00:11  lr: 1.0000000000000009e-15  img/s: 11.005404235291032  loss: 0.7485 (0.7613)  acc1: 78.1250 (77.8000)  acc5: 97.6562 (97.8600)  time: 11.1251  data: 0.0170\n",
      "Epoch: [11] Total time: 0:07:33\n",
      "Epoch 13/15\n",
      "Epoch: [12]  [ 0/40]  eta: 0:07:29  lr: 1.000000000000001e-16  img/s: 11.399930199514047  loss: 0.8901 (0.8901)  acc1: 73.4375 (73.4375)  acc5: 97.6562 (97.6562)  time: 11.2462  data: 0.0181\n",
      "Epoch: [12]  [ 1/40]  eta: 0:07:28  lr: 1.000000000000001e-16  img/s: 10.929578815866927  loss: 0.7959 (0.8430)  acc1: 73.4375 (74.2188)  acc5: 97.6562 (98.0469)  time: 11.4877  data: 0.0179\n",
      "Epoch: [12]  [ 2/40]  eta: 0:07:16  lr: 1.000000000000001e-16  img/s: 11.196577102891691  loss: 0.7959 (0.8268)  acc1: 75.0000 (74.7396)  acc5: 98.4375 (98.1771)  time: 11.4751  data: 0.0179\n",
      "Epoch: [12]  [ 3/40]  eta: 0:07:06  lr: 1.000000000000001e-16  img/s: 11.009302883070008  loss: 0.7959 (0.8478)  acc1: 75.0000 (74.8047)  acc5: 97.6562 (98.0469)  time: 11.5173  data: 0.0178\n",
      "Epoch: [12]  [ 4/40]  eta: 0:06:55  lr: 1.000000000000001e-16  img/s: 11.0540142730725  loss: 0.7959 (0.8340)  acc1: 75.0000 (75.9375)  acc5: 98.4375 (98.2812)  time: 11.5333  data: 0.0178\n",
      "Epoch: [12]  [ 5/40]  eta: 0:06:43  lr: 1.000000000000001e-16  img/s: 11.156465574038526  loss: 0.7945 (0.8223)  acc1: 75.0000 (76.0417)  acc5: 98.4375 (98.3073)  time: 11.5262  data: 0.0178\n",
      "Epoch: [12]  [ 6/40]  eta: 0:06:31  lr: 1.000000000000001e-16  img/s: 11.1333109853203  loss: 0.7945 (0.7914)  acc1: 75.7812 (77.0089)  acc5: 98.4375 (98.4375)  time: 11.5246  data: 0.0178\n",
      "Epoch: [12]  [ 7/40]  eta: 0:06:19  lr: 1.000000000000001e-16  img/s: 11.2151321784025  loss: 0.7945 (0.7982)  acc1: 75.0000 (76.6602)  acc5: 98.4375 (98.3398)  time: 11.5129  data: 0.0178\n",
      "Epoch: [12]  [ 8/40]  eta: 0:06:08  lr: 1.000000000000001e-16  img/s: 11.144576886676283  loss: 0.7959 (0.8096)  acc1: 75.0000 (76.2153)  acc5: 98.4375 (98.2639)  time: 11.5118  data: 0.0178\n",
      "Epoch: [12]  [ 9/40]  eta: 0:05:56  lr: 1.000000000000001e-16  img/s: 11.177593667022325  loss: 0.7959 (0.8250)  acc1: 75.0000 (75.4688)  acc5: 97.6562 (98.1250)  time: 11.5076  data: 0.0178\n",
      "Epoch: [12]  [10/40]  eta: 0:05:45  lr: 1.000000000000001e-16  img/s: 11.137365951769045  loss: 0.7959 (0.8109)  acc1: 75.0000 (76.2074)  acc5: 98.4375 (98.2244)  time: 11.5079  data: 0.0178\n",
      "Epoch: [12]  [11/40]  eta: 0:05:33  lr: 1.000000000000001e-16  img/s: 11.134419300484568  loss: 0.7945 (0.8025)  acc1: 75.0000 (76.8880)  acc5: 98.4375 (98.2422)  time: 11.5084  data: 0.0178\n",
      "Epoch: [12]  [12/40]  eta: 0:05:22  lr: 1.000000000000001e-16  img/s: 11.16062187884791  loss: 0.7945 (0.7999)  acc1: 75.7812 (77.0433)  acc5: 98.4375 (98.3173)  time: 11.5067  data: 0.0178\n",
      "Epoch: [12]  [13/40]  eta: 0:05:10  lr: 1.000000000000001e-16  img/s: 11.014941678273605  loss: 0.7945 (0.8004)  acc1: 75.7812 (76.9531)  acc5: 98.4375 (98.1027)  time: 11.5161  data: 0.0178\n",
      "Epoch: [12]  [14/40]  eta: 0:04:59  lr: 1.000000000000001e-16  img/s: 10.934884763004236  loss: 0.7959 (0.8026)  acc1: 75.7812 (76.9271)  acc5: 98.4375 (98.0729)  time: 11.5299  data: 0.0178\n",
      "Epoch: [12]  [15/40]  eta: 0:04:48  lr: 1.000000000000001e-16  img/s: 10.909483707990802  loss: 0.7945 (0.7948)  acc1: 75.7812 (77.0508)  acc5: 98.4375 (98.0957)  time: 11.5437  data: 0.0178\n",
      "Epoch: [12]  [16/40]  eta: 0:04:37  lr: 1.000000000000001e-16  img/s: 10.891720813090217  loss: 0.7945 (0.7844)  acc1: 76.5625 (77.2978)  acc5: 98.4375 (98.0699)  time: 11.5570  data: 0.0178\n",
      "Epoch: [12]  [17/40]  eta: 0:04:26  lr: 1.000000000000001e-16  img/s: 10.871786123674047  loss: 0.7784 (0.7830)  acc1: 76.5625 (77.3438)  acc5: 98.4375 (98.1337)  time: 11.5700  data: 0.0178\n",
      "Epoch: [12]  [18/40]  eta: 0:04:14  lr: 1.000000000000001e-16  img/s: 10.994220809909022  loss: 0.7784 (0.7814)  acc1: 76.5625 (77.4671)  acc5: 98.4375 (98.1086)  time: 11.5748  data: 0.0178\n",
      "Epoch: [12]  [19/40]  eta: 0:04:03  lr: 1.000000000000001e-16  img/s: 10.934973183422596  loss: 0.7741 (0.7810)  acc1: 76.5625 (77.5391)  acc5: 98.4375 (98.1641)  time: 11.5822  data: 0.0178\n",
      "Epoch: [12]  [20/40]  eta: 0:03:51  lr: 1.000000000000001e-16  img/s: 11.026653310733323  loss: 0.7685 (0.7804)  acc1: 78.1250 (77.5670)  acc5: 98.4375 (98.1399)  time: 11.6012  data: 0.0178\n",
      "Epoch: [12]  [21/40]  eta: 0:03:40  lr: 1.000000000000001e-16  img/s: 11.049156261995307  loss: 0.7679 (0.7786)  acc1: 78.1250 (77.6989)  acc5: 97.6562 (98.0824)  time: 11.5949  data: 0.0178\n",
      "Epoch: [12]  [22/40]  eta: 0:03:28  lr: 1.000000000000001e-16  img/s: 11.05922100097738  loss: 0.7642 (0.7754)  acc1: 78.9062 (77.7514)  acc5: 97.6562 (98.0978)  time: 11.6020  data: 0.0178\n",
      "Epoch: [12]  [23/40]  eta: 0:03:17  lr: 1.000000000000001e-16  img/s: 10.870323599267023  loss: 0.7583 (0.7716)  acc1: 78.9062 (77.8320)  acc5: 98.4375 (98.1445)  time: 11.6095  data: 0.0178\n",
      "Epoch: [12]  [24/40]  eta: 0:03:05  lr: 1.000000000000001e-16  img/s: 10.810387055377351  loss: 0.7583 (0.7783)  acc1: 78.9062 (77.6250)  acc5: 97.6562 (98.1250)  time: 11.6225  data: 0.0178\n",
      "Epoch: [12]  [25/40]  eta: 0:02:54  lr: 1.000000000000001e-16  img/s: 11.057362356338926  loss: 0.7524 (0.7750)  acc1: 78.9062 (77.7043)  acc5: 97.6562 (98.1070)  time: 11.6276  data: 0.0178\n",
      "Epoch: [12]  [26/40]  eta: 0:02:42  lr: 1.000000000000001e-16  img/s: 10.937591699480711  loss: 0.7524 (0.7689)  acc1: 78.9062 (77.8646)  acc5: 97.6562 (98.1192)  time: 11.6379  data: 0.0178\n",
      "Epoch: [12]  [27/40]  eta: 0:02:30  lr: 1.000000000000001e-16  img/s: 11.05960806943165  loss: 0.7449 (0.7680)  acc1: 78.9062 (77.9297)  acc5: 97.6562 (98.1027)  time: 11.6459  data: 0.0178\n",
      "Epoch: [12]  [28/40]  eta: 0:02:19  lr: 1.000000000000001e-16  img/s: 10.826706303418417  loss: 0.7394 (0.7616)  acc1: 78.9062 (78.0981)  acc5: 97.6562 (98.1142)  time: 11.6628  data: 0.0178\n",
      "Epoch: [12]  [29/40]  eta: 0:02:07  lr: 1.000000000000001e-16  img/s: 11.044780339530293  loss: 0.7394 (0.7673)  acc1: 78.9062 (77.9427)  acc5: 97.6562 (98.0469)  time: 11.6697  data: 0.0178\n",
      "Epoch: [12]  [30/40]  eta: 0:01:56  lr: 1.000000000000001e-16  img/s: 11.108821007995246  loss: 0.7394 (0.7621)  acc1: 78.9062 (78.1250)  acc5: 97.6562 (98.0847)  time: 11.6711  data: 0.0178\n",
      "Epoch: [12]  [31/40]  eta: 0:01:44  lr: 1.000000000000001e-16  img/s: 11.1316627746587  loss: 0.7449 (0.7663)  acc1: 78.9062 (77.9297)  acc5: 97.6562 (98.0225)  time: 11.6713  data: 0.0178\n",
      "Epoch: [12]  [32/40]  eta: 0:01:32  lr: 1.000000000000001e-16  img/s: 11.088744025845259  loss: 0.7394 (0.7639)  acc1: 78.9062 (77.9593)  acc5: 97.6562 (97.9403)  time: 11.6750  data: 0.0178\n",
      "Epoch: [12]  [33/40]  eta: 0:01:21  lr: 1.000000000000001e-16  img/s: 11.026623189828026  loss: 0.7394 (0.7647)  acc1: 78.9062 (77.9642)  acc5: 97.6562 (97.8860)  time: 11.6744  data: 0.0178\n",
      "Epoch: [12]  [34/40]  eta: 0:01:09  lr: 1.000000000000001e-16  img/s: 10.98134981867314  loss: 0.7135 (0.7632)  acc1: 78.9062 (78.0804)  acc5: 97.6562 (97.9241)  time: 11.6719  data: 0.0178\n",
      "Epoch: [12]  [35/40]  eta: 0:00:58  lr: 1.000000000000001e-16  img/s: 11.09182947771346  loss: 0.7394 (0.7649)  acc1: 78.9062 (78.0382)  acc5: 97.6562 (97.8950)  time: 11.6623  data: 0.0178\n",
      "Epoch: [12]  [36/40]  eta: 0:00:46  lr: 1.000000000000001e-16  img/s: 10.973154967084065  loss: 0.7414 (0.7643)  acc1: 78.9062 (77.9983)  acc5: 97.6562 (97.9307)  time: 11.6579  data: 0.0178\n",
      "Epoch: [12]  [37/40]  eta: 0:00:34  lr: 1.000000000000001e-16  img/s: 11.11007458370143  loss: 0.7394 (0.7629)  acc1: 78.9062 (78.1250)  acc5: 97.6562 (97.9235)  time: 11.6453  data: 0.0178\n",
      "Epoch: [12]  [38/40]  eta: 0:00:23  lr: 1.000000000000001e-16  img/s: 10.904134607166661  loss: 0.7294 (0.7620)  acc1: 78.9062 (78.1250)  acc5: 97.6562 (97.9367)  time: 11.6501  data: 0.0178\n",
      "Epoch: [12]  [39/40]  eta: 0:00:11  lr: 1.000000000000001e-16  img/s: 11.382355235904196  loss: 0.7294 (0.7656)  acc1: 78.9062 (78.1200)  acc5: 97.6562 (97.9400)  time: 11.0992  data: 0.0170\n",
      "Epoch: [12] Total time: 0:07:33\n",
      "Epoch 14/15\n",
      "Epoch: [13]  [ 0/40]  eta: 0:07:29  lr: 1.000000000000001e-17  img/s: 11.407184459364606  loss: 0.9215 (0.9215)  acc1: 71.8750 (71.8750)  acc5: 97.6562 (97.6562)  time: 11.2390  data: 0.0180\n",
      "Epoch: [13]  [ 1/40]  eta: 0:07:28  lr: 1.000000000000001e-17  img/s: 10.9093411656717  loss: 0.7007 (0.8111)  acc1: 71.8750 (75.7812)  acc5: 96.8750 (97.2656)  time: 11.4949  data: 0.0179\n",
      "Epoch: [13]  [ 2/40]  eta: 0:07:19  lr: 1.000000000000001e-17  img/s: 10.985111186131565  loss: 0.9175 (0.8466)  acc1: 71.8750 (74.2188)  acc5: 97.6562 (97.6562)  time: 11.5533  data: 0.0179\n",
      "Epoch: [13]  [ 3/40]  eta: 0:07:06  lr: 1.000000000000001e-17  img/s: 11.146997033749823  loss: 0.7977 (0.8344)  acc1: 71.8750 (75.5859)  acc5: 97.6562 (97.8516)  time: 11.5401  data: 0.0178\n",
      "Epoch: [13]  [ 4/40]  eta: 0:06:55  lr: 1.000000000000001e-17  img/s: 11.078236645358736  loss: 0.7977 (0.8168)  acc1: 79.6875 (77.0312)  acc5: 98.4375 (98.1250)  time: 11.5464  data: 0.0178\n",
      "Epoch: [13]  [ 5/40]  eta: 0:06:43  lr: 1.000000000000001e-17  img/s: 11.150424383970059  loss: 0.7637 (0.8080)  acc1: 78.9062 (77.3438)  acc5: 98.4375 (98.3073)  time: 11.5382  data: 0.0178\n",
      "Epoch: [13]  [ 6/40]  eta: 0:06:33  lr: 1.000000000000001e-17  img/s: 10.948072958289416  loss: 0.7637 (0.7756)  acc1: 79.6875 (78.2366)  acc5: 98.4375 (98.4375)  time: 11.5627  data: 0.0178\n",
      "Epoch: [13]  [ 7/40]  eta: 0:06:21  lr: 1.000000000000001e-17  img/s: 11.042839777423197  loss: 0.7637 (0.7920)  acc1: 78.9062 (77.3438)  acc5: 98.4375 (98.5352)  time: 11.5685  data: 0.0178\n",
      "Epoch: [13]  [ 8/40]  eta: 0:06:10  lr: 1.000000000000001e-17  img/s: 11.021371533627804  loss: 0.7977 (0.7992)  acc1: 78.9062 (77.0833)  acc5: 98.4375 (98.1771)  time: 11.5755  data: 0.0178\n",
      "Epoch: [13]  [ 9/40]  eta: 0:05:58  lr: 1.000000000000001e-17  img/s: 11.043797478405418  loss: 0.7977 (0.8143)  acc1: 75.0000 (76.6406)  acc5: 98.4375 (98.0469)  time: 11.5788  data: 0.0178\n",
      "Epoch: [13]  [10/40]  eta: 0:05:47  lr: 1.000000000000001e-17  img/s: 11.041598148251019  loss: 0.7977 (0.8046)  acc1: 78.9062 (77.0597)  acc5: 98.4375 (98.0824)  time: 11.5817  data: 0.0178\n",
      "Epoch: [13]  [11/40]  eta: 0:05:35  lr: 1.000000000000001e-17  img/s: 11.024213824086438  loss: 0.7637 (0.8005)  acc1: 77.3438 (77.0833)  acc5: 98.4375 (97.8516)  time: 11.5856  data: 0.0179\n",
      "Epoch: [13]  [12/40]  eta: 0:05:24  lr: 1.000000000000001e-17  img/s: 11.067579635127338  loss: 0.7682 (0.7980)  acc1: 77.3438 (77.1034)  acc5: 98.4375 (97.8966)  time: 11.5854  data: 0.0179\n",
      "Epoch: [13]  [13/40]  eta: 0:05:12  lr: 1.000000000000001e-17  img/s: 11.078274364068163  loss: 0.7637 (0.7915)  acc1: 77.3438 (77.5112)  acc5: 98.4375 (97.8237)  time: 11.5845  data: 0.0179\n",
      "Epoch: [13]  [14/40]  eta: 0:05:01  lr: 1.000000000000001e-17  img/s: 10.954826633893589  loss: 0.7682 (0.7957)  acc1: 77.3438 (77.1875)  acc5: 98.4375 (97.7083)  time: 11.5923  data: 0.0179\n",
      "Epoch: [13]  [15/40]  eta: 0:04:50  lr: 1.000000000000001e-17  img/s: 10.92043934317189  loss: 0.7637 (0.7870)  acc1: 77.3438 (77.3438)  acc5: 98.4375 (97.8027)  time: 11.6015  data: 0.0179\n",
      "Epoch: [13]  [16/40]  eta: 0:04:38  lr: 1.000000000000001e-17  img/s: 11.168524398639942  loss: 0.7682 (0.7864)  acc1: 78.1250 (77.3897)  acc5: 98.4375 (97.8860)  time: 11.5943  data: 0.0179\n",
      "Epoch: [13]  [17/40]  eta: 0:04:26  lr: 1.000000000000001e-17  img/s: 10.856790606945676  loss: 0.7637 (0.7776)  acc1: 78.1250 (77.7344)  acc5: 98.4375 (97.8733)  time: 11.6062  data: 0.0179\n",
      "Epoch: [13]  [18/40]  eta: 0:04:15  lr: 1.000000000000001e-17  img/s: 11.127403695538652  loss: 0.7682 (0.7814)  acc1: 78.1250 (77.6316)  acc5: 98.4375 (97.8207)  time: 11.6017  data: 0.0179\n",
      "Epoch: [13]  [19/40]  eta: 0:04:03  lr: 1.000000000000001e-17  img/s: 11.056780064012937  loss: 0.7637 (0.7744)  acc1: 78.1250 (77.7734)  acc5: 97.6562 (97.7734)  time: 11.6013  data: 0.0179\n",
      "Epoch: [13]  [20/40]  eta: 0:03:52  lr: 1.000000000000001e-17  img/s: 10.9107139830499  loss: 0.7550 (0.7678)  acc1: 78.9062 (78.0134)  acc5: 98.4375 (97.8051)  time: 11.6269  data: 0.0179\n",
      "Epoch: [13]  [21/40]  eta: 0:03:40  lr: 1.000000000000001e-17  img/s: 11.032301963046505  loss: 0.7550 (0.7672)  acc1: 78.9062 (78.0895)  acc5: 98.4375 (97.8338)  time: 11.6203  data: 0.0179\n",
      "Epoch: [13]  [22/40]  eta: 0:03:29  lr: 1.000000000000001e-17  img/s: 10.986125668442414  loss: 0.7530 (0.7659)  acc1: 78.9062 (78.1250)  acc5: 98.4375 (97.7921)  time: 11.6203  data: 0.0179\n",
      "Epoch: [13]  [23/40]  eta: 0:03:17  lr: 1.000000000000001e-17  img/s: 10.841814286601975  loss: 0.7466 (0.7623)  acc1: 78.9062 (78.1576)  acc5: 98.4375 (97.8190)  time: 11.6365  data: 0.0179\n",
      "Epoch: [13]  [24/40]  eta: 0:03:05  lr: 1.000000000000001e-17  img/s: 10.974090075690166  loss: 0.7530 (0.7661)  acc1: 78.9062 (78.0312)  acc5: 98.4375 (97.8438)  time: 11.6420  data: 0.0180\n",
      "Epoch: [13]  [25/40]  eta: 0:02:54  lr: 1.000000000000001e-17  img/s: 11.061085284145916  loss: 0.7390 (0.7622)  acc1: 78.9062 (78.1851)  acc5: 97.6562 (97.8365)  time: 11.6466  data: 0.0180\n",
      "Epoch: [13]  [26/40]  eta: 0:02:42  lr: 1.000000000000001e-17  img/s: 10.932576315767342  loss: 0.7390 (0.7590)  acc1: 78.9062 (78.3275)  acc5: 97.6562 (97.8299)  time: 11.6474  data: 0.0180\n",
      "Epoch: [13]  [27/40]  eta: 0:02:31  lr: 1.000000000000001e-17  img/s: 11.178015364826132  loss: 0.7179 (0.7576)  acc1: 78.9062 (78.3203)  acc5: 97.6562 (97.8237)  time: 11.6404  data: 0.0180\n",
      "Epoch: [13]  [28/40]  eta: 0:02:19  lr: 1.000000000000001e-17  img/s: 11.073235591385822  loss: 0.7076 (0.7518)  acc1: 78.9062 (78.5022)  acc5: 97.6562 (97.8718)  time: 11.6377  data: 0.0180\n",
      "Epoch: [13]  [29/40]  eta: 0:02:07  lr: 1.000000000000001e-17  img/s: 11.088550726795878  loss: 0.7076 (0.7601)  acc1: 78.9062 (78.1771)  acc5: 97.6562 (97.8385)  time: 11.6354  data: 0.0180\n",
      "Epoch: [13]  [30/40]  eta: 0:01:56  lr: 1.000000000000001e-17  img/s: 11.105128822440303  loss: 0.7076 (0.7543)  acc1: 78.9062 (78.3518)  acc5: 97.6562 (97.8579)  time: 11.6320  data: 0.0180\n",
      "Epoch: [13]  [31/40]  eta: 0:01:44  lr: 1.000000000000001e-17  img/s: 10.956820238084257  loss: 0.7076 (0.7575)  acc1: 78.9062 (78.2471)  acc5: 97.6562 (97.8516)  time: 11.6356  data: 0.0180\n",
      "Epoch: [13]  [32/40]  eta: 0:01:32  lr: 1.000000000000001e-17  img/s: 11.146072954532894  loss: 0.7076 (0.7576)  acc1: 78.9062 (78.2434)  acc5: 97.6562 (97.8456)  time: 11.6315  data: 0.0180\n",
      "Epoch: [13]  [33/40]  eta: 0:01:21  lr: 1.000000000000001e-17  img/s: 11.14217143361878  loss: 0.7179 (0.7594)  acc1: 78.9062 (78.0790)  acc5: 97.6562 (97.7711)  time: 11.6282  data: 0.0180\n",
      "Epoch: [13]  [34/40]  eta: 0:01:09  lr: 1.000000000000001e-17  img/s: 11.07459993960107  loss: 0.7179 (0.7590)  acc1: 78.9062 (78.1250)  acc5: 97.6562 (97.8125)  time: 11.6219  data: 0.0180\n",
      "Epoch: [13]  [35/40]  eta: 0:00:58  lr: 1.000000000000001e-17  img/s: 11.088460263323583  loss: 0.7390 (0.7591)  acc1: 78.9062 (78.1250)  acc5: 97.6562 (97.7865)  time: 11.6130  data: 0.0179\n",
      "Epoch: [13]  [36/40]  eta: 0:00:46  lr: 1.000000000000001e-17  img/s: 11.06980005649636  loss: 0.7390 (0.7613)  acc1: 78.9062 (77.9983)  acc5: 97.6562 (97.8041)  time: 11.6181  data: 0.0179\n",
      "Epoch: [13]  [37/40]  eta: 0:00:34  lr: 1.000000000000001e-17  img/s: 10.999913352292838  loss: 0.7390 (0.7587)  acc1: 78.9062 (78.0222)  acc5: 97.6562 (97.7796)  time: 11.6104  data: 0.0179\n",
      "Epoch: [13]  [38/40]  eta: 0:00:23  lr: 1.000000000000001e-17  img/s: 11.098949425141882  loss: 0.7179 (0.7575)  acc1: 78.9062 (77.9848)  acc5: 97.6562 (97.8165)  time: 11.6119  data: 0.0179\n",
      "Epoch: [13]  [39/40]  eta: 0:00:11  lr: 1.000000000000001e-17  img/s: 10.322712752080882  loss: 0.7179 (0.7546)  acc1: 78.1250 (77.9800)  acc5: 97.6562 (97.8200)  time: 11.0710  data: 0.0171\n",
      "Epoch: [13] Total time: 0:07:33\n",
      "Epoch 15/15\n",
      "Epoch: [14]  [ 0/40]  eta: 0:07:30  lr: 1.000000000000001e-18  img/s: 11.375519111663879  loss: 1.1747 (1.1747)  acc1: 65.6250 (65.6250)  acc5: 96.0938 (96.0938)  time: 11.2697  data: 0.0174\n",
      "Epoch: [14]  [ 1/40]  eta: 0:07:27  lr: 1.000000000000001e-18  img/s: 10.962418145208229  loss: 0.7528 (0.9638)  acc1: 65.6250 (73.0469)  acc5: 96.0938 (97.2656)  time: 11.4819  data: 0.0177\n",
      "Epoch: [14]  [ 2/40]  eta: 0:07:15  lr: 1.000000000000001e-18  img/s: 11.21554406128059  loss: 0.8115 (0.9130)  acc1: 75.7812 (73.9583)  acc5: 98.4375 (97.6562)  time: 11.4648  data: 0.0177\n",
      "Epoch: [14]  [ 3/40]  eta: 0:07:06  lr: 1.000000000000001e-18  img/s: 10.969904299409734  loss: 0.8115 (0.9081)  acc1: 70.3125 (73.0469)  acc5: 96.8750 (97.4609)  time: 11.5201  data: 0.0177\n",
      "Epoch: [14]  [ 4/40]  eta: 0:06:54  lr: 1.000000000000001e-18  img/s: 11.120115686952591  loss: 0.8115 (0.8582)  acc1: 75.7812 (75.1562)  acc5: 98.4375 (97.6562)  time: 11.5218  data: 0.0177\n",
      "Epoch: [14]  [ 5/40]  eta: 0:06:43  lr: 1.000000000000001e-18  img/s: 11.100279264993922  loss: 0.8115 (0.8528)  acc1: 72.6562 (74.7396)  acc5: 98.4375 (97.9167)  time: 11.5274  data: 0.0188\n",
      "Epoch: [14]  [ 6/40]  eta: 0:06:32  lr: 1.000000000000001e-18  img/s: 10.96248798456225  loss: 0.8115 (0.8167)  acc1: 75.7812 (75.6696)  acc5: 98.4375 (97.8795)  time: 11.5512  data: 0.0187\n",
      "Epoch: [14]  [ 7/40]  eta: 0:06:20  lr: 1.000000000000001e-18  img/s: 11.1682953172934  loss: 0.8115 (0.8180)  acc1: 75.7812 (75.8789)  acc5: 98.4375 (97.9492)  time: 11.5422  data: 0.0186\n",
      "Epoch: [14]  [ 8/40]  eta: 0:06:09  lr: 1.000000000000001e-18  img/s: 11.182031698830514  loss: 0.8179 (0.8180)  acc1: 75.7812 (75.8681)  acc5: 98.4375 (97.8299)  time: 11.5335  data: 0.0185\n",
      "Epoch: [14]  [ 9/40]  eta: 0:05:57  lr: 1.000000000000001e-18  img/s: 11.001493694034117  loss: 0.8179 (0.8330)  acc1: 75.7812 (75.0000)  acc5: 97.6562 (97.8125)  time: 11.5455  data: 0.0184\n",
      "Epoch: [14]  [10/40]  eta: 0:05:46  lr: 1.000000000000001e-18  img/s: 11.010708653271841  loss: 0.8179 (0.8230)  acc1: 75.7812 (75.4972)  acc5: 98.4375 (97.9403)  time: 11.5543  data: 0.0183\n",
      "Epoch: [14]  [11/40]  eta: 0:05:35  lr: 1.000000000000001e-18  img/s: 11.090845786334818  loss: 0.8115 (0.8130)  acc1: 75.7812 (75.7812)  acc5: 97.6562 (97.7214)  time: 11.5547  data: 0.0183\n",
      "Epoch: [14]  [12/40]  eta: 0:05:23  lr: 1.000000000000001e-18  img/s: 11.029797207352214  loss: 0.8115 (0.8097)  acc1: 77.3438 (75.9014)  acc5: 97.6562 (97.7163)  time: 11.5599  data: 0.0183\n",
      "Epoch: [14]  [13/40]  eta: 0:05:12  lr: 1.000000000000001e-18  img/s: 11.110751489664331  loss: 0.7707 (0.8019)  acc1: 76.5625 (75.9487)  acc5: 97.6562 (97.7121)  time: 11.5584  data: 0.0183\n",
      "Epoch: [14]  [14/40]  eta: 0:05:00  lr: 1.000000000000001e-18  img/s: 10.963853388367548  loss: 0.8115 (0.8074)  acc1: 76.5625 (75.8333)  acc5: 97.6562 (97.7604)  time: 11.5673  data: 0.0182\n",
      "Epoch: [14]  [15/40]  eta: 0:04:49  lr: 1.000000000000001e-18  img/s: 11.111884755998641  loss: 0.7707 (0.7919)  acc1: 76.5625 (76.3672)  acc5: 97.6562 (97.8516)  time: 11.5654  data: 0.0182\n",
      "Epoch: [14]  [16/40]  eta: 0:04:37  lr: 1.000000000000001e-18  img/s: 10.871436305918664  loss: 0.7707 (0.7898)  acc1: 77.3438 (76.6544)  acc5: 97.6562 (97.7941)  time: 11.5788  data: 0.0182\n",
      "Epoch: [14]  [17/40]  eta: 0:04:26  lr: 1.000000000000001e-18  img/s: 10.968410329112633  loss: 0.7554 (0.7847)  acc1: 77.3438 (76.9531)  acc5: 97.6562 (97.8299)  time: 11.5848  data: 0.0182\n",
      "Epoch: [14]  [18/40]  eta: 0:04:14  lr: 1.000000000000001e-18  img/s: 11.121048597907588  loss: 0.7707 (0.7874)  acc1: 77.3438 (77.0148)  acc5: 97.6562 (97.8207)  time: 11.5818  data: 0.0182\n",
      "Epoch: [14]  [19/40]  eta: 0:04:03  lr: 1.000000000000001e-18  img/s: 11.06529236904628  loss: 0.7554 (0.7848)  acc1: 77.3438 (77.0703)  acc5: 97.6562 (97.7734)  time: 11.5820  data: 0.0181\n",
      "Epoch: [14]  [20/40]  eta: 0:03:51  lr: 1.000000000000001e-18  img/s: 11.135602901428708  loss: 0.7528 (0.7794)  acc1: 78.1250 (77.2321)  acc5: 97.6562 (97.7307)  time: 11.5941  data: 0.0181\n",
      "Epoch: [14]  [21/40]  eta: 0:03:39  lr: 1.000000000000001e-18  img/s: 11.097505664809471  loss: 0.7554 (0.7795)  acc1: 77.3438 (77.1662)  acc5: 97.6562 (97.7273)  time: 11.5870  data: 0.0182\n",
      "Epoch: [14]  [22/40]  eta: 0:03:28  lr: 1.000000000000001e-18  img/s: 11.109737311145185  loss: 0.7554 (0.7810)  acc1: 77.3438 (77.0720)  acc5: 97.6562 (97.6223)  time: 11.5925  data: 0.0182\n",
      "Epoch: [14]  [23/40]  eta: 0:03:16  lr: 1.000000000000001e-18  img/s: 11.05943833919889  loss: 0.7346 (0.7784)  acc1: 77.3438 (77.0833)  acc5: 97.6562 (97.6562)  time: 11.5877  data: 0.0182\n",
      "Epoch: [14]  [24/40]  eta: 0:03:05  lr: 1.000000000000001e-18  img/s: 11.143049766199134  loss: 0.7554 (0.7822)  acc1: 77.3438 (77.0000)  acc5: 97.6562 (97.6250)  time: 11.5866  data: 0.0182\n",
      "Epoch: [14]  [25/40]  eta: 0:02:53  lr: 1.000000000000001e-18  img/s: 11.162952675082797  loss: 0.7346 (0.7780)  acc1: 77.3438 (77.2236)  acc5: 97.6562 (97.6262)  time: 11.5830  data: 0.0179\n",
      "Epoch: [14]  [26/40]  eta: 0:02:42  lr: 1.000000000000001e-18  img/s: 10.89486450277266  loss: 0.7346 (0.7735)  acc1: 77.3438 (77.3148)  acc5: 97.6562 (97.6562)  time: 11.5866  data: 0.0179\n",
      "Epoch: [14]  [27/40]  eta: 0:02:30  lr: 1.000000000000001e-18  img/s: 11.18497727622401  loss: 0.7266 (0.7718)  acc1: 78.1250 (77.3438)  acc5: 97.6562 (97.7121)  time: 11.5858  data: 0.0179\n",
      "Epoch: [14]  [28/40]  eta: 0:02:18  lr: 1.000000000000001e-18  img/s: 11.101577514384031  loss: 0.7229 (0.7680)  acc1: 78.1250 (77.4246)  acc5: 97.6562 (97.7101)  time: 11.5899  data: 0.0179\n",
      "Epoch: [14]  [29/40]  eta: 0:02:07  lr: 1.000000000000001e-18  img/s: 11.092074453855668  loss: 0.7229 (0.7752)  acc1: 78.1250 (77.2135)  acc5: 97.6562 (97.6823)  time: 11.5852  data: 0.0179\n",
      "Epoch: [14]  [30/40]  eta: 0:01:55  lr: 1.000000000000001e-18  img/s: 11.092234874404884  loss: 0.7266 (0.7746)  acc1: 78.1250 (77.1925)  acc5: 97.6562 (97.6815)  time: 11.5809  data: 0.0179\n",
      "Epoch: [14]  [31/40]  eta: 0:01:44  lr: 1.000000000000001e-18  img/s: 11.06848367041156  loss: 0.7346 (0.7790)  acc1: 77.3438 (77.0752)  acc5: 97.6562 (97.6074)  time: 11.5821  data: 0.0179\n",
      "Epoch: [14]  [32/40]  eta: 0:01:32  lr: 1.000000000000001e-18  img/s: 10.98626482875672  loss: 0.7346 (0.7777)  acc1: 77.3438 (76.9886)  acc5: 97.6562 (97.6326)  time: 11.5844  data: 0.0178\n",
      "Epoch: [14]  [33/40]  eta: 0:01:21  lr: 1.000000000000001e-18  img/s: 10.99609319531072  loss: 0.7346 (0.7757)  acc1: 78.1250 (77.0680)  acc5: 97.6562 (97.6103)  time: 11.5904  data: 0.0178\n",
      "Epoch: [14]  [34/40]  eta: 0:01:09  lr: 1.000000000000001e-18  img/s: 11.080006958348948  loss: 0.7346 (0.7764)  acc1: 78.1250 (77.0982)  acc5: 97.6562 (97.6339)  time: 11.5843  data: 0.0178\n",
      "Epoch: [14]  [35/40]  eta: 0:00:57  lr: 1.000000000000001e-18  img/s: 11.017416855090298  loss: 0.7371 (0.7777)  acc1: 78.1250 (77.1050)  acc5: 97.6562 (97.5911)  time: 11.5892  data: 0.0178\n",
      "Epoch: [14]  [36/40]  eta: 0:00:46  lr: 1.000000000000001e-18  img/s: 11.018639482845826  loss: 0.7346 (0.7744)  acc1: 78.1250 (77.2382)  acc5: 97.6562 (97.6140)  time: 11.5813  data: 0.0178\n",
      "Epoch: [14]  [37/40]  eta: 0:00:34  lr: 1.000000000000001e-18  img/s: 10.970885485071319  loss: 0.7346 (0.7730)  acc1: 78.1250 (77.3438)  acc5: 97.6562 (97.5946)  time: 11.5812  data: 0.0178\n",
      "Epoch: [14]  [38/40]  eta: 0:00:23  lr: 1.000000000000001e-18  img/s: 10.9672286193131  loss: 0.7266 (0.7698)  acc1: 78.1250 (77.4439)  acc5: 97.6562 (97.6162)  time: 11.5893  data: 0.0179\n",
      "Epoch: [14]  [39/40]  eta: 0:00:11  lr: 1.000000000000001e-18  img/s: 10.320674363140098  loss: 0.7266 (0.7690)  acc1: 78.1250 (77.4600)  acc5: 97.6562 (97.6200)  time: 11.0489  data: 0.0171\n",
      "Epoch: [14] Total time: 0:07:32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [01:52<00:00, 12.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112.58940046280622\n",
      "Accuracy of the network on the 10000 test images: 75.0760 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "axx_mult = 'appro3'\n",
    "model = densenet121(pretrained=True, axx_mult = axx_mult)\n",
    "model.eval() # for evaluation\n",
    "# It is a bit slow since we collect histograms on CPU\n",
    "with torch.no_grad():\n",
    "    stats = collect_stats(model, data_t, num_batches=2)\n",
    "    amax = compute_amax(model, method=\"percentile\", percentile=99.99)\n",
    "    \n",
    "# Inference without retraining\n",
    "import timeit\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "model.eval()\n",
    "start_time = timeit.default_timer()\n",
    "with torch.no_grad():\n",
    "    for iteraction, (images, labels) in tqdm(enumerate(data), total=len(data)):\n",
    "        images, labels = images.to(\"cpu\"), labels.to(\"cpu\")\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(timeit.default_timer() - start_time)\n",
    "print('Accuracy of the network on the 10000 test images: %.4f %%' % (\n",
    "    100 * correct / total))\n",
    "\n",
    "# Retraining for 15 epochs\n",
    "from adapt.references.classification.train import evaluate, train_one_epoch, load_data\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "for epoch in range(15):\n",
    "    print(f\"Epoch {epoch + 1}/15\")\n",
    "    train_one_epoch(model, criterion, optimizer, data_t, \"cpu\", epoch, 1)\n",
    "    lr_scheduler.step()\n",
    "\n",
    "# Inference after retraining\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "model.eval()\n",
    "start_time = timeit.default_timer()\n",
    "with torch.no_grad():\n",
    "    for iteraction, (images, labels) in tqdm(enumerate(data), total=len(data)):\n",
    "        images, labels = images.to(\"cpu\"), labels.to(\"cpu\")\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(timeit.default_timer() - start_time)\n",
    "print('Accuracy of the network on the 10000 test images: %.4f %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ee44b8-3620-4676-9174-f3943094aaec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
